{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"UNSW-NB15 IDS Research Workspace","text":"<p>Welcome to the official documentation for the Handling Class Imbalance in Binary and Multiclass Intrusion Detection on the UNSW-NB15 Dataset research project.</p>"},{"location":"#mission-statement","title":"\ud83c\udfaf Mission Statement","text":"<p>The goal of this project is to provide a rigorous, reproducible, and transparent baseline for evaluating the impact of class imbalance strategies on Network Intrusion Detection Systems (IDS).</p> <p>Traditional IDS models trained on the UNSW-NB15 dataset often achieve high overall accuracy by simply learning to predict the dominant class (Normal traffic). This creates a dangerous illusion of security, as critical minority attack classes like Worms (0.07%) and Shellcode (0.65%) are almost entirely missed.</p> <p>This project directly addresses this problem by:</p> <ol> <li>Systematically Evaluating three imbalance handling strategies (Baseline, Class Weighting, Random Oversampling).</li> <li>Focusing on Rare Class Detection using metrics like G-Mean and Macro-F1, which treat all classes equally.</li> <li>Enforcing Strict Data Contracts to prevent data leakage and ensure scientific validity.</li> </ol>"},{"location":"#key-features","title":"\u2728 Key Features","text":"Feature Description \ud83d\udd2c 18-Experiment Grid Comprehensive evaluation: 2 Tasks \u00d7 3 Models \u00d7 3 Strategies. \ud83c\udfaf Rare Class Focus Explicit analysis of Worms, Shellcode, Backdoor, and Analysis attack types. \ud83d\udd12 Leakage-Proof Strict contracts: preprocessing fitted only on training data. \ud83d\udcca Automated Reports Radar charts, heatmaps, and per-class metrics generated automatically. \u2601\ufe0f Colab Ready Zero-setup cloud execution via Google Colab."},{"location":"#documentation-map","title":"\ud83d\uddfa\ufe0f Documentation Map","text":"<p>Navigate the documentation using the links below:</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Introduction: Prerequisites and setup roadmap.</li> <li>Installation: Local and Colab installation guides.</li> <li>Quickstart: Run your first experiment in minutes.</li> </ul>"},{"location":"#research","title":"Research","text":"<ul> <li>Methodology: Deep dive into S0, S1, S2 strategies with mathematical definitions.</li> <li>Findings: Comprehensive analysis of results with tables and visualizations.</li> <li>Data Contract: Binding rules for data handling.</li> <li>Experiment Contract: Fixed hyperparameters and evaluation protocols.</li> </ul>"},{"location":"#experiments","title":"Experiments","text":"<ul> <li>Running Experiments: CLI reference and output structure.</li> <li>Google Colab Guide: Full tutorial for cloud execution.</li> <li>Reproducibility: How we ensure deterministic results.</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>Data Module: <code>src.data</code> - Preprocessing, loading.</li> <li>Models Module: <code>src.models</code> - LR, RF, XGB wrappers.</li> <li>Evaluation Module: <code>src.evaluation</code> - Metrics, visualizers.</li> <li>Strategies Module: <code>src.strategies</code> - Imbalance handling logic.</li> </ul>"},{"location":"#repository-architecture","title":"\ud83d\udcc2 Repository Architecture","text":"<pre><code>.\n\u251c\u2500\u2500 configs/\n\u2502   \u2514\u2500\u2500 main.yaml          # Master configuration file\n\u251c\u2500\u2500 dataset/\n\u2502   \u251c\u2500\u2500 UNSW_NB15_training-set.csv\n\u2502   \u2514\u2500\u2500 UNSW_NB15_testing-set.csv\n\u251c\u2500\u2500 docs/                  # &lt;- You are here\n\u251c\u2500\u2500 reports/               # Auto-generated analysis reports\n\u251c\u2500\u2500 results/               # Experiment outputs (metrics, figures, logs)\n\u251c\u2500\u2500 scripts/               # Utility scripts (grid runners, visualizers)\n\u251c\u2500\u2500 src/                   # Core source code\n\u2502   \u251c\u2500\u2500 data/              # Data loading &amp; preprocessing\n\u2502   \u251c\u2500\u2500 models/            # Model definitions\n\u2502   \u251c\u2500\u2500 evaluation/        # Metrics &amp; visualization\n\u2502   \u2514\u2500\u2500 strategies/        # Imbalance handling\n\u2514\u2500\u2500 tests/                 # Unit tests\n</code></pre>"},{"location":"#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>Ready to dive in? Start with the Quickstart Guide to run your first experiment, or jump directly to Running in Colab for zero-setup execution.</p>"},{"location":"api/data/","title":"Data Pipeline API","text":""},{"location":"api/data/#src.data.loader","title":"<code>src.data.loader</code>","text":"<p>Data loading utilities for UNSW-NB15 dataset.</p> <p>This module provides a clean interface for loading the official training and testing CSV files with validation.</p>"},{"location":"api/data/#src.data.loader.DataLoader","title":"<code>DataLoader</code>","text":"<p>Load UNSW-NB15 dataset from CSV files.</p> <p>Handles loading of the official training and testing splits with basic validation to ensure file integrity.</p> <p>Attributes:</p> Name Type Description <code>train_path</code> <p>Path to training CSV file</p> <code>test_path</code> <p>Path to testing CSV file</p> Source code in <code>src\\data\\loader.py</code> <pre><code>class DataLoader:\n    \"\"\"\n    Load UNSW-NB15 dataset from CSV files.\n\n    Handles loading of the official training and testing splits\n    with basic validation to ensure file integrity.\n\n    Attributes:\n        train_path: Path to training CSV file\n        test_path: Path to testing CSV file\n    \"\"\"\n\n    def __init__(self, config: dict):\n        \"\"\"\n        Initialize DataLoader with configuration.\n\n        Args:\n            config: Configuration dictionary containing data paths\n        \"\"\"\n        self.train_path = Path(config['data']['train_path'])\n        self.test_path = Path(config['data']['test_path'])\n        self.config = config\n        self._validate_paths()\n\n    def _validate_paths(self) -&gt; None:\n        \"\"\"Validate that required data files exist.\"\"\"\n        if not self.train_path.exists():\n            raise FileNotFoundError(\n                f\"Training file not found: {self.train_path}\\n\"\n                \"Please ensure UNSW_NB15_training-set.csv is in the dataset/ directory.\"\n            )\n        if not self.test_path.exists():\n            raise FileNotFoundError(\n                f\"Test file not found: {self.test_path}\\n\"\n                \"Please ensure UNSW_NB15_testing-set.csv is in the dataset/ directory.\"\n            )\n        logger.info(\"Dataset files validated successfully\")\n\n    def load_train(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Load training dataset.\n\n        Returns:\n            DataFrame containing training data (175,341 rows expected)\n        \"\"\"\n        logger.info(f\"Loading training data from {self.train_path}\")\n        df = pd.read_csv(self.train_path)\n        logger.info(f\"Loaded {len(df):,} training samples with {len(df.columns)} columns\")\n\n        # Auditor T001: Schema Validation\n        self._validate_schema(df, \"Training Set\")\n\n        return df\n\n    def _validate_schema(self, df: pd.DataFrame, set_name: str) -&gt; None:\n        \"\"\"\n        Validate that the dataframe contains all required columns from config.\n\n        Args:\n            df: DataFrame to validate\n            set_name: Name of the dataset for logging\n\n        Raises:\n            ValueError: If required columns are missing\n        \"\"\"\n        # 1. Check target columns        \n        required_cols = []\n        # Add targets\n        required_cols.append(self.config['data']['target_binary'])\n        required_cols.append(self.config['data']['target_multiclass'])\n\n        # Add categorical columns\n        required_cols.extend(self.config['data']['categorical_columns'])\n\n        # Check for missing columns\n        missing = [col for col in required_cols if col not in df.columns]\n\n        if missing:\n            raise ValueError(f\"{set_name} schema validation failed. Missing columns: {missing}\")\n\n        logger.info(f\"{set_name} schema validated successfully (all required columns present).\")\n\n    def load_test(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Load test dataset.\n\n        Note: Test set should ONLY be used for final evaluation,\n        never for training, validation, or hyperparameter tuning.\n\n        Returns:\n            DataFrame containing test data (82,332 rows expected)\n        \"\"\"\n        logger.info(f\"Loading test data from {self.test_path}\")\n        df = pd.read_csv(self.test_path)\n        logger.info(f\"Loaded {len(df):,} test samples with {len(df.columns)} columns\")\n\n        # Auditor T001: Schema Validation\n        self._validate_schema(df, \"Test Set\")\n\n        return df\n\n    def load_all(self) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Load both training and test datasets.\n\n        Returns:\n            Tuple of (train_df, test_df)\n        \"\"\"\n        return self.load_train(), self.load_test()\n</code></pre>"},{"location":"api/data/#src.data.loader.DataLoader.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize DataLoader with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing data paths</p> required Source code in <code>src\\data\\loader.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"\n    Initialize DataLoader with configuration.\n\n    Args:\n        config: Configuration dictionary containing data paths\n    \"\"\"\n    self.train_path = Path(config['data']['train_path'])\n    self.test_path = Path(config['data']['test_path'])\n    self.config = config\n    self._validate_paths()\n</code></pre>"},{"location":"api/data/#src.data.loader.DataLoader.load_all","title":"<code>load_all()</code>","text":"<p>Load both training and test datasets.</p> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple of (train_df, test_df)</p> Source code in <code>src\\data\\loader.py</code> <pre><code>def load_all(self) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Load both training and test datasets.\n\n    Returns:\n        Tuple of (train_df, test_df)\n    \"\"\"\n    return self.load_train(), self.load_test()\n</code></pre>"},{"location":"api/data/#src.data.loader.DataLoader.load_test","title":"<code>load_test()</code>","text":"<p>Load test dataset.</p> <p>Note: Test set should ONLY be used for final evaluation, never for training, validation, or hyperparameter tuning.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing test data (82,332 rows expected)</p> Source code in <code>src\\data\\loader.py</code> <pre><code>def load_test(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Load test dataset.\n\n    Note: Test set should ONLY be used for final evaluation,\n    never for training, validation, or hyperparameter tuning.\n\n    Returns:\n        DataFrame containing test data (82,332 rows expected)\n    \"\"\"\n    logger.info(f\"Loading test data from {self.test_path}\")\n    df = pd.read_csv(self.test_path)\n    logger.info(f\"Loaded {len(df):,} test samples with {len(df.columns)} columns\")\n\n    # Auditor T001: Schema Validation\n    self._validate_schema(df, \"Test Set\")\n\n    return df\n</code></pre>"},{"location":"api/data/#src.data.loader.DataLoader.load_train","title":"<code>load_train()</code>","text":"<p>Load training dataset.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing training data (175,341 rows expected)</p> Source code in <code>src\\data\\loader.py</code> <pre><code>def load_train(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Load training dataset.\n\n    Returns:\n        DataFrame containing training data (175,341 rows expected)\n    \"\"\"\n    logger.info(f\"Loading training data from {self.train_path}\")\n    df = pd.read_csv(self.train_path)\n    logger.info(f\"Loaded {len(df):,} training samples with {len(df.columns)} columns\")\n\n    # Auditor T001: Schema Validation\n    self._validate_schema(df, \"Training Set\")\n\n    return df\n</code></pre>"},{"location":"api/data/#src.data.preprocessing","title":"<code>src.data.preprocessing</code>","text":"<p>Preprocessing pipeline for UNSW-NB15 dataset.</p> <p>This module implements the complete preprocessing pipeline including: - Feature cleaning (drop identifier columns) - Missing value imputation - Categorical encoding (One-Hot) - Numerical scaling (StandardScaler) - Stratified train/validation splitting</p> <p>CRITICAL: All transformers are fit on training data ONLY to prevent data leakage.</p>"},{"location":"api/data/#src.data.preprocessing.UNSWPreprocessor","title":"<code>UNSWPreprocessor</code>","text":"<p>Unified preprocessing pipeline for UNSW-NB15 dataset.</p> <p>This class handles the complete preprocessing workflow while ensuring no data leakage occurs between training, validation, and test sets.</p> <p>Key guarantees: - All statistics (mean, std, categories) computed on training only - Consistent encoding across all splits - Stratified validation split preserves class distribution - Test set remains completely isolated</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration dictionary</p> <code>drop_columns</code> <p>Columns to remove (identifiers)</p> <code>categorical_cols</code> <p>Columns for one-hot encoding</p> <code>numerical_cols</code> <code>List[str]</code> <p>Columns for standard scaling</p> <code>feature_names</code> <code>List[str]</code> <p>Final feature names after transformation</p> Source code in <code>src\\data\\preprocessing.py</code> <pre><code>class UNSWPreprocessor:\n    \"\"\"\n    Unified preprocessing pipeline for UNSW-NB15 dataset.\n\n    This class handles the complete preprocessing workflow while ensuring\n    no data leakage occurs between training, validation, and test sets.\n\n    Key guarantees:\n    - All statistics (mean, std, categories) computed on training only\n    - Consistent encoding across all splits\n    - Stratified validation split preserves class distribution\n    - Test set remains completely isolated\n\n    Attributes:\n        config: Configuration dictionary\n        drop_columns: Columns to remove (identifiers)\n        categorical_cols: Columns for one-hot encoding\n        numerical_cols: Columns for standard scaling\n        feature_names: Final feature names after transformation\n    \"\"\"\n\n    def __init__(self, config: dict):\n        \"\"\"\n        Initialize preprocessor with configuration.\n\n        Args:\n            config: Configuration dictionary with data specifications\n        \"\"\"\n        self.config = config\n        self.drop_columns = config['data']['drop_columns']\n        self.categorical_cols = config['data']['categorical_columns']\n        self.target_binary = config['data']['target_binary']\n        self.target_multi = config['data']['target_multiclass']\n        self.val_size = config.get('validation_size', 0.20)\n        self.random_state = config.get('random_state', 42)\n\n        # Transformers - initialized during fit\n        self.preprocessor: Optional[ColumnTransformer] = None\n        self.label_encoder: Optional[LabelEncoder] = None\n\n        # Feature metadata\n        self.numerical_cols: List[str] = []\n        self.feature_names: List[str] = []\n        self.label_mapping: Dict[str, int] = {}\n\n        # Processed data storage\n        self.X_train: Optional[np.ndarray] = None\n        self.X_val: Optional[np.ndarray] = None\n        self.X_test: Optional[np.ndarray] = None\n        self.y_train_binary: Optional[np.ndarray] = None\n        self.y_val_binary: Optional[np.ndarray] = None\n        self.y_test_binary: Optional[np.ndarray] = None\n        self.y_train_multi: Optional[np.ndarray] = None\n        self.y_val_multi: Optional[np.ndarray] = None\n        self.y_test_multi: Optional[np.ndarray] = None\n\n        # Original sizes for validation\n        self._original_train_size: int = 0\n        self._original_test_size: int = 0\n\n    def _identify_columns(self, df: pd.DataFrame) -&gt; None:\n        \"\"\"Identify numerical columns after dropping identifiers and targets.\"\"\"\n        # All columns except drops, categoricals, and targets\n        exclude_cols = set(self.drop_columns + self.categorical_cols + \n                          [self.target_binary, self.target_multi])\n        self.numerical_cols = [col for col in df.columns if col not in exclude_cols]\n        logger.info(f\"Identified {len(self.numerical_cols)} numerical features\")\n        logger.info(f\"Identified {len(self.categorical_cols)} categorical features\")\n\n    def _create_preprocessor(self) -&gt; ColumnTransformer:\n        \"\"\"\n        Create sklearn ColumnTransformer for preprocessing.\n\n        Returns:\n            Configured ColumnTransformer\n        \"\"\"\n        # Numerical pipeline: impute missing \u2192 scale\n        numerical_pipeline = Pipeline([\n            ('imputer', SimpleImputer(strategy='median')),\n            ('scaler', StandardScaler())\n        ])\n\n        # Categorical pipeline: impute missing \u2192 one-hot encode\n        categorical_pipeline = Pipeline([\n            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n        ])\n\n        preprocessor = ColumnTransformer(\n            transformers=[\n                ('num', numerical_pipeline, self.numerical_cols),\n                ('cat', categorical_pipeline, self.categorical_cols)\n            ],\n            remainder='drop'  # Drop any remaining columns\n        )\n\n        return preprocessor\n\n    def _prepare_labels(self, df: pd.DataFrame, is_train: bool = False\n                       ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Prepare binary and multiclass labels.\n\n        Args:\n            df: DataFrame with target columns\n            is_train: If True, fit the label encoder\n\n        Returns:\n            Tuple of (binary_labels, multiclass_labels)\n        \"\"\"\n        # Binary: 0 = Normal, 1 = Attack\n        y_binary = df[self.target_binary].values.astype(int)\n\n        # Multiclass: encode attack categories\n        if is_train:\n            self.label_encoder = LabelEncoder()\n            # Fit on known labels to ensure consistent ordering\n            known_labels = self.config['data'].get('multiclass_labels', \n                                                    df[self.target_multi].unique())\n            self.label_encoder.fit(known_labels)\n            self.label_mapping = {label: idx for idx, label in \n                                 enumerate(self.label_encoder.classes_)}\n            logger.info(f\"Label mapping: {self.label_mapping}\")\n\n        # Handle unseen categories gracefully\n        y_multi_raw = df[self.target_multi].values\n        y_multi = np.zeros(len(y_multi_raw), dtype=int)\n        for i, label in enumerate(y_multi_raw):\n            if label in self.label_mapping:\n                y_multi[i] = self.label_mapping[label]\n            else:\n                # Map unknown to closest or log warning\n                logger.warning(f\"Unknown label encountered: {label}\")\n                y_multi[i] = 0  # Default to Normal\n\n        return y_binary, y_multi\n\n    def fit_transform(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Fit preprocessor on training data and transform all splits.\n\n        This is the main preprocessing method that:\n        1. Creates stratified train/validation split\n        2. Fits all transformers on training data only\n        3. Transforms all three splits\n\n        Args:\n            train_df: Official training DataFrame (175,341 rows)\n            test_df: Official test DataFrame (82,332 rows)\n        \"\"\"\n        logger.info(\"=\" * 60)\n        logger.info(\"Starting preprocessing pipeline\")\n        logger.info(\"=\" * 60)\n\n        self._original_test_size = len(test_df)\n\n        # Identify column types\n        self._identify_columns(train_df)\n\n        # Step 1: Stratified train/validation split\n        logger.info(f\"Creating {1-self.val_size:.0%}/{self.val_size:.0%} train/val split\")\n        train_split, val_split = train_test_split(\n            train_df,\n            test_size=self.val_size,\n            random_state=self.random_state,\n            stratify=train_df[self.target_multi]  # Stratify by multiclass for better balance\n        )\n\n        self._original_train_size = len(train_split)\n        logger.info(f\"Training samples: {len(train_split):,}\")\n        logger.info(f\"Validation samples: {len(val_split):,}\")\n        logger.info(f\"Test samples: {len(test_df):,}\")\n\n        # Step 2: Prepare features (drop identifiers and targets)\n        feature_cols = self.numerical_cols + self.categorical_cols\n        X_train_raw = train_split[feature_cols]\n        X_val_raw = val_split[feature_cols]\n        X_test_raw = test_df[feature_cols]\n\n        # Step 3: Prepare labels\n        self.y_train_binary, self.y_train_multi = self._prepare_labels(\n            train_split, is_train=True)\n        self.y_val_binary, self.y_val_multi = self._prepare_labels(val_split)\n        self.y_test_binary, self.y_test_multi = self._prepare_labels(test_df)\n\n        # Step 4: Create and fit preprocessor on TRAINING ONLY\n        logger.info(\"Fitting preprocessor on training data...\")\n        self.preprocessor = self._create_preprocessor()\n        self.X_train = self.preprocessor.fit_transform(X_train_raw).astype(np.float32)\n\n        # Step 5: Transform validation and test (NO fitting!)\n        logger.info(\"Transforming validation and test data...\")\n        self.X_val = self.preprocessor.transform(X_val_raw).astype(np.float32)\n        self.X_test = self.preprocessor.transform(X_test_raw).astype(np.float32)\n\n        # Step 6: Extract feature names\n        self._extract_feature_names()\n\n        # Log final shapes\n        logger.info(\"-\" * 40)\n        logger.info(\"Preprocessing complete!\")\n        logger.info(f\"X_train shape: {self.X_train.shape}\")\n        logger.info(f\"X_val shape: {self.X_val.shape}\")\n        logger.info(f\"X_test shape: {self.X_test.shape}\")\n        logger.info(f\"Total features: {self.X_train.shape[1]}\")\n\n        # Validate no data leakage\n        self._validate_no_leakage()\n\n    def _extract_feature_names(self) -&gt; None:\n        \"\"\"Extract feature names from fitted preprocessor.\"\"\"\n        feature_names = []\n\n        # Numerical features (unchanged names)\n        feature_names.extend(self.numerical_cols)\n\n        # Categorical features (one-hot encoded names)\n        cat_encoder = self.preprocessor.named_transformers_['cat']['encoder']\n        for col, categories in zip(self.categorical_cols, cat_encoder.categories_):\n            for cat in categories:\n                feature_names.append(f\"{col}_{cat}\")\n\n        self.feature_names = feature_names\n        logger.info(f\"Extracted {len(feature_names)} feature names\")\n\n    def _validate_no_leakage(self) -&gt; None:\n        \"\"\"Validate that no data leakage has occurred.\"\"\"\n        # Check sizes unchanged\n        assert self.X_val.shape[0] + self.X_train.shape[0] &gt; 0, \"Empty training data!\"\n        assert self.X_test.shape[0] == self._original_test_size, \\\n            f\"Test size changed! Expected {self._original_test_size}, got {self.X_test.shape[0]}\"\n\n        # Check for NaN/Inf\n        assert not np.isnan(self.X_train).any(), \"NaN values in X_train!\"\n        assert not np.isnan(self.X_val).any(), \"NaN values in X_val!\"\n        assert not np.isnan(self.X_test).any(), \"NaN values in X_test!\"\n        assert not np.isinf(self.X_train).any(), \"Inf values in X_train!\"\n\n        logger.info(\"[OK] Data leakage validation passed\")\n\n    def get_splits(self, task: str) -&gt; Tuple[np.ndarray, np.ndarray, \n                                              np.ndarray, np.ndarray,\n                                              np.ndarray, np.ndarray]:\n        \"\"\"\n        Get preprocessed data splits for specified task.\n\n        Args:\n            task: 'binary' or 'multi'\n\n        Returns:\n            Tuple of (X_train, y_train, X_val, y_val, X_test, y_test)\n        \"\"\"\n        if self.X_train is None:\n            raise RuntimeError(\"Preprocessor not fitted. Call fit_transform first.\")\n\n        if task == 'binary':\n            return (self.X_train, self.y_train_binary,\n                    self.X_val, self.y_val_binary,\n                    self.X_test, self.y_test_binary)\n        elif task == 'multi':\n            return (self.X_train, self.y_train_multi,\n                    self.X_val, self.y_val_multi,\n                    self.X_test, self.y_test_multi)\n        else:\n            raise ValueError(f\"Unknown task: {task}. Use 'binary' or 'multi'.\")\n\n    def get_class_distribution(self, task: str) -&gt; Dict[str, Dict[str, int]]:\n        \"\"\"\n        Get class distribution for each split.\n\n        Args:\n            task: 'binary' or 'multi'\n\n        Returns:\n            Dictionary with distributions for train, val, test\n        \"\"\"\n        _, y_train, _, y_val, _, y_test = self.get_splits(task)\n\n        def count_classes(y):\n            unique, counts = np.unique(y, return_counts=True)\n            return dict(zip(unique.astype(str), counts.tolist()))\n\n        return {\n            'train': count_classes(y_train),\n            'val': count_classes(y_val),\n            'test': count_classes(y_test)\n        }\n\n    def save_metadata(self, path: str) -&gt; None:\n        \"\"\"Save preprocessing metadata for reproducibility.\"\"\"\n        metadata = {\n            'feature_names': self.feature_names,\n            'label_mapping': self.label_mapping,\n            'numerical_cols': self.numerical_cols,\n            'categorical_cols': self.categorical_cols,\n            'train_shape': list(self.X_train.shape),\n            'val_shape': list(self.X_val.shape),\n            'test_shape': list(self.X_test.shape)\n        }\n\n        Path(path).parent.mkdir(parents=True, exist_ok=True)\n        with open(path, 'w') as f:\n            json.dump(metadata, f, indent=2)\n        logger.info(f\"Saved preprocessing metadata to {path}\")\n</code></pre>"},{"location":"api/data/#src.data.preprocessing.UNSWPreprocessor.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize preprocessor with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with data specifications</p> required Source code in <code>src\\data\\preprocessing.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"\n    Initialize preprocessor with configuration.\n\n    Args:\n        config: Configuration dictionary with data specifications\n    \"\"\"\n    self.config = config\n    self.drop_columns = config['data']['drop_columns']\n    self.categorical_cols = config['data']['categorical_columns']\n    self.target_binary = config['data']['target_binary']\n    self.target_multi = config['data']['target_multiclass']\n    self.val_size = config.get('validation_size', 0.20)\n    self.random_state = config.get('random_state', 42)\n\n    # Transformers - initialized during fit\n    self.preprocessor: Optional[ColumnTransformer] = None\n    self.label_encoder: Optional[LabelEncoder] = None\n\n    # Feature metadata\n    self.numerical_cols: List[str] = []\n    self.feature_names: List[str] = []\n    self.label_mapping: Dict[str, int] = {}\n\n    # Processed data storage\n    self.X_train: Optional[np.ndarray] = None\n    self.X_val: Optional[np.ndarray] = None\n    self.X_test: Optional[np.ndarray] = None\n    self.y_train_binary: Optional[np.ndarray] = None\n    self.y_val_binary: Optional[np.ndarray] = None\n    self.y_test_binary: Optional[np.ndarray] = None\n    self.y_train_multi: Optional[np.ndarray] = None\n    self.y_val_multi: Optional[np.ndarray] = None\n    self.y_test_multi: Optional[np.ndarray] = None\n\n    # Original sizes for validation\n    self._original_train_size: int = 0\n    self._original_test_size: int = 0\n</code></pre>"},{"location":"api/data/#src.data.preprocessing.UNSWPreprocessor.fit_transform","title":"<code>fit_transform(train_df, test_df)</code>","text":"<p>Fit preprocessor on training data and transform all splits.</p> <p>This is the main preprocessing method that: 1. Creates stratified train/validation split 2. Fits all transformers on training data only 3. Transforms all three splits</p> <p>Parameters:</p> Name Type Description Default <code>train_df</code> <code>DataFrame</code> <p>Official training DataFrame (175,341 rows)</p> required <code>test_df</code> <code>DataFrame</code> <p>Official test DataFrame (82,332 rows)</p> required Source code in <code>src\\data\\preprocessing.py</code> <pre><code>def fit_transform(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Fit preprocessor on training data and transform all splits.\n\n    This is the main preprocessing method that:\n    1. Creates stratified train/validation split\n    2. Fits all transformers on training data only\n    3. Transforms all three splits\n\n    Args:\n        train_df: Official training DataFrame (175,341 rows)\n        test_df: Official test DataFrame (82,332 rows)\n    \"\"\"\n    logger.info(\"=\" * 60)\n    logger.info(\"Starting preprocessing pipeline\")\n    logger.info(\"=\" * 60)\n\n    self._original_test_size = len(test_df)\n\n    # Identify column types\n    self._identify_columns(train_df)\n\n    # Step 1: Stratified train/validation split\n    logger.info(f\"Creating {1-self.val_size:.0%}/{self.val_size:.0%} train/val split\")\n    train_split, val_split = train_test_split(\n        train_df,\n        test_size=self.val_size,\n        random_state=self.random_state,\n        stratify=train_df[self.target_multi]  # Stratify by multiclass for better balance\n    )\n\n    self._original_train_size = len(train_split)\n    logger.info(f\"Training samples: {len(train_split):,}\")\n    logger.info(f\"Validation samples: {len(val_split):,}\")\n    logger.info(f\"Test samples: {len(test_df):,}\")\n\n    # Step 2: Prepare features (drop identifiers and targets)\n    feature_cols = self.numerical_cols + self.categorical_cols\n    X_train_raw = train_split[feature_cols]\n    X_val_raw = val_split[feature_cols]\n    X_test_raw = test_df[feature_cols]\n\n    # Step 3: Prepare labels\n    self.y_train_binary, self.y_train_multi = self._prepare_labels(\n        train_split, is_train=True)\n    self.y_val_binary, self.y_val_multi = self._prepare_labels(val_split)\n    self.y_test_binary, self.y_test_multi = self._prepare_labels(test_df)\n\n    # Step 4: Create and fit preprocessor on TRAINING ONLY\n    logger.info(\"Fitting preprocessor on training data...\")\n    self.preprocessor = self._create_preprocessor()\n    self.X_train = self.preprocessor.fit_transform(X_train_raw).astype(np.float32)\n\n    # Step 5: Transform validation and test (NO fitting!)\n    logger.info(\"Transforming validation and test data...\")\n    self.X_val = self.preprocessor.transform(X_val_raw).astype(np.float32)\n    self.X_test = self.preprocessor.transform(X_test_raw).astype(np.float32)\n\n    # Step 6: Extract feature names\n    self._extract_feature_names()\n\n    # Log final shapes\n    logger.info(\"-\" * 40)\n    logger.info(\"Preprocessing complete!\")\n    logger.info(f\"X_train shape: {self.X_train.shape}\")\n    logger.info(f\"X_val shape: {self.X_val.shape}\")\n    logger.info(f\"X_test shape: {self.X_test.shape}\")\n    logger.info(f\"Total features: {self.X_train.shape[1]}\")\n\n    # Validate no data leakage\n    self._validate_no_leakage()\n</code></pre>"},{"location":"api/data/#src.data.preprocessing.UNSWPreprocessor.get_class_distribution","title":"<code>get_class_distribution(task)</code>","text":"<p>Get class distribution for each split.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>'binary' or 'multi'</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, int]]</code> <p>Dictionary with distributions for train, val, test</p> Source code in <code>src\\data\\preprocessing.py</code> <pre><code>def get_class_distribution(self, task: str) -&gt; Dict[str, Dict[str, int]]:\n    \"\"\"\n    Get class distribution for each split.\n\n    Args:\n        task: 'binary' or 'multi'\n\n    Returns:\n        Dictionary with distributions for train, val, test\n    \"\"\"\n    _, y_train, _, y_val, _, y_test = self.get_splits(task)\n\n    def count_classes(y):\n        unique, counts = np.unique(y, return_counts=True)\n        return dict(zip(unique.astype(str), counts.tolist()))\n\n    return {\n        'train': count_classes(y_train),\n        'val': count_classes(y_val),\n        'test': count_classes(y_test)\n    }\n</code></pre>"},{"location":"api/data/#src.data.preprocessing.UNSWPreprocessor.get_splits","title":"<code>get_splits(task)</code>","text":"<p>Get preprocessed data splits for specified task.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>'binary' or 'multi'</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray]</code> <p>Tuple of (X_train, y_train, X_val, y_val, X_test, y_test)</p> Source code in <code>src\\data\\preprocessing.py</code> <pre><code>def get_splits(self, task: str) -&gt; Tuple[np.ndarray, np.ndarray, \n                                          np.ndarray, np.ndarray,\n                                          np.ndarray, np.ndarray]:\n    \"\"\"\n    Get preprocessed data splits for specified task.\n\n    Args:\n        task: 'binary' or 'multi'\n\n    Returns:\n        Tuple of (X_train, y_train, X_val, y_val, X_test, y_test)\n    \"\"\"\n    if self.X_train is None:\n        raise RuntimeError(\"Preprocessor not fitted. Call fit_transform first.\")\n\n    if task == 'binary':\n        return (self.X_train, self.y_train_binary,\n                self.X_val, self.y_val_binary,\n                self.X_test, self.y_test_binary)\n    elif task == 'multi':\n        return (self.X_train, self.y_train_multi,\n                self.X_val, self.y_val_multi,\n                self.X_test, self.y_test_multi)\n    else:\n        raise ValueError(f\"Unknown task: {task}. Use 'binary' or 'multi'.\")\n</code></pre>"},{"location":"api/data/#src.data.preprocessing.UNSWPreprocessor.save_metadata","title":"<code>save_metadata(path)</code>","text":"<p>Save preprocessing metadata for reproducibility.</p> Source code in <code>src\\data\\preprocessing.py</code> <pre><code>def save_metadata(self, path: str) -&gt; None:\n    \"\"\"Save preprocessing metadata for reproducibility.\"\"\"\n    metadata = {\n        'feature_names': self.feature_names,\n        'label_mapping': self.label_mapping,\n        'numerical_cols': self.numerical_cols,\n        'categorical_cols': self.categorical_cols,\n        'train_shape': list(self.X_train.shape),\n        'val_shape': list(self.X_val.shape),\n        'test_shape': list(self.X_test.shape)\n    }\n\n    Path(path).parent.mkdir(parents=True, exist_ok=True)\n    with open(path, 'w') as f:\n        json.dump(metadata, f, indent=2)\n    logger.info(f\"Saved preprocessing metadata to {path}\")\n</code></pre>"},{"location":"api/evaluation/","title":"Evaluation API","text":""},{"location":"api/evaluation/#src.evaluation.metrics","title":"<code>src.evaluation.metrics</code>","text":"<p>Evaluation metrics computation for UNSW-NB15 experiments.</p> <p>This module provides functions to compute comprehensive metrics including: - Overall metrics: Accuracy, Macro F1, Weighted F1, G-Mean, ROC-AUC - Per-class metrics: Precision, Recall, F1, Support - Rare class analysis: Special focus on Worms, Shellcode, Backdoor, Analysis</p>"},{"location":"api/evaluation/#src.evaluation.metrics.aggregate_experiment_results","title":"<code>aggregate_experiment_results(results)</code>","text":"<p>Aggregate results from multiple experiments into a summary table.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Dict[str, Any]]</code> <p>List of experiment result dictionaries</p> required <p>Returns:</p> Type Description <code>Dict[str, List[Any]]</code> <p>Dictionary suitable for DataFrame conversion</p> Source code in <code>src\\evaluation\\metrics.py</code> <pre><code>def aggregate_experiment_results(\n    results: List[Dict[str, Any]]\n) -&gt; Dict[str, List[Any]]:\n    \"\"\"\n    Aggregate results from multiple experiments into a summary table.\n\n    Args:\n        results: List of experiment result dictionaries\n\n    Returns:\n        Dictionary suitable for DataFrame conversion\n    \"\"\"\n    aggregated = {\n        'experiment_id': [],\n        'task': [],\n        'model': [],\n        'strategy': [],\n        'accuracy': [],\n        'macro_f1': [],\n        'weighted_f1': [],\n        'g_mean': [],\n        'roc_auc': [],\n        'training_time': []\n    }\n\n    for result in results:\n        aggregated['experiment_id'].append(result['experiment_id'])\n        aggregated['task'].append(result['task'])\n        aggregated['model'].append(result['model'])\n        aggregated['strategy'].append(result['strategy'])\n        aggregated['accuracy'].append(result['metrics']['overall']['accuracy'])\n        aggregated['macro_f1'].append(result['metrics']['overall']['macro_f1'])\n        aggregated['weighted_f1'].append(result['metrics']['overall']['weighted_f1'])\n        aggregated['g_mean'].append(result['metrics']['overall']['g_mean'])\n        aggregated['roc_auc'].append(result['metrics']['overall']['roc_auc'])\n        aggregated['training_time'].append(result.get('training_time_seconds', 0))\n\n    return aggregated\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.metrics.compute_all_metrics","title":"<code>compute_all_metrics(y_true, y_pred, y_pred_proba, task, class_names=None)</code>","text":"<p>Compute comprehensive metrics for a single experiment.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>Ground truth labels</p> required <code>y_pred</code> <code>ndarray</code> <p>Predicted class labels</p> required <code>y_pred_proba</code> <code>ndarray</code> <p>Prediction probabilities (n_samples, n_classes)</p> required <code>task</code> <code>str</code> <p>'binary' or 'multi'</p> required <code>class_names</code> <code>Optional[List[str]]</code> <p>Optional list of class names for reporting</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing:</p> <code>Dict[str, Any]</code> <ul> <li>overall: Accuracy, Macro F1, Weighted F1, G-Mean, ROC-AUC</li> </ul> <code>Dict[str, Any]</code> <ul> <li>per_class: Precision, Recall, F1, Support per class</li> </ul> <code>Dict[str, Any]</code> <ul> <li>confusion_matrix: Raw confusion matrix</li> </ul> Source code in <code>src\\evaluation\\metrics.py</code> <pre><code>def compute_all_metrics(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    y_pred_proba: np.ndarray,\n    task: str,\n    class_names: Optional[List[str]] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Compute comprehensive metrics for a single experiment.\n\n    Args:\n        y_true: Ground truth labels\n        y_pred: Predicted class labels\n        y_pred_proba: Prediction probabilities (n_samples, n_classes)\n        task: 'binary' or 'multi'\n        class_names: Optional list of class names for reporting\n\n    Returns:\n        Dictionary containing:\n        - overall: Accuracy, Macro F1, Weighted F1, G-Mean, ROC-AUC\n        - per_class: Precision, Recall, F1, Support per class\n        - confusion_matrix: Raw confusion matrix\n    \"\"\"\n    metrics = {\n        'overall': {},\n        'per_class': {},\n        'confusion_matrix': None\n    }\n\n    # Overall metrics\n    metrics['overall']['accuracy'] = float(accuracy_score(y_true, y_pred))\n    metrics['overall']['macro_f1'] = float(f1_score(y_true, y_pred, average='macro', zero_division=0))\n    metrics['overall']['weighted_f1'] = float(f1_score(y_true, y_pred, average='weighted', zero_division=0))\n\n    # G-Mean (primary metric for imbalanced data)\n    try:\n        metrics['overall']['g_mean'] = float(geometric_mean_score(y_true, y_pred, average='macro'))\n    except Exception as e:\n        logger.warning(f\"G-Mean calculation failed: {e}\")\n        metrics['overall']['g_mean'] = 0.0\n\n    # ROC-AUC\n    try:\n        if task == 'binary':\n            # For binary, use probability of positive class\n            if y_pred_proba.ndim == 2:\n                proba = y_pred_proba[:, 1]\n            else:\n                proba = y_pred_proba\n            metrics['overall']['roc_auc'] = float(roc_auc_score(y_true, proba))\n        else:\n            # For multiclass, use One-vs-Rest with macro averaging\n            metrics['overall']['roc_auc'] = float(roc_auc_score(\n                y_true, y_pred_proba,\n                multi_class='ovr',\n                average='macro'\n            ))\n    except ValueError as e:\n        # Can fail if a class has no samples in y_true\n        logger.warning(f\"ROC-AUC calculation failed: {e}\")\n        metrics['overall']['roc_auc'] = None\n\n    # Per-class metrics using classification_report\n    report = classification_report(\n        y_true, y_pred,\n        output_dict=True,\n        zero_division=0,\n        target_names=class_names\n    )\n\n    for class_key, class_metrics in report.items():\n        # Skip summary rows\n        if class_key in ['accuracy', 'macro avg', 'weighted avg']:\n            continue\n\n        metrics['per_class'][str(class_key)] = {\n            'precision': float(class_metrics['precision']),\n            'recall': float(class_metrics['recall']),\n            'f1': float(class_metrics['f1-score']),\n            'support': int(class_metrics['support'])\n        }\n\n    # Confusion matrix\n    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred).tolist()\n\n    return metrics\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.metrics.compute_rare_class_analysis","title":"<code>compute_rare_class_analysis(metrics, rare_classes, label_mapping=None)</code>","text":"<p>Extract performance metrics specifically for rare attack classes.</p> <p>This is a key differentiator of the study - focusing on classes that are typically ignored in aggregate metrics.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Dict[str, Any]</code> <p>Full metrics dictionary from compute_all_metrics</p> required <code>rare_classes</code> <code>List[str]</code> <p>List of rare class names (e.g., ['Worms', 'Shellcode'])</p> required <code>label_mapping</code> <code>Optional[Dict[str, int]]</code> <p>Optional mapping from class name to index</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, float]]</code> <p>Dictionary with metrics for each rare class:</p> <code>Dict[str, Dict[str, float]]</code> <p>{ 'Worms': {'precision': 0.xx, 'recall': 0.xx, 'f1': 0.xx, 'support': n}, ...</p> <code>Dict[str, Dict[str, float]]</code> <p>}</p> Source code in <code>src\\evaluation\\metrics.py</code> <pre><code>def compute_rare_class_analysis(\n    metrics: Dict[str, Any],\n    rare_classes: List[str],\n    label_mapping: Optional[Dict[str, int]] = None\n) -&gt; Dict[str, Dict[str, float]]:\n    \"\"\"\n    Extract performance metrics specifically for rare attack classes.\n\n    This is a key differentiator of the study - focusing on classes\n    that are typically ignored in aggregate metrics.\n\n    Args:\n        metrics: Full metrics dictionary from compute_all_metrics\n        rare_classes: List of rare class names (e.g., ['Worms', 'Shellcode'])\n        label_mapping: Optional mapping from class name to index\n\n    Returns:\n        Dictionary with metrics for each rare class:\n        {\n            'Worms': {'precision': 0.xx, 'recall': 0.xx, 'f1': 0.xx, 'support': n},\n            ...\n        }\n    \"\"\"\n    rare_analysis = {}\n\n    for cls in rare_classes:\n        # Try to find the class in per_class metrics\n        # It might be stored by name or by index\n        cls_key = str(cls)\n        if label_mapping and cls in label_mapping:\n            cls_idx_key = str(label_mapping[cls])\n        else:\n            cls_idx_key = cls_key\n\n        # Check both name and index keys\n        if cls_key in metrics['per_class']:\n            rare_analysis[cls] = metrics['per_class'][cls_key]\n        elif cls_idx_key in metrics['per_class']:\n            rare_analysis[cls] = metrics['per_class'][cls_idx_key]\n        else:\n            logger.warning(f\"Rare class '{cls}' not found in per_class metrics\")\n            rare_analysis[cls] = {\n                'precision': 0.0,\n                'recall': 0.0,\n                'f1': 0.0,\n                'support': 0\n            }\n\n    return rare_analysis\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.metrics.format_metrics_for_logging","title":"<code>format_metrics_for_logging(metrics, task)</code>","text":"<p>Format metrics as a readable string for logging.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Dict[str, Any]</code> <p>Metrics dictionary</p> required <code>task</code> <code>str</code> <p>'binary' or 'multi'</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string for logging</p> Source code in <code>src\\evaluation\\metrics.py</code> <pre><code>def format_metrics_for_logging(metrics: Dict[str, Any], task: str) -&gt; str:\n    \"\"\"\n    Format metrics as a readable string for logging.\n\n    Args:\n        metrics: Metrics dictionary\n        task: 'binary' or 'multi'\n\n    Returns:\n        Formatted string for logging\n    \"\"\"\n    overall = metrics['overall']\n\n    lines = [\n        f\"Accuracy:    {overall['accuracy']:.4f}\",\n        f\"Macro F1:    {overall['macro_f1']:.4f}\",\n        f\"Weighted F1: {overall['weighted_f1']:.4f}\",\n        f\"G-Mean:      {overall['g_mean']:.4f}\",\n    ]\n\n    if overall['roc_auc'] is not None:\n        lines.append(f\"ROC-AUC:     {overall['roc_auc']:.4f}\")\n    else:\n        lines.append(\"ROC-AUC:     N/A\")\n\n    return '\\n'.join(lines)\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.visualizer","title":"<code>src.evaluation.visualizer</code>","text":""},{"location":"api/evaluation/#src.evaluation.visualizer.PublicationVisualizer","title":"<code>PublicationVisualizer</code>","text":"<p>Generates \"Wow\"-factor, publication-ready visualizations for ML papers. Focuses on aesthetics, clarity, and deep insight.</p> Source code in <code>src\\evaluation\\visualizer.py</code> <pre><code>class PublicationVisualizer:\n    \"\"\"\n    Generates \"Wow\"-factor, publication-ready visualizations for ML papers.\n    Focuses on aesthetics, clarity, and deep insight.\n    \"\"\"\n\n    def __init__(self, style: str = 'seaborn-v0_8-whitegrid'):\n        self.style = style\n        plt.style.use(style)\n        self._set_custom_rc()\n\n        # Premium pairings\n        self.colors = {\n            'primary': '#2C3E50',    # Dark Blue\n            'secondary': '#E74C3C',  # Red\n            'accent': '#3498DB',     # Bright Blue\n            'success': '#27AE60',    # Green\n            'purple': '#8E44AD',     # Wisteria\n            'orange': '#D35400',     # Pumpkin\n        }\n\n    def _set_custom_rc(self):\n        \"\"\"Configure matplotlib for high-DPI, professional output.\"\"\"\n        plt.rcParams.update({\n            'font.family': 'sans-serif',\n            'font.sans-serif': ['Arial', 'DejaVu Sans', 'Liberation Sans'],\n            'font.size': 11,\n            'axes.titlesize': 14,\n            'axes.titleweight': 'bold',\n            'axes.labelsize': 12,\n            'axes.labelweight': 'bold',\n            'xtick.labelsize': 10,\n            'ytick.labelsize': 10,\n            'legend.fontsize': 10,\n            'figure.dpi': 300,\n            'savefig.dpi': 300,\n            'lines.linewidth': 2.5,\n            'lines.markersize': 8,\n            'axes.grid': True,\n            'grid.alpha': 0.3,\n            'axes.spines.top': False,\n            'axes.spines.right': False,\n        })\n\n    def plot_radar_chart(self, \n                        data: Dict[str, Dict[str, float]], \n                        metrics: List[str], \n                        save_path: str,\n                        title: str = \"Model Comparison\"):\n        \"\"\"\n        Create a radar chart comparing multiple models across metrics.\n\n        Args:\n            data: {model_name: {metric: value}}\n            metrics: List of metrics to plot axes for\n            save_path: Output file path\n        \"\"\"\n        N = len(metrics)\n        angles = [n / float(N) * 2 * pi for n in range(N)]\n        angles += angles[:1]  # Close the loop\n\n        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'projection': 'polar'})\n\n        # Draw one axe per variable + labels\n        plt.xticks(angles[:-1], [m.replace('_', ' ').title() for m in metrics], color='black', size=10)\n\n        # Draw ylabels\n        ax.set_rlabel_position(0)\n        plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], [\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], color=\"grey\", size=8)\n        plt.ylim(0, 1)\n\n        # Plot data\n        palette = sns.color_palette(\"husl\", len(data))\n\n        for idx, (label, metrics_dict) in enumerate(data.items()):\n            values = [metrics_dict.get(m, 0.0) for m in metrics]\n            values += values[:1]  # Close loop\n\n            ax.plot(angles, values, linewidth=2, linestyle='solid', label=label, color=palette[idx])\n            ax.fill(angles, values, color=palette[idx], alpha=0.1)\n\n        plt.title(title, size=16, color=self.colors['primary'], y=1.1)\n        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n\n        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n        plt.savefig(save_path, bbox_inches='tight')\n        plt.close()\n        logger.info(f\"Saved radar chart to {save_path}\")\n\n    def plot_critical_difference_proxy(self, \n                                     ranks: Dict[str, float], \n                                     save_path: str):\n        \"\"\"\n        Visualizes average rank of strategies (Proxy for Critical Difference Diagram).\n        Real CD diagrams require statistical tests, this creates a beautiful rank comparison.\n        \"\"\"\n        sorted_ranks = sorted(ranks.items(), key=lambda item: item[1])\n        names = [x[0] for x in sorted_ranks]\n        values = [x[1] for x in sorted_ranks]\n\n        fig, ax = plt.subplots(figsize=(10, 4))\n\n        # Create a horizontal line\n        ax.hlines(y=1, xmin=0.5, xmax=len(ranks)+0.5, color='gray', alpha=0.5, linewidth=2)\n\n        # Plot points on the line\n        # We normalize ranks to map onto the line nicely, actually let's just do a bar chart rank\n        # A simple bar chart is clearer if we aren't doing the Nemenyi test cliques\n        plt.close()\n\n        # Fallback to horizontal bar chart for ranks (Lower is better)\n        fig, ax = plt.subplots(figsize=(10, 6))\n        bars = ax.barh(names, values, color=self.colors['accent'])\n\n        ax.set_xlabel('Average Rank (Lower is Better)')\n        ax.set_title(\"Strategy Ranking Comparison\", fontsize=14)\n        ax.invert_yaxis()  # Best on top\n\n        # Value labels\n        for bar in bars:\n            width = bar.get_width()\n            ax.text(width + 0.05, bar.get_y() + bar.get_height()/2, \n                    f'{width:.2f}', ha='left', va='center')\n\n        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n        plt.savefig(save_path, bbox_inches='tight')\n        plt.close()\n        logger.info(f\"Saved rank comparison to {save_path}\")\n\n    def plot_rare_class_heatmap_grid(self, \n                                     df_results: pd.DataFrame, \n                                     rare_classes: List[str],\n                                     save_path: str):\n        \"\"\"\n        Creates a faceted heatmap specifically for rare classes.\n        Input DF must have columns: model, strategy, [class_names...]\n        \"\"\"\n        # Pivot functionality would be specific to how we parse the CSV.\n        # This is a placeholder for the logic in generate_report.py\n        pass\n\n    def plot_confusion_matrix_styled(self, cm, classes, save_path, title='Confusion Matrix'):\n        \"\"\"\n        Premium styled confusion matrix.\n        \"\"\"\n        fig, ax = plt.subplots(figsize=(10, 8))\n\n        # Normalize\n        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n        sns.heatmap(cm_norm, annot=True, fmt='.1%', cmap='RdPu', \n                    xticklabels=classes, yticklabels=classes,\n                    cbar=True, square=True, linewidths=.5, linecolor='white')\n\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.title(title, pad=20)\n\n        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n        plt.savefig(save_path, bbox_inches='tight')\n        plt.close()\n        logger.info(f\"Saved styled CM to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.visualizer.PublicationVisualizer.plot_confusion_matrix_styled","title":"<code>plot_confusion_matrix_styled(cm, classes, save_path, title='Confusion Matrix')</code>","text":"<p>Premium styled confusion matrix.</p> Source code in <code>src\\evaluation\\visualizer.py</code> <pre><code>def plot_confusion_matrix_styled(self, cm, classes, save_path, title='Confusion Matrix'):\n    \"\"\"\n    Premium styled confusion matrix.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    # Normalize\n    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    sns.heatmap(cm_norm, annot=True, fmt='.1%', cmap='RdPu', \n                xticklabels=classes, yticklabels=classes,\n                cbar=True, square=True, linewidths=.5, linecolor='white')\n\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.title(title, pad=20)\n\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight')\n    plt.close()\n    logger.info(f\"Saved styled CM to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.visualizer.PublicationVisualizer.plot_critical_difference_proxy","title":"<code>plot_critical_difference_proxy(ranks, save_path)</code>","text":"<p>Visualizes average rank of strategies (Proxy for Critical Difference Diagram). Real CD diagrams require statistical tests, this creates a beautiful rank comparison.</p> Source code in <code>src\\evaluation\\visualizer.py</code> <pre><code>def plot_critical_difference_proxy(self, \n                                 ranks: Dict[str, float], \n                                 save_path: str):\n    \"\"\"\n    Visualizes average rank of strategies (Proxy for Critical Difference Diagram).\n    Real CD diagrams require statistical tests, this creates a beautiful rank comparison.\n    \"\"\"\n    sorted_ranks = sorted(ranks.items(), key=lambda item: item[1])\n    names = [x[0] for x in sorted_ranks]\n    values = [x[1] for x in sorted_ranks]\n\n    fig, ax = plt.subplots(figsize=(10, 4))\n\n    # Create a horizontal line\n    ax.hlines(y=1, xmin=0.5, xmax=len(ranks)+0.5, color='gray', alpha=0.5, linewidth=2)\n\n    # Plot points on the line\n    # We normalize ranks to map onto the line nicely, actually let's just do a bar chart rank\n    # A simple bar chart is clearer if we aren't doing the Nemenyi test cliques\n    plt.close()\n\n    # Fallback to horizontal bar chart for ranks (Lower is better)\n    fig, ax = plt.subplots(figsize=(10, 6))\n    bars = ax.barh(names, values, color=self.colors['accent'])\n\n    ax.set_xlabel('Average Rank (Lower is Better)')\n    ax.set_title(\"Strategy Ranking Comparison\", fontsize=14)\n    ax.invert_yaxis()  # Best on top\n\n    # Value labels\n    for bar in bars:\n        width = bar.get_width()\n        ax.text(width + 0.05, bar.get_y() + bar.get_height()/2, \n                f'{width:.2f}', ha='left', va='center')\n\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight')\n    plt.close()\n    logger.info(f\"Saved rank comparison to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.visualizer.PublicationVisualizer.plot_radar_chart","title":"<code>plot_radar_chart(data, metrics, save_path, title='Model Comparison')</code>","text":"<p>Create a radar chart comparing multiple models across metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Dict[str, float]]</code> <p>{model_name: {metric: value}}</p> required <code>metrics</code> <code>List[str]</code> <p>List of metrics to plot axes for</p> required <code>save_path</code> <code>str</code> <p>Output file path</p> required Source code in <code>src\\evaluation\\visualizer.py</code> <pre><code>def plot_radar_chart(self, \n                    data: Dict[str, Dict[str, float]], \n                    metrics: List[str], \n                    save_path: str,\n                    title: str = \"Model Comparison\"):\n    \"\"\"\n    Create a radar chart comparing multiple models across metrics.\n\n    Args:\n        data: {model_name: {metric: value}}\n        metrics: List of metrics to plot axes for\n        save_path: Output file path\n    \"\"\"\n    N = len(metrics)\n    angles = [n / float(N) * 2 * pi for n in range(N)]\n    angles += angles[:1]  # Close the loop\n\n    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'projection': 'polar'})\n\n    # Draw one axe per variable + labels\n    plt.xticks(angles[:-1], [m.replace('_', ' ').title() for m in metrics], color='black', size=10)\n\n    # Draw ylabels\n    ax.set_rlabel_position(0)\n    plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], [\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], color=\"grey\", size=8)\n    plt.ylim(0, 1)\n\n    # Plot data\n    palette = sns.color_palette(\"husl\", len(data))\n\n    for idx, (label, metrics_dict) in enumerate(data.items()):\n        values = [metrics_dict.get(m, 0.0) for m in metrics]\n        values += values[:1]  # Close loop\n\n        ax.plot(angles, values, linewidth=2, linestyle='solid', label=label, color=palette[idx])\n        ax.fill(angles, values, color=palette[idx], alpha=0.1)\n\n    plt.title(title, size=16, color=self.colors['primary'], y=1.1)\n    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight')\n    plt.close()\n    logger.info(f\"Saved radar chart to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.visualizer.PublicationVisualizer.plot_rare_class_heatmap_grid","title":"<code>plot_rare_class_heatmap_grid(df_results, rare_classes, save_path)</code>","text":"<p>Creates a faceted heatmap specifically for rare classes. Input DF must have columns: model, strategy, [class_names...]</p> Source code in <code>src\\evaluation\\visualizer.py</code> <pre><code>def plot_rare_class_heatmap_grid(self, \n                                 df_results: pd.DataFrame, \n                                 rare_classes: List[str],\n                                 save_path: str):\n    \"\"\"\n    Creates a faceted heatmap specifically for rare classes.\n    Input DF must have columns: model, strategy, [class_names...]\n    \"\"\"\n    # Pivot functionality would be specific to how we parse the CSV.\n    # This is a placeholder for the logic in generate_report.py\n    pass\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.plots","title":"<code>src.evaluation.plots</code>","text":"<p>Professional visualization functions for UNSW-NB15 experiment results.</p> <p>This module provides publication-quality visualizations including: - Confusion matrix heatmaps - Strategy comparison bar charts - Rare class recall analysis - Class distribution plots</p> <p>All figures use consistent professional styling suitable for research papers.</p>"},{"location":"api/evaluation/#src.evaluation.plots.plot_class_distribution","title":"<code>plot_class_distribution(class_counts, save_path, title='Class Distribution')</code>","text":"<p>Create bar chart showing class distribution.</p> <p>Parameters:</p> Name Type Description Default <code>class_counts</code> <code>Dict[str, int]</code> <p>Dictionary {class_name: count}</p> required <code>save_path</code> <code>str</code> <p>Path to save figure</p> required <code>title</code> <code>str</code> <p>Plot title</p> <code>'Class Distribution'</code> Source code in <code>src\\evaluation\\plots.py</code> <pre><code>def plot_class_distribution(\n    class_counts: Dict[str, int],\n    save_path: str,\n    title: str = 'Class Distribution'\n) -&gt; None:\n    \"\"\"\n    Create bar chart showing class distribution.\n\n    Args:\n        class_counts: Dictionary {class_name: count}\n        save_path: Path to save figure\n        title: Plot title\n    \"\"\"\n    set_plot_style()\n\n    classes = list(class_counts.keys())\n    counts = list(class_counts.values())\n\n    # Sort by count descending\n    sorted_pairs = sorted(zip(classes, counts), key=lambda x: x[1], reverse=True)\n    classes, counts = zip(*sorted_pairs)\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    # Color rare classes differently\n    colors = ['#E74C3C' if c &lt; 3000 else '#3498DB' for c in counts]\n\n    bars = ax.bar(classes, counts, color=colors, edgecolor='white', linewidth=0.5)\n\n    # Add count labels\n    for bar, count in zip(bars, counts):\n        ax.annotate(\n            f'{count:,}',\n            xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n            ha='center', va='bottom',\n            fontsize=9,\n            fontweight='bold'\n        )\n\n    ax.set_xlabel('Attack Category', fontweight='bold')\n    ax.set_ylabel('Sample Count', fontweight='bold')\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.set_yscale('log')  # Log scale for better visibility of rare classes\n\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight', facecolor='white')\n    plt.close()\n\n    logger.info(f\"Saved class distribution to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.plots.plot_confusion_matrix","title":"<code>plot_confusion_matrix(cm, labels, save_path, title='Confusion Matrix', normalize=True, figsize=(10, 8), cmap='Blues')</code>","text":"<p>Generate professional confusion matrix heatmap.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>ndarray</code> <p>Confusion matrix array (n_classes, n_classes)</p> required <code>labels</code> <code>List[str]</code> <p>Class labels</p> required <code>save_path</code> <code>str</code> <p>Path to save the figure</p> required <code>title</code> <code>str</code> <p>Plot title</p> <code>'Confusion Matrix'</code> <code>normalize</code> <code>bool</code> <p>If True, show percentages; if False, show counts</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure dimensions</p> <code>(10, 8)</code> <code>cmap</code> <code>str</code> <p>Colormap name</p> <code>'Blues'</code> Source code in <code>src\\evaluation\\plots.py</code> <pre><code>def plot_confusion_matrix(\n    cm: np.ndarray,\n    labels: List[str],\n    save_path: str,\n    title: str = 'Confusion Matrix',\n    normalize: bool = True,\n    figsize: tuple = (10, 8),\n    cmap: str = 'Blues'\n) -&gt; None:\n    \"\"\"\n    Generate professional confusion matrix heatmap.\n\n    Args:\n        cm: Confusion matrix array (n_classes, n_classes)\n        labels: Class labels\n        save_path: Path to save the figure\n        title: Plot title\n        normalize: If True, show percentages; if False, show counts\n        figsize: Figure dimensions\n        cmap: Colormap name\n    \"\"\"\n    set_plot_style()\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Normalize if requested\n    if normalize:\n        # Avoid division by zero\n        row_sums = cm.sum(axis=1, keepdims=True)\n        row_sums[row_sums == 0] = 1\n        cm_display = cm.astype('float') / row_sums\n        fmt = '.1%'\n        vmax = 1.0\n    else:\n        cm_display = cm\n        fmt = 'd'\n        vmax = None\n\n    # Create heatmap\n    sns.heatmap(\n        cm_display,\n        annot=True,\n        fmt=fmt,\n        cmap=cmap,\n        xticklabels=labels,\n        yticklabels=labels,\n        ax=ax,\n        vmin=0,\n        vmax=vmax,\n        cbar_kws={'label': 'Proportion' if normalize else 'Count'},\n        square=True,\n        linewidths=0.5\n    )\n\n    ax.set_xlabel('Predicted Label', fontweight='bold')\n    ax.set_ylabel('True Label', fontweight='bold')\n    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n\n    # Rotate x-axis labels for multiclass\n    if len(labels) &gt; 4:\n        plt.xticks(rotation=45, ha='right')\n        plt.yticks(rotation=0)\n\n    plt.tight_layout()\n\n    # Save figure\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight', facecolor='white')\n    plt.close()\n\n    logger.info(f\"Saved confusion matrix to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.plots.plot_feature_importance","title":"<code>plot_feature_importance(importances, feature_names, save_path, title='Feature Importance', top_n=20)</code>","text":"<p>Plot top N feature importances.</p> <p>Parameters:</p> Name Type Description Default <code>importances</code> <code>ndarray</code> <p>Array of feature importance scores</p> required <code>feature_names</code> <code>List[str]</code> <p>List of feature names</p> required <code>save_path</code> <code>str</code> <p>Path to save figure</p> required <code>title</code> <code>str</code> <p>Plot title</p> <code>'Feature Importance'</code> <code>top_n</code> <code>int</code> <p>Number of top features to show</p> <code>20</code> Source code in <code>src\\evaluation\\plots.py</code> <pre><code>def plot_feature_importance(\n    importances: np.ndarray,\n    feature_names: List[str],\n    save_path: str,\n    title: str = 'Feature Importance',\n    top_n: int = 20\n) -&gt; None:\n    \"\"\"\n    Plot top N feature importances.\n\n    Args:\n        importances: Array of feature importance scores\n        feature_names: List of feature names\n        save_path: Path to save figure\n        title: Plot title\n        top_n: Number of top features to show\n    \"\"\"\n    set_plot_style()\n\n    # Sort importances\n    indices = np.argsort(importances)[::-1]\n\n    # Select top N\n    top_indices = indices[:top_n]\n    top_importances = importances[top_indices]\n    top_names = [feature_names[i] for i in top_indices]\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n\n    y_pos = np.arange(len(top_names))\n    ax.barh(y_pos, top_importances, align='center', color='#34495E')\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(top_names)\n    ax.invert_yaxis()  # labels read top-to-bottom\n\n    ax.set_xlabel('Importance Score', fontweight='bold')\n    ax.set_title(title, fontsize=14, fontweight='bold')\n\n    plt.tight_layout()\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight', facecolor='white')\n    plt.close()\n    logger.info(f\"Saved feature importance plot to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.plots.plot_learning_curves","title":"<code>plot_learning_curves(learning_curve_data, save_path, title='Learning Curves')</code>","text":"<p>Plot learning curves (Training vs Validation) for available metrics.</p> <p>Parameters:</p> Name Type Description Default <code>learning_curve_data</code> <code>Dict[str, Dict[str, List[float]]]</code> <p>Nested dict {set_name: {metric_name: [values]}}                  e.g. {'validation_0': {'logloss': [...]}, ...}</p> required <code>save_path</code> <code>str</code> <p>Path to save figure</p> required <code>title</code> <code>str</code> <p>Plot title</p> <code>'Learning Curves'</code> Source code in <code>src\\evaluation\\plots.py</code> <pre><code>def plot_learning_curves(\n    learning_curve_data: Dict[str, Dict[str, List[float]]],\n    save_path: str,\n    title: str = 'Learning Curves'\n) -&gt; None:\n    \"\"\"\n    Plot learning curves (Training vs Validation) for available metrics.\n\n    Args:\n        learning_curve_data: Nested dict {set_name: {metric_name: [values]}}\n                             e.g. {'validation_0': {'logloss': [...]}, ...}\n        save_path: Path to save figure\n        title: Plot title\n    \"\"\"\n    set_plot_style()\n\n    # Identify metrics available\n    first_set = next(iter(learning_curve_data.values()))\n    metrics = list(first_set.keys())\n\n    fig, axes = plt.subplots(1, len(metrics), figsize=(7 * len(metrics), 5))\n    if len(metrics) == 1:\n        axes = [axes]\n\n    mapping = {'validation_0': 'Train', 'validation_1': 'Validation'}\n    colors = {'validation_0': '#3498DB', 'validation_1': '#E74C3C'}\n\n    for i, metric in enumerate(metrics):\n        ax = axes[i]\n\n        for set_key, metrics_dict in learning_curve_data.items():\n            if metric in metrics_dict:\n                values = metrics_dict[metric]\n                epochs = range(1, len(values) + 1)\n                label = mapping.get(set_key, set_key)\n\n                ax.plot(epochs, values, label=label, linewidth=2, \n                        color=colors.get(set_key, f'C{i}'))\n\n        ax.set_xlabel('Epochs')\n        ax.set_ylabel(metric.title())\n        ax.set_title(f'{metric.title()}')\n        ax.legend()\n        ax.grid(True, linestyle='--', alpha=0.3)\n\n    plt.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n    plt.tight_layout()\n\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight', facecolor='white')\n    plt.close()\n\n    logger.info(f\"Saved learning curve plot to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.plots.plot_metric_comparison_grid","title":"<code>plot_metric_comparison_grid(results, metrics, save_path)</code>","text":"<p>Create a grid of comparison plots for multiple metrics.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Dict[str, Any]]</code> <p>List of experiment result dictionaries</p> required <code>metrics</code> <code>List[str]</code> <p>List of metric names to plot</p> required <code>save_path</code> <code>str</code> <p>Path to save figure</p> required Source code in <code>src\\evaluation\\plots.py</code> <pre><code>def plot_metric_comparison_grid(\n    results: List[Dict[str, Any]],\n    metrics: List[str],\n    save_path: str\n) -&gt; None:\n    \"\"\"\n    Create a grid of comparison plots for multiple metrics.\n\n    Args:\n        results: List of experiment result dictionaries\n        metrics: List of metric names to plot\n        save_path: Path to save figure\n    \"\"\"\n    set_plot_style()\n\n    n_metrics = len(metrics)\n    fig, axes = plt.subplots(2, (n_metrics + 1) // 2, figsize=(14, 10))\n    axes = axes.flatten()\n\n    for i, metric in enumerate(metrics):\n        ax = axes[i]\n\n        # Group results by task\n        for task in ['binary', 'multi']:\n            task_results = [r for r in results if r['task'] == task]\n            models = [r['model'] for r in task_results]\n            values = [r['metrics']['overall'].get(metric, 0) for r in task_results]\n\n            ax.scatter(range(len(values)), values, label=task, alpha=0.7)\n\n        ax.set_title(metric.replace('_', ' ').title())\n        ax.set_ylim(0, 1.1)\n        ax.legend()\n\n    # Hide unused axes\n    for i in range(n_metrics, len(axes)):\n        axes[i].set_visible(False)\n\n    plt.suptitle('Metric Comparison Across Experiments', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight', facecolor='white')\n    plt.close()\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.plots.plot_pr_curve","title":"<code>plot_pr_curve(y_true, y_scores, save_path, title='Precision-Recall Curve')</code>","text":"<p>Plot Precision-Recall curve (crucial for imbalanced data).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>True binary labels</p> required <code>y_scores</code> <code>ndarray</code> <p>Predicted probabilities for the positive class</p> required <code>save_path</code> <code>str</code> <p>Path to save figure</p> required <code>title</code> <code>str</code> <p>Plot title</p> <code>'Precision-Recall Curve'</code> Source code in <code>src\\evaluation\\plots.py</code> <pre><code>def plot_pr_curve(\n    y_true: np.ndarray,\n    y_scores: np.ndarray,\n    save_path: str,\n    title: str = 'Precision-Recall Curve'\n) -&gt; None:\n    \"\"\"\n    Plot Precision-Recall curve (crucial for imbalanced data).\n\n    Args:\n        y_true: True binary labels\n        y_scores: Predicted probabilities for the positive class\n        save_path: Path to save figure\n        title: Plot title\n    \"\"\"\n    from sklearn.metrics import precision_recall_curve, average_precision_score\n\n    set_plot_style()\n    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n    average_precision = average_precision_score(y_true, y_scores)\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    ax.plot(recall, precision, color='#9B59B6', lw=2,\n            label=f'AP = {average_precision:.3f}')\n\n    ax.set_xlabel('Recall', fontweight='bold')\n    ax.set_ylabel('Precision', fontweight='bold')\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.legend(loc=\"lower left\")\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlim([0.0, 1.0])\n\n    plt.tight_layout()\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight', facecolor='white')\n    plt.close()\n    logger.info(f\"Saved PR curve to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.plots.plot_rare_class_recall","title":"<code>plot_rare_class_recall(results, rare_classes, save_path)</code>","text":"<p>Create heatmap showing rare class recall across experiments.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Dict[str, Dict[str, Dict[str, float]]]</code> <p>Nested dict {experiment_id: {class_name: {'recall': value}}}</p> required <code>rare_classes</code> <code>List[str]</code> <p>List of rare class names</p> required <code>save_path</code> <code>str</code> <p>Path to save figure</p> required Source code in <code>src\\evaluation\\plots.py</code> <pre><code>def plot_rare_class_recall(\n    results: Dict[str, Dict[str, Dict[str, float]]],\n    rare_classes: List[str],\n    save_path: str\n) -&gt; None:\n    \"\"\"\n    Create heatmap showing rare class recall across experiments.\n\n    Args:\n        results: Nested dict {experiment_id: {class_name: {'recall': value}}}\n        rare_classes: List of rare class names\n        save_path: Path to save figure\n    \"\"\"\n    set_plot_style()\n\n    # Build recall matrix\n    experiments = list(results.keys())\n    recall_matrix = np.zeros((len(rare_classes), len(experiments)))\n\n    for j, exp in enumerate(experiments):\n        for i, cls in enumerate(rare_classes):\n            if cls in results[exp]:\n                recall_matrix[i, j] = results[exp][cls].get('recall', 0)\n\n    fig, ax = plt.subplots(figsize=(14, 6))\n\n    sns.heatmap(\n        recall_matrix,\n        annot=True,\n        fmt='.2f',\n        cmap='RdYlGn',\n        xticklabels=experiments,\n        yticklabels=rare_classes,\n        ax=ax,\n        vmin=0,\n        vmax=1,\n        cbar_kws={'label': 'Recall'},\n        linewidths=0.5\n    )\n\n    ax.set_xlabel('Experiment', fontweight='bold')\n    ax.set_ylabel('Rare Class', fontweight='bold')\n    ax.set_title('Rare Class Recall Across Experiments',\n                 fontsize=14, fontweight='bold', pad=20)\n\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight', facecolor='white')\n    plt.close()\n\n    logger.info(f\"Saved rare class recall heatmap to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.plots.plot_roc_curve","title":"<code>plot_roc_curve(y_true, y_scores, save_path, title='ROC Curve')</code>","text":"<p>Plot Receiver Operating Characteristic (ROC) curve.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>True binary labels</p> required <code>y_scores</code> <code>ndarray</code> <p>Predicted probabilities for the positive class</p> required <code>save_path</code> <code>str</code> <p>Path to save figure</p> required <code>title</code> <code>str</code> <p>Plot title</p> <code>'ROC Curve'</code> Source code in <code>src\\evaluation\\plots.py</code> <pre><code>def plot_roc_curve(\n    y_true: np.ndarray,\n    y_scores: np.ndarray,\n    save_path: str,\n    title: str = 'ROC Curve'\n) -&gt; None:\n    \"\"\"\n    Plot Receiver Operating Characteristic (ROC) curve.\n\n    Args:\n        y_true: True binary labels\n        y_scores: Predicted probabilities for the positive class\n        save_path: Path to save figure\n        title: Plot title\n    \"\"\"\n    from sklearn.metrics import roc_curve, auc\n\n    set_plot_style()\n    fpr, tpr, _ = roc_curve(y_true, y_scores)\n    roc_auc = auc(fpr, tpr)\n\n    fig, ax = plt.subplots(figsize=(8, 8))\n\n    ax.plot(fpr, tpr, color='#2ECC71', lw=2,\n            label=f'ROC curve (area = {roc_auc:.3f})')\n    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate', fontweight='bold')\n    ax.set_ylabel('True Positive Rate', fontweight='bold')\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.legend(loc=\"lower right\")\n\n    plt.tight_layout()\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight', facecolor='white')\n    plt.close()\n    logger.info(f\"Saved ROC curve to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.plots.plot_strategy_comparison","title":"<code>plot_strategy_comparison(results, metric, save_path, title=None)</code>","text":"<p>Create grouped bar chart comparing strategies across models.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Dict[str, Dict[str, float]]</code> <p>Nested dict {model: {strategy: metric_value}}</p> required <code>metric</code> <code>str</code> <p>Name of metric being compared</p> required <code>save_path</code> <code>str</code> <p>Path to save figure</p> required <code>title</code> <code>Optional[str]</code> <p>Optional custom title</p> <code>None</code> Source code in <code>src\\evaluation\\plots.py</code> <pre><code>def plot_strategy_comparison(\n    results: Dict[str, Dict[str, float]],\n    metric: str,\n    save_path: str,\n    title: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Create grouped bar chart comparing strategies across models.\n\n    Args:\n        results: Nested dict {model: {strategy: metric_value}}\n        metric: Name of metric being compared\n        save_path: Path to save figure\n        title: Optional custom title\n    \"\"\"\n    set_plot_style()\n\n    models = list(results.keys())\n    strategies = list(results[models[0]].keys())\n\n    x = np.arange(len(models))\n    width = 0.25\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    for i, strategy in enumerate(strategies):\n        values = [results[model][strategy] for model in models]\n        offset = (i - len(strategies)/2 + 0.5) * width\n        bars = ax.bar(\n            x + offset, values, width,\n            label=strategy.upper(),\n            color=STRATEGY_COLORS.get(strategy, f'C{i}'),\n            edgecolor='white',\n            linewidth=0.5\n        )\n\n        # Add value labels on bars\n        for bar, val in zip(bars, values):\n            ax.annotate(\n                f'{val:.3f}',\n                xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n                ha='center', va='bottom',\n                fontsize=8\n            )\n\n    ax.set_xlabel('Model', fontweight='bold')\n    ax.set_ylabel(metric.replace('_', ' ').title(), fontweight='bold')\n    ax.set_title(title or f'{metric.replace(\"_\", \" \").title()} by Model and Strategy',\n                 fontsize=14, fontweight='bold')\n    ax.set_xticks(x)\n    ax.set_xticklabels([m.upper() for m in models])\n    ax.legend(title='Strategy', loc='lower right')\n    ax.set_ylim(0, 1.1)\n\n    plt.tight_layout()\n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(save_path, bbox_inches='tight', facecolor='white')\n    plt.close()\n\n    logger.info(f\"Saved strategy comparison to {save_path}\")\n</code></pre>"},{"location":"api/evaluation/#src.evaluation.plots.set_plot_style","title":"<code>set_plot_style()</code>","text":"<p>Set consistent professional plot styling.</p> Source code in <code>src\\evaluation\\plots.py</code> <pre><code>def set_plot_style():\n    \"\"\"Set consistent professional plot styling.\"\"\"\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.rcParams.update({\n        'figure.dpi': 150,\n        'savefig.dpi': 300,\n        'font.size': 10,\n        'axes.titlesize': 12,\n        'axes.labelsize': 10,\n        'xtick.labelsize': 9,\n        'ytick.labelsize': 9,\n        'legend.fontsize': 9,\n        'figure.figsize': (10, 8),\n        'axes.spines.top': False,\n        'axes.spines.right': False\n    })\n</code></pre>"},{"location":"api/models/","title":"Models API","text":""},{"location":"api/models/#src.models.config","title":"<code>src.models.config</code>","text":"<p>Model hyperparameter configurations.</p> <p>This module contains the default hyperparameter configurations for all three model families used in the experiments: - Logistic Regression (LR) - Random Forest (RF) - XGBoost (XGB)</p> <p>Configurations are provided for both binary and multiclass tasks.</p>"},{"location":"api/models/#src.models.config.get_xgb_device","title":"<code>get_xgb_device()</code>","text":"<p>Detect if NVIDIA GPU is available for XGBoost.</p> Source code in <code>src\\models\\config.py</code> <pre><code>def get_xgb_device():\n    \"\"\"Detect if NVIDIA GPU is available for XGBoost.\"\"\"\n    try:\n        subprocess.check_output('nvidia-smi')\n        logger.info(\"NVIDIA GPU detected. Using 'gpu_hist' for XGBoost.\")\n        # Explicit confirmation for Colab user\n        logger.info(f\"\u2705 XGBoost using GPU: {subprocess.check_output('nvidia-smi --query-gpu=name --format=csv,noheader', shell=True).decode().strip()}\")\n        return 'gpu_hist'\n    except Exception:\n        return 'hist'\n</code></pre>"},{"location":"api/models/#src.models.trainer","title":"<code>src.models.trainer</code>","text":"<p>Model training framework for UNSW-NB15 experiments.</p> <p>This module provides a unified interface for training all three model families (LR, RF, XGB) with consistent handling of class weights and sample weights across different imbalance strategies.</p>"},{"location":"api/models/#src.models.trainer.ModelTrainer","title":"<code>ModelTrainer</code>","text":"<p>Unified training interface for all classification models.</p> <p>Handles the complexity of applying class weights and sample weights appropriately for each model type and strategy combination.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration dictionary</p> <code>random_state</code> <p>Random seed for reproducibility</p> Source code in <code>src\\models\\trainer.py</code> <pre><code>class ModelTrainer:\n    \"\"\"\n    Unified training interface for all classification models.\n\n    Handles the complexity of applying class weights and sample weights\n    appropriately for each model type and strategy combination.\n\n    Attributes:\n        config: Configuration dictionary\n        random_state: Random seed for reproducibility\n    \"\"\"\n\n    # Model class mapping\n    MODELS = {\n        'lr': LogisticRegression,\n        'rf': RandomForestClassifier,\n        'xgb': XGBClassifier\n    }\n\n    def __init__(self, config: dict):\n        \"\"\"\n        Initialize ModelTrainer.\n\n        Args:\n            config: Configuration dictionary\n        \"\"\"\n        self.config = config\n        self.random_state = config.get('random_state', 42)\n\n    def get_model(self,\n                  model_name: str,\n                  task: str,\n                  class_weight: Optional[str] = None,\n                  scale_pos_weight: Optional[float] = None,\n                  num_classes: int = 2) -&gt; BaseEstimator:\n        \"\"\"\n        Create and configure a model instance.\n\n        Args:\n            model_name: 'lr', 'rf', or 'xgb'\n            task: 'binary' or 'multi'\n            class_weight: 'balanced' or None (for LR/RF)\n            scale_pos_weight: For XGBoost binary with S1\n            num_classes: Number of classes for multiclass XGBoost\n\n        Returns:\n            Configured but unfitted model instance\n        \"\"\"\n        if model_name not in self.MODELS:\n            raise ValueError(f\"Unknown model: {model_name}. Choose from {list(self.MODELS.keys())}\")\n\n        # Get base configuration\n        model_config = MODEL_CONFIGS[model_name][task].copy()\n        model_config['random_state'] = self.random_state\n\n        # Apply class weight for LR and RF\n        if model_name in ['lr', 'rf'] and class_weight is not None:\n            model_config['class_weight'] = class_weight\n\n        # Apply scale_pos_weight for XGBoost binary\n        if model_name == 'xgb' and task == 'binary' and scale_pos_weight is not None:\n            model_config['scale_pos_weight'] = scale_pos_weight\n\n        # Set num_class for XGBoost multiclass\n        if model_name == 'xgb' and task == 'multi':\n            model_config['num_class'] = num_classes\n\n        # Create model instance\n        model_class = self.MODELS[model_name]\n        model = model_class(**model_config)\n\n        logger.debug(f\"Created {model_name.upper()} for {task} task\")\n        return model\n\n    def train(self,\n              model: BaseEstimator,\n              X_train: np.ndarray,\n              y_train: np.ndarray,\n              sample_weight: Optional[np.ndarray] = None,\n              X_val: Optional[np.ndarray] = None,\n              y_val: Optional[np.ndarray] = None) -&gt; Dict[str, Any]:\n        \"\"\"\n        Train a model and return training metadata with learning curves.\n\n        Dispatches to specialized training methods based on model type:\n        - XGBoost: Uses native early stopping and evals_result\n        - Sklearn (LR/RF): Uses custom iterative warm_start loop\n        \"\"\"\n        logger.info(f\"Training {type(model).__name__} on {len(y_train):,} samples...\")\n        start_time = time.time()\n\n        metadata = {}\n\n        # Dispatch to specific handler\n        if isinstance(model, XGBClassifier):\n            metadata = self._train_xgboost(\n                model, X_train, y_train, sample_weight, X_val, y_val\n            )\n        else:\n            # Check if model supports warm_start for iterative curves\n            if hasattr(model, 'warm_start') and model.warm_start:\n                metadata = self._train_sklearn_iterative(\n                    model, X_train, y_train, sample_weight, X_val, y_val\n                )\n            else:\n                # Fallback for standard training\n                self._fit_standard(model, X_train, y_train, sample_weight)\n\n        training_time = time.time() - start_time\n        logger.info(f\"Training completed in {training_time:.2f}s\")\n\n        # Enforce metadata structure\n        metadata.update({\n            'training_time_seconds': training_time,\n            'n_samples': len(y_train),\n            'n_features': X_train.shape[1]\n        })\n\n        return metadata\n\n    def _train_xgboost(self, model, X_train, y_train, sample_weight, X_val, y_val) -&gt; Dict[str, Any]:\n        \"\"\"Handle XGBoost training with native evaluation monitoring.\"\"\"\n        fit_params = {}\n        if sample_weight is not None:\n            fit_params['sample_weight'] = sample_weight\n\n        if X_val is not None and y_val is not None:\n             fit_params['eval_set'] = [(X_train, y_train), (X_val, y_val)]\n             fit_params['verbose'] = False\n             logger.info(\"Enabled XGBoost native tracking\")\n\n        logger.info(f\"Fitting XGBoost with params: {list(fit_params.keys())}\")\n        model.fit(X_train, y_train, **fit_params)\n\n        metadata = {}\n        try:\n            evals_result = model.evals_result()\n            if evals_result:\n                metadata['learning_curve'] = evals_result\n        except Exception as e:\n            logger.warning(f\"Could not retrieve XGBoost learning curve: {e}\")\n\n        return metadata\n\n    def _train_sklearn_iterative(self, model, X_train, y_train, sample_weight, X_val, y_val) -&gt; Dict[str, Any]:\n        \"\"\"\n        Train sklearn models iteratively using warm_start to generate learning curves.\n\n        Uses a step size of 10 to minimize overhead (proven 1.3x vs 9x for step=1).\n        \"\"\"\n        # Determine iteration parameter and target\n        if isinstance(model, RandomForestClassifier):\n            param_name = 'n_estimators'\n            target_value = model.n_estimators\n            model.n_estimators = 0 # Start from 0\n        elif isinstance(model, LogisticRegression):\n            param_name = 'max_iter'\n            target_value = model.max_iter\n            model.max_iter = 0\n        else:\n            # Should not happen given dispatch logic but safe fallback\n            self._fit_standard(model, X_train, y_train, sample_weight)\n            return {}\n\n        step_size = 10\n        current_step = 0\n\n        # Storage for metrics: {'validation_0': {'logloss': []}, ...}\n        # mimicking XGBoost structure for compatibility with plotting tools\n        history = {\n            'validation_0': {'score': []}, # Train\n            'validation_1': {'score': []}  # Val\n        }\n\n        logger.info(f\"Starting iterative training ({param_name}) -&gt; {target_value}\")\n\n        while current_step &lt; target_value:\n            # Increment\n            next_step = min(current_step + step_size, target_value)\n            setattr(model, param_name, next_step)\n\n            # Fit (warm_start=True ensures it builds on previous)\n            # Log Reg requires unique classes check on first fit usually, but standard fit handles it\n            with warnings.catch_warnings():\n                # Filter benign warning about class_weight + warm_start (we use full dataset, so weights are stable)\n                warnings.filterwarnings(\"ignore\", \".*class_weight presets.*\", category=UserWarning)\n                # Filter convergence warnings as we are incrementally training\n                warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n                if sample_weight is not None:\n                    model.fit(X_train, y_train, sample_weight=sample_weight)\n                else:\n                    model.fit(X_train, y_train)\n\n            # Log Metrics\n            # Using 'score' (accuracy) as generic metric for now\n            history['validation_0']['score'].append(model.score(X_train, y_train))\n            if X_val is not None:\n                history['validation_1']['score'].append(model.score(X_val, y_val))\n\n            current_step = next_step\n\n        return {'learning_curve': history}\n\n    def _fit_standard(self, model, X_train, y_train, sample_weight):\n        \"\"\"Standard single-shot fit.\"\"\"\n        if sample_weight is not None:\n            model.fit(X_train, y_train, sample_weight=sample_weight)\n        else:\n            model.fit(X_train, y_train)\n\n    def predict(self, model: BaseEstimator, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Get class predictions from trained model.\n\n        Args:\n            model: Trained model\n            X: Features to predict\n\n        Returns:\n            Array of predicted class labels\n        \"\"\"\n        return model.predict(X)\n\n    def predict_proba(self, model: BaseEstimator, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Get probability predictions from trained model.\n\n        Args:\n            model: Trained model  \n            X: Features to predict\n\n        Returns:\n            Array of class probabilities (n_samples, n_classes)\n        \"\"\"\n        return model.predict_proba(X)\n\n    def get_model_name(self, model_name: str) -&gt; str:\n        \"\"\"Get human-readable model name.\"\"\"\n        names = {\n            'lr': 'Logistic Regression',\n            'rf': 'Random Forest',\n            'xgb': 'XGBoost'\n        }\n        return names.get(model_name, model_name)\n</code></pre>"},{"location":"api/models/#src.models.trainer.ModelTrainer.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize ModelTrainer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary</p> required Source code in <code>src\\models\\trainer.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"\n    Initialize ModelTrainer.\n\n    Args:\n        config: Configuration dictionary\n    \"\"\"\n    self.config = config\n    self.random_state = config.get('random_state', 42)\n</code></pre>"},{"location":"api/models/#src.models.trainer.ModelTrainer.get_model","title":"<code>get_model(model_name, task, class_weight=None, scale_pos_weight=None, num_classes=2)</code>","text":"<p>Create and configure a model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>'lr', 'rf', or 'xgb'</p> required <code>task</code> <code>str</code> <p>'binary' or 'multi'</p> required <code>class_weight</code> <code>Optional[str]</code> <p>'balanced' or None (for LR/RF)</p> <code>None</code> <code>scale_pos_weight</code> <code>Optional[float]</code> <p>For XGBoost binary with S1</p> <code>None</code> <code>num_classes</code> <code>int</code> <p>Number of classes for multiclass XGBoost</p> <code>2</code> <p>Returns:</p> Type Description <code>BaseEstimator</code> <p>Configured but unfitted model instance</p> Source code in <code>src\\models\\trainer.py</code> <pre><code>def get_model(self,\n              model_name: str,\n              task: str,\n              class_weight: Optional[str] = None,\n              scale_pos_weight: Optional[float] = None,\n              num_classes: int = 2) -&gt; BaseEstimator:\n    \"\"\"\n    Create and configure a model instance.\n\n    Args:\n        model_name: 'lr', 'rf', or 'xgb'\n        task: 'binary' or 'multi'\n        class_weight: 'balanced' or None (for LR/RF)\n        scale_pos_weight: For XGBoost binary with S1\n        num_classes: Number of classes for multiclass XGBoost\n\n    Returns:\n        Configured but unfitted model instance\n    \"\"\"\n    if model_name not in self.MODELS:\n        raise ValueError(f\"Unknown model: {model_name}. Choose from {list(self.MODELS.keys())}\")\n\n    # Get base configuration\n    model_config = MODEL_CONFIGS[model_name][task].copy()\n    model_config['random_state'] = self.random_state\n\n    # Apply class weight for LR and RF\n    if model_name in ['lr', 'rf'] and class_weight is not None:\n        model_config['class_weight'] = class_weight\n\n    # Apply scale_pos_weight for XGBoost binary\n    if model_name == 'xgb' and task == 'binary' and scale_pos_weight is not None:\n        model_config['scale_pos_weight'] = scale_pos_weight\n\n    # Set num_class for XGBoost multiclass\n    if model_name == 'xgb' and task == 'multi':\n        model_config['num_class'] = num_classes\n\n    # Create model instance\n    model_class = self.MODELS[model_name]\n    model = model_class(**model_config)\n\n    logger.debug(f\"Created {model_name.upper()} for {task} task\")\n    return model\n</code></pre>"},{"location":"api/models/#src.models.trainer.ModelTrainer.get_model_name","title":"<code>get_model_name(model_name)</code>","text":"<p>Get human-readable model name.</p> Source code in <code>src\\models\\trainer.py</code> <pre><code>def get_model_name(self, model_name: str) -&gt; str:\n    \"\"\"Get human-readable model name.\"\"\"\n    names = {\n        'lr': 'Logistic Regression',\n        'rf': 'Random Forest',\n        'xgb': 'XGBoost'\n    }\n    return names.get(model_name, model_name)\n</code></pre>"},{"location":"api/models/#src.models.trainer.ModelTrainer.predict","title":"<code>predict(model, X)</code>","text":"<p>Get class predictions from trained model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator</code> <p>Trained model</p> required <code>X</code> <code>ndarray</code> <p>Features to predict</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of predicted class labels</p> Source code in <code>src\\models\\trainer.py</code> <pre><code>def predict(self, model: BaseEstimator, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Get class predictions from trained model.\n\n    Args:\n        model: Trained model\n        X: Features to predict\n\n    Returns:\n        Array of predicted class labels\n    \"\"\"\n    return model.predict(X)\n</code></pre>"},{"location":"api/models/#src.models.trainer.ModelTrainer.predict_proba","title":"<code>predict_proba(model, X)</code>","text":"<p>Get probability predictions from trained model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator</code> <p>Trained model  </p> required <code>X</code> <code>ndarray</code> <p>Features to predict</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of class probabilities (n_samples, n_classes)</p> Source code in <code>src\\models\\trainer.py</code> <pre><code>def predict_proba(self, model: BaseEstimator, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Get probability predictions from trained model.\n\n    Args:\n        model: Trained model  \n        X: Features to predict\n\n    Returns:\n        Array of class probabilities (n_samples, n_classes)\n    \"\"\"\n    return model.predict_proba(X)\n</code></pre>"},{"location":"api/models/#src.models.trainer.ModelTrainer.train","title":"<code>train(model, X_train, y_train, sample_weight=None, X_val=None, y_val=None)</code>","text":"<p>Train a model and return training metadata with learning curves.</p> <p>Dispatches to specialized training methods based on model type: - XGBoost: Uses native early stopping and evals_result - Sklearn (LR/RF): Uses custom iterative warm_start loop</p> Source code in <code>src\\models\\trainer.py</code> <pre><code>def train(self,\n          model: BaseEstimator,\n          X_train: np.ndarray,\n          y_train: np.ndarray,\n          sample_weight: Optional[np.ndarray] = None,\n          X_val: Optional[np.ndarray] = None,\n          y_val: Optional[np.ndarray] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Train a model and return training metadata with learning curves.\n\n    Dispatches to specialized training methods based on model type:\n    - XGBoost: Uses native early stopping and evals_result\n    - Sklearn (LR/RF): Uses custom iterative warm_start loop\n    \"\"\"\n    logger.info(f\"Training {type(model).__name__} on {len(y_train):,} samples...\")\n    start_time = time.time()\n\n    metadata = {}\n\n    # Dispatch to specific handler\n    if isinstance(model, XGBClassifier):\n        metadata = self._train_xgboost(\n            model, X_train, y_train, sample_weight, X_val, y_val\n        )\n    else:\n        # Check if model supports warm_start for iterative curves\n        if hasattr(model, 'warm_start') and model.warm_start:\n            metadata = self._train_sklearn_iterative(\n                model, X_train, y_train, sample_weight, X_val, y_val\n            )\n        else:\n            # Fallback for standard training\n            self._fit_standard(model, X_train, y_train, sample_weight)\n\n    training_time = time.time() - start_time\n    logger.info(f\"Training completed in {training_time:.2f}s\")\n\n    # Enforce metadata structure\n    metadata.update({\n        'training_time_seconds': training_time,\n        'n_samples': len(y_train),\n        'n_features': X_train.shape[1]\n    })\n\n    return metadata\n</code></pre>"},{"location":"api/strategies/","title":"Strategies API","text":""},{"location":"api/strategies/#src.strategies.imbalance","title":"<code>src.strategies.imbalance</code>","text":"<p>Imbalance handling strategies for UNSW-NB15 experiments.</p> <p>This module implements the three core strategies for handling class imbalance: - S0: No balancing (baseline) - S1: Class weighting (cost-sensitive learning) - S2a: Random oversampling - S2b: SMOTE (optional, with fallback)</p> <p>Each strategy is applied ONLY to training data to prevent data leakage.</p>"},{"location":"api/strategies/#src.strategies.imbalance.ImbalanceStrategy","title":"<code>ImbalanceStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for imbalance handling strategies.</p> <p>All strategies must implement the apply() method which takes training data and returns (potentially modified) training data.</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>class ImbalanceStrategy(ABC):\n    \"\"\"\n    Abstract base class for imbalance handling strategies.\n\n    All strategies must implement the apply() method which takes\n    training data and returns (potentially modified) training data.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Strategy identifier.\"\"\"\n        pass\n\n    @abstractmethod\n    def apply(self, X: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Apply the imbalance strategy to training data.\n\n        Args:\n            X: Training features\n            y: Training labels\n\n        Returns:\n            Tuple of (X_modified, y_modified)\n        \"\"\"\n        pass\n\n    def get_class_weight(self) -&gt; Optional[str]:\n        \"\"\"\n        Get class_weight parameter for sklearn models.\n\n        Returns:\n            None for most strategies, 'balanced' for S1\n        \"\"\"\n        return None\n\n    def get_sample_weight(self, y: np.ndarray) -&gt; Optional[np.ndarray]:\n        \"\"\"\n        Compute sample weights for models without class_weight support.\n\n        Args:\n            y: Training labels\n\n        Returns:\n            Sample weight array or None\n        \"\"\"\n        return None\n\n    def get_scale_pos_weight(self, y: np.ndarray) -&gt; Optional[float]:\n        \"\"\"\n        Compute scale_pos_weight for XGBoost binary classification.\n\n        Args:\n            y: Binary training labels\n\n        Returns:\n            Ratio of negative to positive samples or None\n        \"\"\"\n        return None\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.ImbalanceStrategy.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Strategy identifier.</p>"},{"location":"api/strategies/#src.strategies.imbalance.ImbalanceStrategy.apply","title":"<code>apply(X, y)</code>  <code>abstractmethod</code>","text":"<p>Apply the imbalance strategy to training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Training features</p> required <code>y</code> <code>ndarray</code> <p>Training labels</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple of (X_modified, y_modified)</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>@abstractmethod\ndef apply(self, X: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Apply the imbalance strategy to training data.\n\n    Args:\n        X: Training features\n        y: Training labels\n\n    Returns:\n        Tuple of (X_modified, y_modified)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.ImbalanceStrategy.get_class_weight","title":"<code>get_class_weight()</code>","text":"<p>Get class_weight parameter for sklearn models.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>None for most strategies, 'balanced' for S1</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def get_class_weight(self) -&gt; Optional[str]:\n    \"\"\"\n    Get class_weight parameter for sklearn models.\n\n    Returns:\n        None for most strategies, 'balanced' for S1\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.ImbalanceStrategy.get_sample_weight","title":"<code>get_sample_weight(y)</code>","text":"<p>Compute sample weights for models without class_weight support.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>Training labels</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Sample weight array or None</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def get_sample_weight(self, y: np.ndarray) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Compute sample weights for models without class_weight support.\n\n    Args:\n        y: Training labels\n\n    Returns:\n        Sample weight array or None\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.ImbalanceStrategy.get_scale_pos_weight","title":"<code>get_scale_pos_weight(y)</code>","text":"<p>Compute scale_pos_weight for XGBoost binary classification.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>Binary training labels</p> required <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Ratio of negative to positive samples or None</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def get_scale_pos_weight(self, y: np.ndarray) -&gt; Optional[float]:\n    \"\"\"\n    Compute scale_pos_weight for XGBoost binary classification.\n\n    Args:\n        y: Binary training labels\n\n    Returns:\n        Ratio of negative to positive samples or None\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S0_NoBalancing","title":"<code>S0_NoBalancing</code>","text":"<p>               Bases: <code>ImbalanceStrategy</code></p> <p>Baseline strategy: No modification to training data.</p> <p>This serves as the baseline to demonstrate how models fail on minority classes without any intervention.</p> <p>Expected results: - High overall accuracy - Near-zero recall for rare classes (Worms, Shellcode)</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>class S0_NoBalancing(ImbalanceStrategy):\n    \"\"\"\n    Baseline strategy: No modification to training data.\n\n    This serves as the baseline to demonstrate how models fail\n    on minority classes without any intervention.\n\n    Expected results:\n    - High overall accuracy\n    - Near-zero recall for rare classes (Worms, Shellcode)\n    \"\"\"\n\n    @property\n    def name(self) -&gt; str:\n        return \"s0\"\n\n    def apply(self, X: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Return data unchanged.\"\"\"\n        logger.info(f\"S0: No balancing applied. Training size: {len(y):,}\")\n        return X, y\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S0_NoBalancing.apply","title":"<code>apply(X, y)</code>","text":"<p>Return data unchanged.</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def apply(self, X: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return data unchanged.\"\"\"\n    logger.info(f\"S0: No balancing applied. Training size: {len(y):,}\")\n    return X, y\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S1_ClassWeight","title":"<code>S1_ClassWeight</code>","text":"<p>               Bases: <code>ImbalanceStrategy</code></p> <p>Class weighting strategy: Apply inverse frequency weights during training.</p> <p>This penalizes misclassification of minority classes more heavily without modifying the actual training data.</p> <p>For sklearn models: Use class_weight='balanced' For XGBoost binary: Use scale_pos_weight For XGBoost multiclass: Use sample_weight</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>class S1_ClassWeight(ImbalanceStrategy):\n    \"\"\"\n    Class weighting strategy: Apply inverse frequency weights during training.\n\n    This penalizes misclassification of minority classes more heavily\n    without modifying the actual training data.\n\n    For sklearn models: Use class_weight='balanced'\n    For XGBoost binary: Use scale_pos_weight\n    For XGBoost multiclass: Use sample_weight\n    \"\"\"\n\n    @property\n    def name(self) -&gt; str:\n        return \"s1\"\n\n    def apply(self, X: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Return data unchanged (weights applied at training time).\"\"\"\n        logger.info(f\"S1: Class weighting enabled. Training size: {len(y):,}\")\n        return X, y\n\n    def get_class_weight(self) -&gt; str:\n        \"\"\"Return 'balanced' for sklearn models.\"\"\"\n        return 'balanced'\n\n    def get_sample_weight(self, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute sample weights for models without class_weight parameter.\n\n        This is particularly useful for XGBoost multiclass where\n        class_weight is not directly supported.\n        \"\"\"\n        classes = np.unique(y)\n        weights = compute_class_weight('balanced', classes=classes, y=y)\n        weight_dict = dict(zip(classes, weights))\n        sample_weights = np.array([weight_dict[label] for label in y])\n\n        logger.info(f\"S1: Computed sample weights for {len(classes)} classes\")\n        logger.debug(f\"Weight range: [{sample_weights.min():.2f}, {sample_weights.max():.2f}]\")\n\n        return sample_weights\n\n    def get_scale_pos_weight(self, y: np.ndarray) -&gt; float:\n        \"\"\"\n        Compute scale_pos_weight for XGBoost binary classification.\n\n        Formula: count(negative) / count(positive)\n        \"\"\"\n        n_negative = np.sum(y == 0)\n        n_positive = np.sum(y == 1)\n\n        if n_positive == 0:\n            logger.warning(\"No positive samples found!\")\n            return 1.0\n\n        ratio = n_negative / n_positive\n        logger.info(f\"S1: XGBoost scale_pos_weight = {ratio:.2f}\")\n        return ratio\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S1_ClassWeight.apply","title":"<code>apply(X, y)</code>","text":"<p>Return data unchanged (weights applied at training time).</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def apply(self, X: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Return data unchanged (weights applied at training time).\"\"\"\n    logger.info(f\"S1: Class weighting enabled. Training size: {len(y):,}\")\n    return X, y\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S1_ClassWeight.get_class_weight","title":"<code>get_class_weight()</code>","text":"<p>Return 'balanced' for sklearn models.</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def get_class_weight(self) -&gt; str:\n    \"\"\"Return 'balanced' for sklearn models.\"\"\"\n    return 'balanced'\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S1_ClassWeight.get_sample_weight","title":"<code>get_sample_weight(y)</code>","text":"<p>Compute sample weights for models without class_weight parameter.</p> <p>This is particularly useful for XGBoost multiclass where class_weight is not directly supported.</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def get_sample_weight(self, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute sample weights for models without class_weight parameter.\n\n    This is particularly useful for XGBoost multiclass where\n    class_weight is not directly supported.\n    \"\"\"\n    classes = np.unique(y)\n    weights = compute_class_weight('balanced', classes=classes, y=y)\n    weight_dict = dict(zip(classes, weights))\n    sample_weights = np.array([weight_dict[label] for label in y])\n\n    logger.info(f\"S1: Computed sample weights for {len(classes)} classes\")\n    logger.debug(f\"Weight range: [{sample_weights.min():.2f}, {sample_weights.max():.2f}]\")\n\n    return sample_weights\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S1_ClassWeight.get_scale_pos_weight","title":"<code>get_scale_pos_weight(y)</code>","text":"<p>Compute scale_pos_weight for XGBoost binary classification.</p> <p>Formula: count(negative) / count(positive)</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def get_scale_pos_weight(self, y: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute scale_pos_weight for XGBoost binary classification.\n\n    Formula: count(negative) / count(positive)\n    \"\"\"\n    n_negative = np.sum(y == 0)\n    n_positive = np.sum(y == 1)\n\n    if n_positive == 0:\n        logger.warning(\"No positive samples found!\")\n        return 1.0\n\n    ratio = n_negative / n_positive\n    logger.info(f\"S1: XGBoost scale_pos_weight = {ratio:.2f}\")\n    return ratio\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S2a_RandomOverSampler","title":"<code>S2a_RandomOverSampler</code>","text":"<p>               Bases: <code>ImbalanceStrategy</code></p> <p>Random oversampling strategy: Duplicate minority class samples.</p> <p>This creates exact copies of minority class samples until all classes have equal representation.</p> <p>Post-resampling sizes: - Binary: ~224,000 (doubles attack class) - Multiclass: ~560,000 (all classes equal to majority)</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>class S2a_RandomOverSampler(ImbalanceStrategy):\n    \"\"\"\n    Random oversampling strategy: Duplicate minority class samples.\n\n    This creates exact copies of minority class samples until all\n    classes have equal representation.\n\n    Post-resampling sizes:\n    - Binary: ~224,000 (doubles attack class)\n    - Multiclass: ~560,000 (all classes equal to majority)\n    \"\"\"\n\n    def __init__(self, random_state: int = 42):\n        \"\"\"\n        Initialize RandomOverSampler.\n\n        Args:\n            random_state: Random seed for reproducibility\n        \"\"\"\n        self.random_state = random_state\n        self.sampler = RandomOverSampler(\n            sampling_strategy='auto',  # Resample all minority to majority\n            random_state=random_state\n        )\n\n    @property\n    def name(self) -&gt; str:\n        return \"s2a\"\n\n    def apply(self, X: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Apply random oversampling to training data.\n\n        Returns:\n            Tuple of (X_resampled, y_resampled) with equal class sizes\n        \"\"\"\n        original_size = len(y)\n        X_resampled, y_resampled = self.sampler.fit_resample(X, y)\n        new_size = len(y_resampled)\n\n        logger.info(f\"S2a: RandomOverSampler applied\")\n        logger.info(f\"     Original size: {original_size:,} -&gt; New size: {new_size:,}\")\n        logger.info(f\"     Expansion ratio: {new_size/original_size:.2f}x\")\n\n        return X_resampled, y_resampled\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S2a_RandomOverSampler.__init__","title":"<code>__init__(random_state=42)</code>","text":"<p>Initialize RandomOverSampler.</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>int</code> <p>Random seed for reproducibility</p> <code>42</code> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def __init__(self, random_state: int = 42):\n    \"\"\"\n    Initialize RandomOverSampler.\n\n    Args:\n        random_state: Random seed for reproducibility\n    \"\"\"\n    self.random_state = random_state\n    self.sampler = RandomOverSampler(\n        sampling_strategy='auto',  # Resample all minority to majority\n        random_state=random_state\n    )\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S2a_RandomOverSampler.apply","title":"<code>apply(X, y)</code>","text":"<p>Apply random oversampling to training data.</p> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple of (X_resampled, y_resampled) with equal class sizes</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def apply(self, X: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Apply random oversampling to training data.\n\n    Returns:\n        Tuple of (X_resampled, y_resampled) with equal class sizes\n    \"\"\"\n    original_size = len(y)\n    X_resampled, y_resampled = self.sampler.fit_resample(X, y)\n    new_size = len(y_resampled)\n\n    logger.info(f\"S2a: RandomOverSampler applied\")\n    logger.info(f\"     Original size: {original_size:,} -&gt; New size: {new_size:,}\")\n    logger.info(f\"     Expansion ratio: {new_size/original_size:.2f}x\")\n\n    return X_resampled, y_resampled\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S2b_SMOTE","title":"<code>S2b_SMOTE</code>","text":"<p>               Bases: <code>ImbalanceStrategy</code></p> <p>SMOTE strategy: Generate synthetic minority samples.</p> <p>Creates new samples by interpolating between minority class neighbors in feature space.</p> <p>Note: Falls back to RandomOverSampler if SMOTE fails (e.g., when a class has fewer samples than k_neighbors).</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>class S2b_SMOTE(ImbalanceStrategy):\n    \"\"\"\n    SMOTE strategy: Generate synthetic minority samples.\n\n    Creates new samples by interpolating between minority class\n    neighbors in feature space.\n\n    Note: Falls back to RandomOverSampler if SMOTE fails\n    (e.g., when a class has fewer samples than k_neighbors).\n    \"\"\"\n\n    def __init__(self, k_neighbors: int = 5, random_state: int = 42):\n        \"\"\"\n        Initialize SMOTE sampler.\n\n        Args:\n            k_neighbors: Number of nearest neighbors for interpolation\n            random_state: Random seed for reproducibility\n        \"\"\"\n        self.k_neighbors = k_neighbors\n        self.random_state = random_state\n\n    @property\n    def name(self) -&gt; str:\n        return \"s2b\"\n\n    def apply(self, X: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Apply SMOTE with fallback to RandomOverSampler.\n\n        Returns:\n            Tuple of (X_resampled, y_resampled)\n        \"\"\"\n        original_size = len(y)\n\n        try:\n            smote = SMOTE(\n                k_neighbors=self.k_neighbors,\n                random_state=self.random_state,\n                n_jobs=-1\n            )\n            X_resampled, y_resampled = smote.fit_resample(X, y)\n            method = \"SMOTE\"\n\n        except ValueError as e:\n            # Fallback for classes with insufficient samples\n            logger.warning(f\"SMOTE failed: {e}\")\n            logger.warning(\"Falling back to RandomOverSampler\")\n\n            ros = RandomOverSampler(random_state=self.random_state)\n            X_resampled, y_resampled = ros.fit_resample(X, y)\n            method = \"RandomOverSampler (fallback)\"\n\n        new_size = len(y_resampled)\n        logger.info(f\"S2b: {method} applied\")\n        logger.info(f\"     Original size: {original_size:,} -&gt; New size: {new_size:,}\")\n\n        return X_resampled, y_resampled\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S2b_SMOTE.__init__","title":"<code>__init__(k_neighbors=5, random_state=42)</code>","text":"<p>Initialize SMOTE sampler.</p> <p>Parameters:</p> Name Type Description Default <code>k_neighbors</code> <code>int</code> <p>Number of nearest neighbors for interpolation</p> <code>5</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility</p> <code>42</code> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def __init__(self, k_neighbors: int = 5, random_state: int = 42):\n    \"\"\"\n    Initialize SMOTE sampler.\n\n    Args:\n        k_neighbors: Number of nearest neighbors for interpolation\n        random_state: Random seed for reproducibility\n    \"\"\"\n    self.k_neighbors = k_neighbors\n    self.random_state = random_state\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.S2b_SMOTE.apply","title":"<code>apply(X, y)</code>","text":"<p>Apply SMOTE with fallback to RandomOverSampler.</p> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple of (X_resampled, y_resampled)</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def apply(self, X: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Apply SMOTE with fallback to RandomOverSampler.\n\n    Returns:\n        Tuple of (X_resampled, y_resampled)\n    \"\"\"\n    original_size = len(y)\n\n    try:\n        smote = SMOTE(\n            k_neighbors=self.k_neighbors,\n            random_state=self.random_state,\n            n_jobs=-1\n        )\n        X_resampled, y_resampled = smote.fit_resample(X, y)\n        method = \"SMOTE\"\n\n    except ValueError as e:\n        # Fallback for classes with insufficient samples\n        logger.warning(f\"SMOTE failed: {e}\")\n        logger.warning(\"Falling back to RandomOverSampler\")\n\n        ros = RandomOverSampler(random_state=self.random_state)\n        X_resampled, y_resampled = ros.fit_resample(X, y)\n        method = \"RandomOverSampler (fallback)\"\n\n    new_size = len(y_resampled)\n    logger.info(f\"S2b: {method} applied\")\n    logger.info(f\"     Original size: {original_size:,} -&gt; New size: {new_size:,}\")\n\n    return X_resampled, y_resampled\n</code></pre>"},{"location":"api/strategies/#src.strategies.imbalance.get_strategy","title":"<code>get_strategy(name, random_state=42)</code>","text":"<p>Factory function to get strategy by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Strategy identifier ('s0', 's1', 's2a', 's2b')</p> required <code>random_state</code> <code>int</code> <p>Random seed for reproducibility</p> <code>42</code> <p>Returns:</p> Type Description <code>ImbalanceStrategy</code> <p>Configured ImbalanceStrategy instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If strategy name is unknown</p> Source code in <code>src\\strategies\\imbalance.py</code> <pre><code>def get_strategy(name: str, random_state: int = 42) -&gt; ImbalanceStrategy:\n    \"\"\"\n    Factory function to get strategy by name.\n\n    Args:\n        name: Strategy identifier ('s0', 's1', 's2a', 's2b')\n        random_state: Random seed for reproducibility\n\n    Returns:\n        Configured ImbalanceStrategy instance\n\n    Raises:\n        ValueError: If strategy name is unknown\n    \"\"\"\n    strategies = {\n        's0': S0_NoBalancing,\n        's1': S1_ClassWeight,\n        's2a': lambda: S2a_RandomOverSampler(random_state=random_state),\n        's2b': lambda: S2b_SMOTE(random_state=random_state)\n    }\n\n    if name not in strategies:\n        raise ValueError(f\"Unknown strategy: {name}. Choose from {list(strategies.keys())}\")\n\n    strategy_class = strategies[name]\n    return strategy_class() if callable(strategy_class) else strategy_class\n</code></pre>"},{"location":"api/utils/","title":"Utils API","text":""},{"location":"api/utils/#src.utils.config","title":"<code>src.utils.config</code>","text":"<p>Configuration management utilities.</p> <p>Handles loading and validation of the main YAML configuration file.</p>"},{"location":"api/utils/#src.utils.config.get_experiment_grid","title":"<code>get_experiment_grid(config)</code>","text":"<p>Generate the experiment grid from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>Configuration dictionary</p> required <p>Yields:</p> Type Description <p>Tuples of (task, model, strategy)</p> Source code in <code>src\\utils\\config.py</code> <pre><code>def get_experiment_grid(config: Dict[str, Any]):\n    \"\"\"\n    Generate the experiment grid from configuration.\n\n    Args:\n        config: Configuration dictionary\n\n    Yields:\n        Tuples of (task, model, strategy)\n    \"\"\"\n    experiments = config.get('experiments', {})\n    tasks = experiments.get('tasks', ['binary', 'multi'])\n    models = experiments.get('models', ['lr', 'rf', 'xgb'])\n    strategies = experiments.get('strategies', ['s0', 's1', 's2a'])\n\n    for task in tasks:\n        for model in models:\n            for strategy in strategies:\n                yield task, model, strategy\n</code></pre>"},{"location":"api/utils/#src.utils.config.load_config","title":"<code>load_config(config_path='configs/main.yaml')</code>","text":"<p>Load and validate the main configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to YAML configuration file</p> <code>'configs/main.yaml'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Configuration dictionary</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If config file doesn't exist</p> <code>ValueError</code> <p>If required keys are missing</p> Source code in <code>src\\utils\\config.py</code> <pre><code>def load_config(config_path: str = \"configs/main.yaml\") -&gt; Dict[str, Any]:\n    \"\"\"\n    Load and validate the main configuration file.\n\n    Args:\n        config_path: Path to YAML configuration file\n\n    Returns:\n        Configuration dictionary\n\n    Raises:\n        FileNotFoundError: If config file doesn't exist\n        ValueError: If required keys are missing\n    \"\"\"\n    path = Path(config_path)\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Configuration file not found: {path}\")\n\n    with open(path, 'r') as f:\n        config = yaml.safe_load(f)\n\n    # Validate required keys\n    required_keys = ['data', 'random_state', 'results_dir']\n    missing = [k for k in required_keys if k not in config]\n    if missing:\n        raise ValueError(f\"Missing required configuration keys: {missing}\")\n\n    logger.info(f\"Loaded configuration from {path}\")\n\n    return config\n</code></pre>"},{"location":"api/utils/#src.utils.logging","title":"<code>src.utils.logging</code>","text":"<p>Logging configuration and utilities.</p> <p>Provides consistent logging setup across all modules.</p>"},{"location":"api/utils/#src.utils.logging.get_logger","title":"<code>get_logger(name)</code>","text":"<p>Get a logger with the specified name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Logger name (typically name)</p> required <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance</p> Source code in <code>src\\utils\\logging.py</code> <pre><code>def get_logger(name: str) -&gt; logging.Logger:\n    \"\"\"\n    Get a logger with the specified name.\n\n    Args:\n        name: Logger name (typically __name__)\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n    return logging.getLogger(name)\n</code></pre>"},{"location":"api/utils/#src.utils.logging.log_experiment_end","title":"<code>log_experiment_end(experiment_id, metrics, duration)</code>","text":"<p>Log the completion of an experiment.</p> Source code in <code>src\\utils\\logging.py</code> <pre><code>def log_experiment_end(experiment_id: str, metrics: dict, duration: float):\n    \"\"\"Log the completion of an experiment.\"\"\"\n    logger = logging.getLogger('experiment')\n    logger.info(\"-\" * 40)\n    logger.info(f\"COMPLETED: {experiment_id}\")\n    logger.info(f\"  Accuracy: {metrics['overall']['accuracy']:.4f}\")\n    logger.info(f\"  Macro F1: {metrics['overall']['macro_f1']:.4f}\")\n    logger.info(f\"  G-Mean:   {metrics['overall']['g_mean']:.4f}\")\n    logger.info(f\"  Duration: {duration:.2f}s\")\n    logger.info(\"-\" * 40)\n</code></pre>"},{"location":"api/utils/#src.utils.logging.log_experiment_start","title":"<code>log_experiment_start(experiment_id, task, model, strategy)</code>","text":"<p>Log the start of an experiment.</p> Source code in <code>src\\utils\\logging.py</code> <pre><code>def log_experiment_start(experiment_id: str, task: str, model: str, strategy: str):\n    \"\"\"Log the start of an experiment.\"\"\"\n    logger = logging.getLogger('experiment')\n    logger.info(\"=\" * 60)\n    logger.info(f\"EXPERIMENT: {experiment_id}\")\n    logger.info(f\"  Task:     {task}\")\n    logger.info(f\"  Model:    {model}\")\n    logger.info(f\"  Strategy: {strategy}\")\n    logger.info(\"=\" * 60)\n</code></pre>"},{"location":"api/utils/#src.utils.logging.setup_logging","title":"<code>setup_logging(level='INFO', log_file=None, log_format=None)</code>","text":"<p>Configure logging for the experiment pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR)</p> <code>'INFO'</code> <code>log_file</code> <code>Optional[str]</code> <p>Optional file path for logging output</p> <code>None</code> <code>log_format</code> <code>Optional[str]</code> <p>Optional custom format string</p> <code>None</code> Source code in <code>src\\utils\\logging.py</code> <pre><code>def setup_logging(\n    level: str = \"INFO\",\n    log_file: Optional[str] = None,\n    log_format: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Configure logging for the experiment pipeline.\n\n    Args:\n        level: Logging level (DEBUG, INFO, WARNING, ERROR)\n        log_file: Optional file path for logging output\n        log_format: Optional custom format string\n    \"\"\"\n    if log_format is None:\n        log_format = \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\"\n\n    # Create handlers with UTF-8 encoding for Windows compatibility\n    # This prevents UnicodeEncodeError with special characters\n    if sys.platform == 'win32':\n        sys.stdout.reconfigure(encoding='utf-8', errors='replace')\n    stream_handler = logging.StreamHandler(sys.stdout)\n    handlers = [stream_handler]\n\n    if log_file:\n        Path(log_file).parent.mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(log_file)\n        handlers.append(file_handler)\n\n    # Configure root logger\n    logging.basicConfig(\n        level=getattr(logging, level.upper()),\n        format=log_format,\n        handlers=handlers,\n        force=True\n    )\n\n    # Reduce verbosity of some libraries\n    logging.getLogger('matplotlib').setLevel(logging.WARNING)\n    logging.getLogger('sklearn').setLevel(logging.WARNING)\n</code></pre>"},{"location":"artifacts/reports/","title":"Experiment Reports &amp; Artifacts","text":"<p>This section documents the generated reports and how to interpret the output artifacts from the experiment pipeline.</p>"},{"location":"artifacts/reports/#available-reports","title":"\ud83d\udcca Available Reports","text":"<p>The <code>reports/</code> directory contains auto-generated analysis files:</p> Report Path Description Final Results <code>reports/final_results.md</code> Executive summary with strategy rankings. Final Result Report <code>reports/final_result_report.md</code> Comprehensive per-experiment breakdown. XGBoost Improvement <code>reports/XGBoost_Improvement_Report.md</code> Deep dive into XGBoost performance gains. Rare Class Analysis <code>reports/deep_dive_rare_classes.md</code> Focus on Worms, Shellcode, Backdoor. Strategic Audit <code>reports/strategic_audit_report.md</code> Compliance check against contracts."},{"location":"artifacts/reports/#artifact-schema","title":"\ud83d\udcc1 Artifact Schema","text":""},{"location":"artifacts/reports/#metrics-json-resultsmetricsjson","title":"Metrics JSON (<code>results/metrics/*.json</code>)","text":"<p>Each JSON file represents one complete experiment run.</p> <pre><code>{\n  \"experiment_id\": \"multi_xgb_s1_s42\",\n  \"task\": \"multi\",\n  \"model\": \"xgb\",\n  \"strategy\": \"s1\",\n  \"seed\": 42,\n  \"timestamp\": \"2026-01-23T12:34:56\",\n  \"training_time_seconds\": 142.5,\n  \"metrics\": {\n    \"accuracy\": 0.686,\n    \"macro_f1\": 0.513,\n    \"weighted_f1\": 0.740,\n    \"g_mean\": 0.795,\n    \"roc_auc_ovr\": 0.959\n  },\n  \"per_class_report\": {\n    \"Normal\": {\"precision\": 0.98, \"recall\": 0.65, \"f1\": 0.78, \"support\": 37000},\n    \"Worms\": {\"precision\": 0.12, \"recall\": 0.75, \"f1\": 0.21, \"support\": 44},\n    // ... other classes\n  },\n  \"confusion_matrix\": [[...], [...], ...]\n}\n</code></pre>"},{"location":"artifacts/reports/#summary-tables-resultstables","title":"Summary Tables (<code>results/tables/</code>)","text":"File Content <code>experiment_log.csv</code> Master log with all experiments and their metrics. <code>per_class_metrics.csv</code> Precision/Recall/F1 for each class, per experiment. <code>rare_class_report.csv</code> Focused report on Worms, Shellcode, Backdoor, Analysis. <code>final_summary_tables.csv</code> Aggregated summary by Task/Model/Strategy."},{"location":"artifacts/reports/#figures-resultsfigures","title":"Figures (<code>results/figures/</code>)","text":"File Pattern Description <code>cm_*.png</code> Confusion matrix heatmaps. <code>radar_*.png</code> Strategy comparison radar charts. <code>rare_class_*.png</code> Bar charts for rare class performance."},{"location":"artifacts/reports/#interpreting-results","title":"\ud83d\udcc8 Interpreting Results","text":"<p>When analyzing results, focus on:</p> <ol> <li>G-Mean over Accuracy: High Accuracy with low G-Mean indicates majority class bias.</li> <li>Rare Class Recall: Check <code>per_class_report</code> for Worms, Shellcode specifically.</li> <li>Strategy Comparison: Compare S0 vs S1 vs S2a on the same model/task.</li> </ol> <p>[!TIP] Use the radar charts (<code>radar_*.png</code>) to quickly visualize the trade-offs between Accuracy, F1, and G-Mean.</p>"},{"location":"contracts/data_contract/","title":"Data Contract: UNSW-NB15","text":"<p>Document Version: 2.0 Last Updated: 2026-01-17 Status: APPROVED</p>"},{"location":"contracts/data_contract/#1-source-system","title":"1. Source System","text":"Attribute Value Dataset UNSW-NB15 Provider Australian Centre for Cyber Security (ACCS) Creation Tool IXIA PerfectStorm Format CSV (comma-separated) Encoding UTF-8 Location Local Storage (<code>dataset/</code>)"},{"location":"contracts/data_contract/#11-source-files","title":"1.1 Source Files","text":"File Path Size Records Training Set <code>dataset/UNSW_NB15_training-set.csv</code> ~32 MB 175,341 Testing Set <code>dataset/UNSW_NB15_testing-set.csv</code> ~15 MB 82,332 Feature List <code>dataset/NUSW-NB15_features.csv</code> ~4 KB Feature descriptions"},{"location":"contracts/data_contract/#2-schema-definition","title":"2. Schema Definition","text":"<p>The dataset must contain the following features (after header normalization):</p>"},{"location":"contracts/data_contract/#21-identifiers-to-be-removed-in-modeling","title":"2.1 Identifiers (To Be Removed in Modeling)","text":"Feature Type Description Removal Reason <code>id</code> Integer Row identifier Non-predictive <code>srcip</code> String Source IP Address High cardinality, prevents generalization <code>dstip</code> String Destination IP Address High cardinality, prevents generalization <code>sport</code> Integer Source Port Configuration noise <code>dsport</code> Integer Destination Port Configuration noise <code>stime</code> Float Start Timestamp Temporal leakage <code>ltime</code> Float Last Timestamp Temporal leakage <p>[!NOTE] These 7 columns are already removed in the source dataset files used for this project (<code>dataset/UNSW_NB15_training-set.csv</code> and <code>testing-set.csv</code>). The code does not need to drop them explicitly as they are absent. The remaining 42 predictive features are retained.</p>"},{"location":"contracts/data_contract/#22-categorical-features-one-hot-encoded","title":"2.2 Categorical Features (One-Hot Encoded)","text":"Feature Description Unique Values Encoding <code>proto</code> Network Protocol ~130 (tcp, udp, etc.) OneHotEncoder <code>state</code> Connection State ~15 (FIN, CON, INT, etc.) OneHotEncoder <code>service</code> Application Service ~13 (http, ftp, dns, etc.) OneHotEncoder <p>Post-Encoding Dimensionality:</p> Feature Original Columns After One-Hot proto 1 ~130 state 1 ~15 service 1 ~13 Total Categorical 3 ~158"},{"location":"contracts/data_contract/#23-numerical-features-scaled","title":"2.3 Numerical Features (Scaled)","text":""},{"location":"contracts/data_contract/#flow-features","title":"Flow Features","text":"Feature Description Type Range <code>dur</code> Duration of connection Float 0 - 60+ <code>sbytes</code> Source to destination bytes Integer 0 - 10M+ <code>dbytes</code> Destination to source bytes Integer 0 - 10M+ <code>sttl</code> Source to destination TTL Integer 0 - 255 <code>dttl</code> Destination to source TTL Integer 0 - 255 <code>sloss</code> Source packets retransmitted/dropped Integer 0 - 1000+ <code>dloss</code> Destination packets retransmitted/dropped Integer 0 - 1000+ <code>Sload</code> Source bits per second Float 0 - 1B+ <code>Dload</code> Destination bits per second Float 0 - 1B+ <code>Spkts</code> Source to destination packet count Integer 0 - 100K+ <code>Dpkts</code> Destination to source packet count Integer 0 - 100K+"},{"location":"contracts/data_contract/#window-features","title":"Window Features","text":"Feature Description Type <code>swin</code> Source TCP window advertisement Integer <code>dwin</code> Destination TCP window advertisement Integer <code>stcpb</code> Source TCP base sequence number Integer <code>dtcpb</code> Destination TCP base sequence number Integer"},{"location":"contracts/data_contract/#derivedstatistical-features","title":"Derived/Statistical Features","text":"Feature Description Type <code>smeansz</code> Mean of packet size transmitted by src Integer <code>dmeansz</code> Mean of packet size transmitted by dst Integer <code>trans_depth</code> Connection depth (pipelined) Integer <code>res_bdy_len</code> Response body length (HTTP) Integer <code>Sjit</code> Source jitter Float <code>Djit</code> Destination jitter Float <code>sintpkt</code> Source interpacket arrival time Float <code>dintpkt</code> Destination interpacket arrival time Float <code>tcprtt</code> TCP connection round-trip time Float <code>synack</code> Time between SYN and SYN-ACK Float <code>ackdat</code> Time between SYN-ACK and ACK Float"},{"location":"contracts/data_contract/#binary-indicator-features","title":"Binary Indicator Features","text":"Feature Description Values <code>is_sm_ips_ports</code> If source/dest IPs and ports are equal 0, 1 <code>is_ftp_login</code> If FTP session has login 0, 1"},{"location":"contracts/data_contract/#connection-count-features","title":"Connection Count Features","text":"Feature Description Type <code>ct_state_ttl</code> Count of same state/TTL connections Integer <code>ct_flw_http_mthd</code> Count of HTTP methods Integer <code>ct_ftp_cmd</code> Count of FTP commands Integer <code>ct_srv_src</code> Count connections same service/src Integer <code>ct_srv_dst</code> Count connections same service/dst Integer <code>ct_dst_ltm</code> Count connections to dst in last 100 Integer <code>ct_src_ltm</code> Count connections from src in last 100 Integer <code>ct_src_dport_ltm</code> Count same src/dst port in last 100 Integer <code>ct_dst_sport_ltm</code> Count same dst/src port in last 100 Integer <code>ct_dst_src_ltm</code> Count same dst/src in last 100 Integer <p>Total Numerical Features: 36</p>"},{"location":"contracts/data_contract/#24-target-labels","title":"2.4 Target Labels","text":"Label Type Description Values <code>label</code> Binary Attack presence 0 = Normal, 1 = Attack <code>attack_cat</code> Multiclass Attack category 10 classes (see below)"},{"location":"contracts/data_contract/#241-attack-category-mapping","title":"2.4.1 Attack Category Mapping","text":"Index Category Description Training Count Test Count Total % 0 Normal Benign traffic 56,000 37,000 36.10% 1 Fuzzers Protocol fuzzing attacks 18,184 6,062 9.41% 2 Analysis Port scan, spam, HTML attacks 2,000 677 1.04% 3 Backdoor Persistent access channels 1,746 583 0.90% 4 DoS Denial of service 12,264 4,089 6.35% 5 Exploits Known vulnerability exploits 33,393 11,132 17.28% 6 Generic Generic attack patterns 40,000 18,871 22.85% 7 Reconnaissance Network probing 10,491 3,496 5.43% 8 Shellcode Executable code injection 1,133 378 0.59% 9 Worms Self-replicating malware 130 44 0.07%"},{"location":"contracts/data_contract/#3-class-distribution-analysis","title":"3. Class Distribution Analysis","text":""},{"location":"contracts/data_contract/#31-binary-classification-distribution","title":"3.1 Binary Classification Distribution","text":"Class Training Testing Total Normal (0) 56,000 (31.94%) 37,000 (44.94%) 93,000 (36.10%) Attack (1) 119,341 (68.06%) 45,332 (55.06%) 164,673 (63.90%) Total 175,341 82,332 257,673 <p>Imbalance Ratio (Binary): Normal : Attack = 1 : 1.77</p>"},{"location":"contracts/data_contract/#32-multiclass-classification-distribution","title":"3.2 Multiclass Classification Distribution","text":""},{"location":"contracts/data_contract/#training-set-distribution","title":"Training Set Distribution","text":"<pre><code> Normal        \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  56,000 (31.94%)\n Generic       \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        40,000 (22.82%)\n Exploits      \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              33,393 (19.04%)\n Fuzzers       \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                        18,184 (10.37%)\n DoS           \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                            12,264 (6.99%)\n Reconnaissance\u2588\u2588\u2588\u2588\u2588\u2588                              10,491 (5.98%)\n Analysis      \u2588                                    2,000 (1.14%)\n Backdoor      \u2588                                    1,746 (1.00%)\n Shellcode     \u2588                                    1,133 (0.65%)\n Worms         \u258e                                      130 (0.07%)\n</code></pre>"},{"location":"contracts/data_contract/#imbalance-ratios-relative-to-worms","title":"Imbalance Ratios (Relative to Worms)","text":"Class Count Ratio to Worms Category Normal 56,000 430.8x Majority Generic 40,000 307.7x Majority Exploits 33,393 256.9x Majority Fuzzers 18,184 139.9x Majority DoS 12,264 94.3x Moderate Reconnaissance 10,491 80.7x Moderate Analysis 2,000 15.4x Rare Backdoor 1,746 13.4x Rare Shellcode 1,133 8.7x Rare Worms 130 1.0x Critically Rare <p>[!WARNING] Extreme Imbalance Warning: - Worms class has only 130 training samples (0.07%) - Without imbalance handling, models will likely predict 0% recall for Worms - Special attention required during SMOTE (k_neighbors &gt; 5 may fail)</p>"},{"location":"contracts/data_contract/#4-data-quality-expectations","title":"4. Data Quality Expectations","text":""},{"location":"contracts/data_contract/#41-missing-values","title":"4.1 Missing Values","text":"Feature Type Expected Missing Handling Strategy Numerical \u2264 1% Median Imputation Categorical \u2264 0.1% \"missing\" token <p>Implementation:</p> <pre><code>from sklearn.impute import SimpleImputer\n\nnum_imputer = SimpleImputer(strategy='median')\ncat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n</code></pre>"},{"location":"contracts/data_contract/#42-data-types","title":"4.2 Data Types","text":"Column Group Expected Type Validation Identifiers int/string Drop before modeling Categoricals string Check for novel categories Numericals float64/int64 Check for infinity/NaN Labels int/string Validate against known categories"},{"location":"contracts/data_contract/#43-split-integrity","title":"4.3 Split Integrity","text":"Constraint Requirement Official Split Training and Test sets must NOT be merged and re-split Benchmark Compatibility Official split enables comparison with published results Validation Set Created from Training Set (20%, stratified) Test Set Isolation NEVER touched during preprocessing, tuning, or balancing"},{"location":"contracts/data_contract/#5-derived-tables-preprocessing-output","title":"5. Derived Tables (Preprocessing Output)","text":""},{"location":"contracts/data_contract/#51-output-artifacts","title":"5.1 Output Artifacts","text":"Artifact Format Contents <code>X_train_processed.npz</code> NumPy sparse/dense Preprocessed training features <code>X_val_processed.npz</code> NumPy sparse/dense Preprocessed validation features <code>X_test_processed.npz</code> NumPy sparse/dense Preprocessed test features <code>y_train_binary.npy</code> NumPy array Binary labels (training) <code>y_val_binary.npy</code> NumPy array Binary labels (validation) <code>y_test_binary.npy</code> NumPy array Binary labels (test) <code>y_train_multi.npy</code> NumPy array Multiclass labels (training) <code>y_val_multi.npy</code> NumPy array Multiclass labels (validation) <code>y_test_multi.npy</code> NumPy array Multiclass labels (test) <code>feature_names.json</code> JSON List of feature names after encoding <code>label_mapping.json</code> JSON Multiclass label to integer mapping <code>preprocessing_metadata.json</code> JSON Scaler means, encoder categories, etc."},{"location":"contracts/data_contract/#52-expected-dimensions","title":"5.2 Expected Dimensions","text":"Artifact Rows Columns (approx.) X_train 140,273 ~196 X_val 35,068 ~196 X_test 82,332 ~196 y_* Same as X 1"},{"location":"contracts/data_contract/#53-after-resampling-s2-strategy","title":"5.3 After Resampling (S2 Strategy)","text":"Strategy Task X_train Rows S0 Both 140,273 S1 Both 140,273 S2a (ROS) Binary ~224,000 S2a (ROS) Multi ~560,000 (all classes = max class) S2b (SMOTE) Binary ~224,000 S2b (SMOTE) Multi ~560,000"},{"location":"contracts/data_contract/#6-data-leakage-prevention-protocol","title":"6. Data Leakage Prevention Protocol","text":"<p>[!CAUTION] Critical Constraints - Violation Invalidates Results</p>"},{"location":"contracts/data_contract/#61-preprocessing-leakage-prevention","title":"6.1 Preprocessing Leakage Prevention","text":"Step Constraint Validation Imputation Fit ONLY on X_train Check imputer fitted before transform Scaling Fit ONLY on X_train Check scaler.mean_ from train only Encoding Learn categories from X_train ONLY Check encoder.categories_ source Label Encoding Fit on training labels Same for all splits"},{"location":"contracts/data_contract/#62-resampling-leakage-prevention","title":"6.2 Resampling Leakage Prevention","text":"Constraint Validation Apply ONLY to X_train, y_train Never call fit_resample on val/test Synthetic samples ONLY in training Val/test row counts unchanged Cross-validation: apply inside fold Use <code>Pipeline</code> with <code>imblearn</code>"},{"location":"contracts/data_contract/#63-validation-checklist","title":"6.3 Validation Checklist","text":"<pre><code># Run these assertions before training\nassert X_val.shape[0] == ORIGINAL_VAL_SIZE, \"Validation set modified!\"\nassert X_test.shape[0] == ORIGINAL_TEST_SIZE, \"Test set modified!\"\nassert not np.isnan(X_train).any(), \"NaN values in training!\"\nassert not np.isinf(X_train).any(), \"Inf values in training!\"\n</code></pre>"},{"location":"contracts/data_contract/#7-ownership-slas","title":"7. Ownership &amp; SLAs","text":"Role Responsibility Owner Data Engineering Agent (Preprocessing) Consumer Model Training Agent Validator QA Agent"},{"location":"contracts/data_contract/#71-change-management","title":"7.1 Change Management","text":"Change Type Required Action Schema Change (add/remove feature) Version bump (major) Imputation Strategy Change Version bump (minor) Split Ratio Change Version bump (minor) Bug Fix Version bump (patch)"},{"location":"contracts/data_contract/#72-versioning-protocol","title":"7.2 Versioning Protocol","text":"<pre><code>data_contract_v{major}.{minor}.{patch}.md\n</code></pre> <p>Current Version: 2.0.0</p>"},{"location":"contracts/data_contract/#8-usage-in-experiment-grid","title":"8. Usage in Experiment Grid","text":"<p>This exact processed dataset is the single consistent input for all 18 experiments:</p> Task Model Strategy Data Source Binary LR, RF, XGB S0 X_train (raw), y_train_binary Binary LR, RF, XGB S1 X_train + class_weight Binary LR, RF, XGB S2 X_train_resampled, y_train_resampled Multi LR, RF, XGB S0 X_train (raw), y_train_multi Multi LR, RF, XGB S1 X_train + sample_weight Multi LR, RF, XGB S2 X_train_resampled, y_train_resampled"},{"location":"contracts/data_contract/#appendix-a-feature-list-reference","title":"Appendix A: Feature List Reference","text":""},{"location":"contracts/data_contract/#a1-complete-feature-list-after-id-removal-42-features","title":"A.1 Complete Feature List After ID Removal (42 features)","text":"<pre><code>1.  dur           22. ct_state_ttl\n2.  proto         23. ct_flw_http_mthd\n3.  state         24. is_ftp_login\n4.  service       25. ct_ftp_cmd\n5.  sbytes        26. ct_srv_src\n6.  dbytes        27. ct_srv_dst\n7.  sttl          28. ct_dst_ltm\n8.  dttl          29. ct_src_ltm\n9.  sloss         30. ct_src_dport_ltm\n10. dloss         31. ct_dst_sport_ltm\n11. Sload         32. ct_dst_src_ltm\n12. Dload         33. swin\n13. Spkts         34. dwin\n14. Dpkts         35. stcpb\n15. smeansz       36. dtcpb\n16. dmeansz       37. Sjit\n17. trans_depth   38. Djit\n18. res_bdy_len   39. tcprtt\n19. sintpkt       40. synack\n20. dintpkt       41. ackdat\n21. is_sm_ips_ports 42. (targets: label, attack_cat)\n</code></pre>"},{"location":"contracts/data_contract/#9-feature-correlation-audit","title":"9. Feature Correlation Audit","text":"<p>[!NOTE] All 42 predictive features are retained by design. This section documents the rationale.</p>"},{"location":"contracts/data_contract/#91-multicollinearity-assessment","title":"9.1 Multicollinearity Assessment","text":"Feature Pair Correlation Action Rationale <code>sbytes</code> \u2194 <code>Sload</code> 0.85+ Retain Both Capture different attack signatures <code>sttl</code> \u2194 <code>dttl</code> 0.70+ Retain Both TTL asymmetry indicates spoofing <code>sintpkt</code> \u2194 <code>dintpkt</code> 0.60+ Retain Both Jitter patterns differ by attack type"},{"location":"contracts/data_contract/#92-feature-retention-justification","title":"9.2 Feature Retention Justification","text":"<p>Why retain all 42 features? 1. Rare Class Signal Preservation: Aggressive feature selection optimizes for majority variance, potentially discarding subtle signals critical for Worms (130 samples) and Shellcode (1,133 samples). 2. Model Robustness: Tree-based models (RF, XGB) naturally handle correlated features via feature importance. 3. Reproducibility: Consistent feature set across all 18 experiments ensures fair comparison.</p>"},{"location":"contracts/data_contract/#10-statistical-validation-requirements","title":"10. Statistical Validation Requirements","text":""},{"location":"contracts/data_contract/#101-uncertainty-quantification","title":"10.1 Uncertainty Quantification","text":"Metric Method Parameters Macro-F1 Bootstrap CI 1000 iterations, 95% CI G-Mean Bootstrap CI 1000 iterations, 95% CI Per-Class Recall Bootstrap CI 1000 iterations, 95% CI"},{"location":"contracts/data_contract/#102-significance-testing","title":"10.2 Significance Testing","text":"Comparison Type Test Correction Paired Models (same data) McNemar's Test Bonferroni (\u03b1 = 0.05/18) Strategy Comparison Wilcoxon Signed-Rank Bonferroni Effect Size Cohen's \u03ba Report with CI"},{"location":"contracts/data_contract/#103-required-statistical-artifacts","title":"10.3 Required Statistical Artifacts","text":"<pre><code>statistical_outputs:\n  - \"results/tables/metric_confidence_intervals.csv\"\n  - \"results/tables/paired_significance_tests.csv\"\n  - \"results/tables/effect_sizes.csv\"\n  - \"results/tables/rare_class_ci.csv\"\n</code></pre>"},{"location":"contracts/data_contract/#document-history","title":"Document History","text":"Version Date Changes 1.0 2026-01-17 Initial contract 2.0 2026-01-17 Complete enhancement with class distributions, leakage protocols 3.0 2026-01-18 Added \u00a79 Feature Correlation Audit, \u00a710 Statistical Validation Requirements"},{"location":"contracts/experiment_contract/","title":"Experiment Contract: UNSW-NB15 Imbalance Study","text":"<p>Document Version: 3.0 Last Updated: 2026-01-22 Status: APPROVED</p>"},{"location":"contracts/experiment_contract/#1-experiment-scope","title":"1. Experiment Scope","text":"<p>This contract defines the complete scope of experiments to analyze the impact of class imbalance handling on the UNSW-NB15 dataset for intrusion detection.</p>"},{"location":"contracts/experiment_contract/#11-research-objectives","title":"1.1 Research Objectives","text":"Objective ID Description Addressed By O1 Build binary IDS classifiers Task A experiments O2 Build multiclass IDS classifiers Task B experiments O3 Compare imbalance handling strategies S0/S1/S2 comparison O4 Analyze rare attack detection Per-class metrics focus O5 Establish reproducible baseline Fixed seeds, configs"},{"location":"contracts/experiment_contract/#12-research-questions-mapping","title":"1.2 Research Questions Mapping","text":"RQ Question Experiments Needed RQ1 Imbalance effect on binary vs multiclass All Task A vs Task B RQ2 Effectiveness of weighting/oversampling S1/S2 vs S0 comparisons RQ3 Model response patterns to strategies Cross-model analysis RQ4 Rare class improvement with S2 Multi_S2 vs Multi_S0"},{"location":"contracts/experiment_contract/#2-methodology-matrix","title":"2. Methodology Matrix","text":"<p>The experiment space is defined by the Cartesian product of the following dimensions:</p>"},{"location":"contracts/experiment_contract/#21-dimension-a-tasks","title":"2.1 Dimension A: Tasks","text":"Task ID Name Description Classes Label Column A Binary Normal vs Attack 2 <code>label</code> B Multiclass Normal + 9 attack types 10 <code>attack_cat</code>"},{"location":"contracts/experiment_contract/#22-dimension-b-models","title":"2.2 Dimension B: Models","text":"Model ID Full Name Family scikit-learn Class LR Logistic Regression Linear <code>LogisticRegression</code> RF Random Forest Bagging <code>RandomForestClassifier</code> XGB XGBoost Boosting <code>XGBClassifier</code>"},{"location":"contracts/experiment_contract/#23-dimension-c-imbalance-strategies","title":"2.3 Dimension C: Imbalance Strategies","text":"Strategy ID Name Description Data Modification S0 None (Baseline) No balancing applied None S1 Class Weight Inverse frequency weights Model parameter S2a RandomOverSampler Duplicate minority samples Training data expanded S2b SMOTE (Optional) Synthetic minority samples Training data expanded"},{"location":"contracts/experiment_contract/#24-experiment-grid-summary","title":"2.4 Experiment Grid Summary","text":"<p>Core Grid:</p> <pre><code>Tasks (2) \u00d7 Models (3) \u00d7 Strategies (3) = 18 Core Experiments\n</code></pre> <p>With Optional SMOTE:</p> <pre><code>Tasks (2) \u00d7 Models (3) \u00d7 Strategies (4) = 24 Total Experiments\n</code></pre>"},{"location":"contracts/experiment_contract/#25-complete-experiment-list","title":"2.5 Complete Experiment List","text":"# Experiment ID Task Model Strategy 1 <code>binary_lr_s0</code> Binary LR None 2 <code>binary_lr_s1</code> Binary LR Class Weight 3 <code>binary_lr_s2a</code> Binary LR RandomOverSampler 4 <code>binary_rf_s0</code> Binary RF None 5 <code>binary_rf_s1</code> Binary RF Class Weight 6 <code>binary_rf_s2a</code> Binary RF RandomOverSampler 7 <code>binary_xgb_s0</code> Binary XGB None 8 <code>binary_xgb_s1</code> Binary XGB Class Weight 9 <code>binary_xgb_s2a</code> Binary XGB RandomOverSampler 10 <code>multi_lr_s0</code> Multiclass LR None 11 <code>multi_lr_s1</code> Multiclass LR Class Weight 12 <code>multi_lr_s2a</code> Multiclass LR RandomOverSampler 13 <code>multi_rf_s0</code> Multiclass RF None 14 <code>multi_rf_s1</code> Multiclass RF Class Weight 15 <code>multi_rf_s2a</code> Multiclass RF RandomOverSampler 16 <code>multi_xgb_s0</code> Multiclass XGB None 17 <code>multi_xgb_s1</code> Multiclass XGB Class Weight 18 <code>multi_xgb_s2a</code> Multiclass XGB RandomOverSampler <p>Optional SMOTE Experiments (S2b):</p> # Experiment ID Task Model Strategy 19 <code>binary_lr_s2b</code> Binary LR SMOTE 20 <code>binary_rf_s2b</code> Binary RF SMOTE 21 <code>binary_xgb_s2b</code> Binary XGB SMOTE 22 <code>multi_lr_s2b</code> Multiclass LR SMOTE 23 <code>multi_rf_s2b</code> Multiclass RF SMOTE 24 <code>multi_xgb_s2b</code> Multiclass XGB SMOTE"},{"location":"contracts/experiment_contract/#3-configuration-parameters","title":"3. Configuration Parameters","text":""},{"location":"contracts/experiment_contract/#31-data-splitting","title":"3.1 Data Splitting","text":"Parameter Value Rationale Training Set Official UNSW-NB15 train Benchmark compatibility Validation Split 20% of Training (stratified) Hyperparameter tuning Test Set Official UNSW-NB15 test Final evaluation only Stratification Yes (by target class) Preserve class ratios"},{"location":"contracts/experiment_contract/#32-reproducibility-parameters","title":"3.2 Reproducibility Parameters","text":"Parameter Value Implementation Random Seed 42 All random operations NumPy Seed 42 <code>np.random.seed(42)</code> Python Hash Seed 42 <code>PYTHONHASHSEED=42</code>"},{"location":"contracts/experiment_contract/#33-hyperparameter-specifications","title":"3.3 Hyperparameter Specifications","text":""},{"location":"contracts/experiment_contract/#logistic-regression-binary","title":"Logistic Regression (Binary)","text":"<pre><code>LogisticRegression(\n    C=1.0,                    # Regularization strength\n    solver='lbfgs',           # Optimized: Faster than saga\n    max_iter=1000,            # Convergence iterations\n    penalty='l2',             # Regularization type\n    class_weight=None,        # S0: None, S1: 'balanced'\n    random_state=42\n)\n</code></pre>"},{"location":"contracts/experiment_contract/#logistic-regression-multiclass","title":"Logistic Regression (Multiclass)","text":"<pre><code>LogisticRegression(\n    C=1.0,\n    solver='lbfgs',\n    max_iter=1000,\n    penalty='l2',\n    # multi_class='multinomial', # Removed: auto-handled by sklearn/deprecated\n    class_weight=None,          # S0: None, S1: 'balanced'\n    random_state=42\n)\n</code></pre>"},{"location":"contracts/experiment_contract/#random-forest-both-tasks","title":"Random Forest (Both Tasks)","text":"<pre><code>RandomForestClassifier(\n    n_estimators=300,         # Optimized (was 200)\n    max_depth=None,           # Optimized: Unbounded (was 20)\n    min_samples_split=2,      # Optimized (was 5)\n    min_samples_leaf=1,       # Optimized (was 2)\n    max_features='sqrt',\n    criterion='gini',         # Explicit\n    class_weight='balanced_subsample', # Optimized (was S1: 'balanced')\n    bootstrap=True,\n    oob_score=True,           # Added validation\n    n_jobs=-1,\n    random_state=42\n)\n</code></pre>"},{"location":"contracts/experiment_contract/#xgboost-binary","title":"XGBoost (Binary)","text":"<pre><code>XGBClassifier(\n    n_estimators=150,         # Optimized (was 200)\n    learning_rate=0.05,       # Optimized (was 0.1)\n    max_depth=15,             # Optimized (was 10)\n    min_child_weight=2,       # Optimized (was 1)\n    subsample=0.85,           # Optimized (was 0.8)\n    colsample_bytree=0.85,    # Optimized (was 0.8)\n    gamma=1.0,                # New regularization\n    reg_lambda=1.0,           # New regularization\n    reg_alpha=0.5,            # New regularization\n    scale_pos_weight=None,    # S0: None, S1: computed ratio\n    use_label_encoder=False,\n    eval_metric='logloss',\n    objective='binary:logistic',\n    n_jobs=-1,\n    random_state=42,\n    verbosity=0\n)\n</code></pre>"},{"location":"contracts/experiment_contract/#xgboost-multiclass","title":"XGBoost (Multiclass)","text":"<pre><code>XGBClassifier(\n    n_estimators=150,         # Optimized (was 200)\n    learning_rate=0.05,       # Optimized (was 0.1)\n    max_depth=15,             # Optimized (was 10)\n    min_child_weight=2,       # Optimized (was 1)\n    subsample=0.85,           # Optimized (was 0.8)\n    colsample_bytree=0.85,    # Optimized (was 0.8)\n    gamma=1.0,                # New regularization\n    reg_lambda=1.0,           # New regularization\n    reg_alpha=0.5,            # New regularization\n    use_label_encoder=False,\n    eval_metric='mlogloss',\n    objective='multi:softprob',\n    num_class=10,\n    n_jobs=-1,\n    random_state=42,\n    verbosity=0\n)\n</code></pre>"},{"location":"contracts/experiment_contract/#34-optional-hyperparameter-tuning-grid","title":"3.4 Optional Hyperparameter Tuning Grid","text":"<p>If time permits, limited tuning on validation set:</p> Model Parameter Values LR C [0.01, 0.1, 1.0, 10.0] RF n_estimators [100, 200] RF max_depth [10, 20, None] XGB n_estimators [100, 200] XGB learning_rate [0.01, 0.1] XGB max_depth [5, 10] <p>Tuning Method: 3-Fold Stratified Cross-Validation on Training Set Scoring Metric: Macro F1 (to weight all classes equally)</p>"},{"location":"contracts/experiment_contract/#4-imbalance-strategy-specifications","title":"4. Imbalance Strategy Specifications","text":""},{"location":"contracts/experiment_contract/#41-s0-no-balancing-baseline","title":"4.1 S0: No Balancing (Baseline)","text":"<p>Purpose: Establish baseline to show how models fail on minority classes.</p> <pre><code># No modification to training data\nX_train_s0 = X_train\ny_train_s0 = y_train\n\n# No class weight\nmodel.fit(X_train_s0, y_train_s0)\n</code></pre> <p>Expected Results: - High overall accuracy (&gt;85%) - Very low recall for rare classes (Worms: 0-5%, Shellcode: 5-20%) - This demonstrates the severity of imbalance problem</p>"},{"location":"contracts/experiment_contract/#42-s1-class-weighting","title":"4.2 S1: Class Weighting","text":"<p>Purpose: Penalize misclassification of minority classes more heavily.</p> <pre><code># For sklearn models\nmodel = RandomForestClassifier(class_weight='balanced', ...)\n\n# For XGBoost Binary\nscale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])\nmodel = XGBClassifier(scale_pos_weight=scale_pos_weight, ...)\n\n# For XGBoost Multiclass (via sample_weight)\nfrom sklearn.utils.class_weight import compute_sample_weight\nsample_weights = compute_sample_weight('balanced', y_train)\nmodel.fit(X_train, y_train, sample_weight=sample_weights)\n</code></pre> <p>Computed Weights (Multiclass, Training Set):</p> Class Count Weight (approx.) Normal 56,000 0.31 Generic 40,000 0.44 Exploits 33,393 0.53 Fuzzers 18,184 0.96 DoS 12,264 1.43 Reconnaissance 10,491 1.67 Analysis 2,000 8.77 Backdoor 1,746 10.04 Shellcode 1,133 15.48 Worms 130 134.88"},{"location":"contracts/experiment_contract/#43-s2a-random-oversampling","title":"4.3 S2a: Random Oversampling","text":"<p>Purpose: Duplicate minority class samples to balance distribution.</p> <pre><code>from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(\n    sampling_strategy='auto',  # Upsample all minorities to majority\n    random_state=42\n)\nX_train_s2a, y_train_s2a = ros.fit_resample(X_train, y_train)\n</code></pre> <p>Post-Resampling Size:</p> Task Original Size After ROS Binary 140,273 ~224,000 (2\u00d7116,341 attacks) Multiclass 140,273 ~560,000 (10\u00d756,000 per class)"},{"location":"contracts/experiment_contract/#44-s2b-smote-optional","title":"4.4 S2b: SMOTE (Optional)","text":"<p>Purpose: Generate synthetic samples in feature space for minority classes.</p> <pre><code>from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(\n    k_neighbors=5,            # Nearest neighbors for interpolation\n    sampling_strategy='auto',\n    random_state=42,\n    n_jobs=-1\n)\n\ntry:\n    X_train_s2b, y_train_s2b = smote.fit_resample(X_train, y_train)\nexcept ValueError:\n    # Fallback for classes with &lt; k_neighbors samples (e.g., Worms=130 &lt; 5 is OK, but edge cases)\n    X_train_s2b, y_train_s2b = RandomOverSampler(random_state=42).fit_resample(X_train, y_train)\n</code></pre> <p>[!WARNING] SMOTE Memory Requirements: - Multiclass SMOTE can generate ~560K samples - Requires ~10GB RAM for full dataset - Use S2a if memory is constrained</p>"},{"location":"contracts/experiment_contract/#5-evaluation-metrics-specification","title":"5. Evaluation Metrics Specification","text":""},{"location":"contracts/experiment_contract/#51-overall-metrics-per-experiment","title":"5.1 Overall Metrics (Per Experiment)","text":"Metric Formula scikit-learn Purpose Accuracy (TP+TN)/Total <code>accuracy_score</code> Baseline (misleading for imbalance) Macro F1 Mean(F1_per_class) <code>f1_score(average='macro')</code> Equal weight all classes Weighted F1 Weighted Mean(F1) <code>f1_score(average='weighted')</code> Reflects majority G-Mean \u221a(Sensitivity\u00d7Specificity) <code>geometric_mean_score</code> Primary metric ROC-AUC Area under ROC <code>roc_auc_score</code> Threshold-independent <p>G-Mean Implementation:</p> <pre><code>from imblearn.metrics import geometric_mean_score\n\n# Binary\ng_mean = geometric_mean_score(y_true, y_pred)\n\n# Multiclass\ng_mean = geometric_mean_score(y_true, y_pred, average='macro')\n</code></pre> <p>ROC-AUC Implementation:</p> <pre><code>from sklearn.metrics import roc_auc_score\n\n# Binary\nroc_auc = roc_auc_score(y_true, y_pred_proba[:, 1])\n\n# Multiclass (One-vs-Rest)\nroc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='macro')\n</code></pre>"},{"location":"contracts/experiment_contract/#52-per-class-metrics","title":"5.2 Per-Class Metrics","text":"Metric Formula Purpose Precision TP / (TP + FP) False positive control Recall TP / (TP + FN) Critical for rare classes F1-Score 2\u00d7(P\u00d7R)/(P+R) Balance of P and R Support Count per class Context for interpretation"},{"location":"contracts/experiment_contract/#53-confusion-matrix","title":"5.3 Confusion Matrix","text":"Task Size Focus Areas Binary 2\u00d72 False Negatives (missed attacks) Multiclass 10\u00d710 Rows for Worms, Shellcode, Backdoor, Analysis"},{"location":"contracts/experiment_contract/#54-rare-class-analysis","title":"5.4 Rare Class Analysis","text":"<p>Rare Classes (Training &lt; 3% of data):</p> Class Index Training Samples Target Recall Worms 9 130 &gt; 20% Shellcode 8 1,133 &gt; 40% Backdoor 3 1,746 &gt; 50% Analysis 2 2,000 &gt; 60% <p>Analysis Report Structure:</p> <pre><code>{\n  \"rare_class_analysis\": {\n    \"Worms\": {\n      \"s0_recall\": 0.02,\n      \"s1_recall\": 0.15,\n      \"s2a_recall\": 0.25,\n      \"improvement_s2a_vs_s0\": 0.23\n    },\n    \"Shellcode\": {...},\n    \"Backdoor\": {...},\n    \"Analysis\": {...}\n  }\n}\n</code></pre>"},{"location":"contracts/experiment_contract/#6-output-artifacts","title":"6. Output Artifacts","text":""},{"location":"contracts/experiment_contract/#61-per-experiment-outputs","title":"6.1 Per-Experiment Outputs","text":"Artifact Format Path Contents Metrics JSON JSON <code>results/metrics/{exp_id}.json</code> All metrics Confusion Matrix PNG <code>results/figures/cm_{exp_id}.png</code> Heatmap Model Object joblib <code>results/models/{exp_id}.joblib</code> Trained model Predictions NPY <code>results/predictions/{exp_id}.npy</code> Test set predictions"},{"location":"contracts/experiment_contract/#62-metrics-json-schema","title":"6.2 Metrics JSON Schema","text":"<pre><code>{\n  \"experiment_id\": \"binary_rf_s1\",\n  \"task\": \"binary\",\n  \"model\": \"rf\",\n  \"strategy\": \"s1\",\n  \"timestamp\": \"2026-01-17T12:00:00Z\",\n  \"training_time_seconds\": 120.5,\n  \"overall\": {\n    \"accuracy\": 0.89,\n    \"macro_f1\": 0.87,\n    \"weighted_f1\": 0.89,\n    \"g_mean\": 0.88,\n    \"roc_auc\": 0.92\n  },\n  \"per_class\": {\n    \"0\": {\"precision\": 0.85, \"recall\": 0.92, \"f1\": 0.88, \"support\": 37000},\n    \"1\": {\"precision\": 0.91, \"recall\": 0.84, \"f1\": 0.87, \"support\": 45332}\n  },\n  \"confusion_matrix\": [[34040, 2960], [7253, 38079]],\n  \"rare_class_analysis\": null\n}\n</code></pre>"},{"location":"contracts/experiment_contract/#63-aggregated-outputs","title":"6.3 Aggregated Outputs","text":"Artifact Path Contents Experiment Log <code>results/experiment_log.csv</code> Summary of all experiments Comparison Tables <code>results/tables/</code> LaTeX-ready comparison tables Final Report <code>reports/final_results.md</code> Narrative analysis"},{"location":"contracts/experiment_contract/#64-experiment-log-schema","title":"6.4 Experiment Log Schema","text":"Column Type Description <code>run_id</code> String Unique run identifier <code>experiment_id</code> String Experiment name <code>timestamp</code> DateTime Run timestamp <code>task</code> String 'binary' or 'multi' <code>model</code> String 'lr', 'rf', 'xgb' <code>strategy</code> String 's0', 's1', 's2a', 's2b' <code>accuracy</code> Float Accuracy score <code>macro_f1</code> Float Macro F1 score <code>weighted_f1</code> Float Weighted F1 score <code>g_mean</code> Float Geometric mean score <code>roc_auc</code> Float ROC-AUC score <code>training_time</code> Float Seconds to train <code>status</code> String 'success' or 'failed'"},{"location":"contracts/experiment_contract/#7-success-criteria","title":"7. Success Criteria","text":""},{"location":"contracts/experiment_contract/#71-execution-criteria","title":"7.1 Execution Criteria","text":"Criterion Requirement Validation Completeness All 18 core experiments completed Check experiment_log.csv Reproducibility Same seed \u2192 identical results Re-run and compare No Crashes All experiments succeed status == 'success' Reasonable Runtime Total &lt; 8 hours Sum(training_time)"},{"location":"contracts/experiment_contract/#72-quality-criteria","title":"7.2 Quality Criteria","text":"Criterion Requirement Validation Non-trivial Metrics Accuracy not 0 or 1 0.5 &lt; accuracy &lt; 1.0 Strategy Variation Metrics differ across S0/S1/S2 Compare JSONs Model Variation Metrics differ across LR/RF/XGB Compare JSONs Rare Class Focus Worms/Shellcode metrics recorded Check per_class in multi"},{"location":"contracts/experiment_contract/#73-scientific-validity-criteria","title":"7.3 Scientific Validity Criteria","text":"Criterion Requirement Validation No Data Leakage Test metrics from test set only Code review Proper Stratification Val set class ratios match train Distribution check Correct Resampling S2 only on training Code review Consistent Preprocessing Same pipeline for all Config-driven"},{"location":"contracts/experiment_contract/#8-experiment-execution-protocol","title":"8. Experiment Execution Protocol","text":""},{"location":"contracts/experiment_contract/#81-execution-order","title":"8.1 Execution Order","text":"<pre><code>Phase 1: Preprocessing\n\u251c\u2500\u2500 Load raw data\n\u251c\u2500\u2500 Apply preprocessing pipeline (fit on train only)\n\u251c\u2500\u2500 Create train/val/test splits\n\u2514\u2500\u2500 Save processed data artifacts\n\nPhase 2: Experiments (Loop)\n\u251c\u2500\u2500 For each task in [binary, multi]:\n\u2502   \u251c\u2500\u2500 For each model in [lr, rf, xgb]:\n\u2502   \u2502   \u251c\u2500\u2500 For each strategy in [s0, s1, s2a]:\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Apply strategy to training data\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Train model\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Evaluate on test set\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Compute all metrics\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Save artifacts\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 Log to experiment_log\n\nPhase 3: Analysis\n\u251c\u2500\u2500 Aggregate results\n\u251c\u2500\u2500 Generate comparison tables\n\u251c\u2500\u2500 Create visualizations\n\u2514\u2500\u2500 Compile final report\n</code></pre>"},{"location":"contracts/experiment_contract/#82-error-handling","title":"8.2 Error Handling","text":"Error Type Handling Recovery Memory Error (SMOTE) Skip S2b, log warning Continue with other experiments Convergence Warning (LR) Increase max_iter, retry Log warning, proceed File I/O Error Retry 3 times Skip and log error Unexpected Exception Log full traceback Skip experiment, continue grid"},{"location":"contracts/experiment_contract/#83-checkpointing","title":"8.3 Checkpointing","text":"<ul> <li>Save processed data after preprocessing</li> <li>Save metrics JSON immediately after each experiment</li> <li>Experiment log updated after each experiment</li> <li>Support resuming from failed point</li> </ul>"},{"location":"contracts/experiment_contract/#9-tracking-monitoring","title":"9. Tracking &amp; Monitoring","text":""},{"location":"contracts/experiment_contract/#91-progress-tracking","title":"9.1 Progress Tracking","text":"<pre><code>Experiment Progress: [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591] 18/18 (100%)\nCurrent: multi_xgb_s2a\nElapsed: 02:45:30\nETA: 00:15:00\n</code></pre>"},{"location":"contracts/experiment_contract/#92-real-time-metrics-dashboard-optional","title":"9.2 Real-time Metrics Dashboard (Optional)","text":"Experiment Status Accuracy Macro F1 G-Mean binary_lr_s0 \u2705 Done 0.87 0.85 0.86 binary_lr_s1 \u2705 Done 0.86 0.86 0.86 ... ... ... ... ... multi_xgb_s2a \ud83d\udd04 Running - - -"},{"location":"contracts/experiment_contract/#10-documentation-requirements","title":"10. Documentation Requirements","text":""},{"location":"contracts/experiment_contract/#101-per-experiment-documentation","title":"10.1 Per-Experiment Documentation","text":"<p>Each experiment must include: - Exact hyperparameters used - Training data shape (after resampling if applicable) - Execution time - Any warnings or issues encountered</p>"},{"location":"contracts/experiment_contract/#102-final-documentation","title":"10.2 Final Documentation","text":"<ul> <li>Complete methodology description</li> <li>Experiment grid summary</li> <li>Results tables (binary and multiclass)</li> <li>Rare class analysis section</li> <li>Limitations and future work</li> </ul>"},{"location":"contracts/experiment_contract/#appendix-a-experiment-id-naming-convention","title":"Appendix A: Experiment ID Naming Convention","text":"<pre><code>{task}_{model}_{strategy}\n</code></pre> Component Values Example task binary, multi binary model lr, rf, xgb rf strategy s0, s1, s2a, s2b s1 <p>Examples: - <code>binary_lr_s0</code> - Binary Logistic Regression, No balancing - <code>multi_rf_s1</code> - Multiclass Random Forest, Class weighting - <code>binary_xgb_s2a</code> - Binary XGBoost, RandomOverSampler</p>"},{"location":"contracts/experiment_contract/#appendix-b-expected-results-summary-template","title":"Appendix B: Expected Results Summary Template","text":""},{"location":"contracts/experiment_contract/#binary-classification-results","title":"Binary Classification Results","text":"Model Strategy Accuracy Macro F1 G-Mean ROC-AUC LR S0 LR S1 LR S2a RF S0 RF S1 RF S2a XGB S0 XGB S1 XGB S2a"},{"location":"contracts/experiment_contract/#multiclass-rare-class-recall","title":"Multiclass Rare Class Recall","text":"Model Strategy Worms Shellcode Backdoor Analysis LR S0 LR S1 LR S2a RF S0 RF S1 RF S2a XGB S0 XGB S1 XGB S2a"},{"location":"contracts/experiment_contract/#11-literature-benchmark-comparison","title":"11. Literature Benchmark Comparison","text":"<p>[!IMPORTANT] This table establishes the competitive landscape for contextualizing our results.</p>"},{"location":"contracts/experiment_contract/#111-binary-classification-benchmarks","title":"11.1 Binary Classification Benchmarks","text":"Study Year Dataset Model Accuracy Macro-F1 Notes Primartha &amp; Tama 2017 UNSW-NB15 RF (800 trees) 95.5% N/R FAR 7.22% Amin et al. 2021 UNSW-NB15 RF + ANOVA 99.28% N/R Cloud env More et al. 2024 UNSW-NB15 RF (optimized) 99.45% N/R Feature selection This Study 2026 UNSW-NB15 RF + S1 90.36% 90.08% Focus on rare classes"},{"location":"contracts/experiment_contract/#112-multiclass-classification-benchmarks","title":"11.2 Multiclass Classification Benchmarks","text":"Study Year Dataset Model Accuracy Macro-F1 Rare Class Focus Kasongo &amp; Sun 2020 UNSW-NB15 XGB + DT 67.57% N/R Partial Vinayakumar et al. 2019 UNSW-NB15 DNN ~75% N/R No This Study 2026 UNSW-NB15 XGB + S1 69.57% 52.40% Yes (Worms, Shellcode) <p>[!NOTE] Our multiclass Macro-F1 of 52.40% with explicit rare-class analysis represents a unique contribution, as prior work does not report per-class metrics for Worms/Shellcode.</p>"},{"location":"contracts/experiment_contract/#12-ablation-study-design","title":"12. Ablation Study Design","text":""},{"location":"contracts/experiment_contract/#121-ablation-matrix","title":"12.1 Ablation Matrix","text":"ID Ablation Hypothesis Baseline Comparison A1 Remove S2 (SMOTE/ROS) Oversampling is critical for rare class recall multi_xgb_s2a multi_xgb_s0 A2 Remove S1 (Class Weights) CSL improves recall/precision trade-off multi_rf_s1 multi_rf_s0 A3 Feature Subset (Top-20 by RF importance) Feature redundancy is acceptable Full 196 features Top-20 features A4 Single Model (RF only) Model choice is secondary to strategy All 3 models RF only"},{"location":"contracts/experiment_contract/#122-ablation-success-criteria","title":"12.2 Ablation Success Criteria","text":"Ablation Expected Result Conclusion If True A1 Worms recall drops by &gt;15pp Oversampling essential for rare classes A2 Backdoor recall drops by &gt;10pp CSL provides complementary benefit A3 Macro-F1 drops by &lt;2pp Feature redundancy is tolerable A4 RF \u2248 XGB performance Strategy &gt; Model for this dataset"},{"location":"contracts/experiment_contract/#13-statistical-validation-protocol","title":"13. Statistical Validation Protocol","text":""},{"location":"contracts/experiment_contract/#131-confidence-interval-computation","title":"13.1 Confidence Interval Computation","text":"<pre><code>def compute_bootstrap_ci(y_true, y_pred, metric_fn, n_iter=1000, ci=0.95):\n    \"\"\"Bootstrap 95% CI for any metric.\"\"\"\n    scores = []\n    n = len(y_true)\n    for _ in range(n_iter):\n        idx = np.random.choice(n, n, replace=True)\n        scores.append(metric_fn(y_true[idx], y_pred[idx]))\n    lower = np.percentile(scores, (1 - ci) / 2 * 100)\n    upper = np.percentile(scores, (1 + ci) / 2 * 100)\n    return {\"mean\": np.mean(scores), \"ci_lower\": lower, \"ci_upper\": upper}\n</code></pre>"},{"location":"contracts/experiment_contract/#132-paired-comparison-protocol","title":"13.2 Paired Comparison Protocol","text":"Comparison Test \u03b1 (Bonferroni) Interpretation Model A vs Model B (same test set) McNemar's \u03c7\u00b2 0.05/18 = 0.0028 Significant if p &lt; 0.0028 Strategy S1 vs S0 Wilcoxon Signed-Rank 0.05/3 = 0.0167 Significant if p &lt; 0.0167"},{"location":"contracts/experiment_contract/#133-effect-size-reporting","title":"13.3 Effect Size Reporting","text":"Metric Effect Size Measure Interpretation Classification Agreement Cohen's \u03ba \u03ba &gt; 0.8 = excellent Recall Improvement \u0394 (percentage points) Report with 95% CI"},{"location":"contracts/experiment_contract/#14-team-coordination-protocol","title":"14. Team Coordination Protocol","text":"<p>This section defines how the personas in <code>persona/team.yaml</code> should collaborate.</p>"},{"location":"contracts/experiment_contract/#141-workflow-stages","title":"14.1 Workflow Stages","text":"<pre><code>graph LR\n    A[Lead: Define Scope] --&gt; B[Executor: Implement]\n    B --&gt; C[Auditor: Validate]\n    C --&gt; D{Pass?}\n    D -- No --&gt; B\n    D -- Yes --&gt; E[Author: Document]\n    E --&gt; F[Reviewer: Stats Check]\n    F --&gt; G{Approved?}\n    G -- No --&gt; E\n    G -- Yes --&gt; H[Lead: Finalize]\n</code></pre>"},{"location":"contracts/experiment_contract/#142-handoff-checklist","title":"14.2 Handoff Checklist","text":"From To Artifact Required Validation Gate Lead Executor <code>implementation_plan.md</code> User approval Executor Auditor <code>experiment_log.csv</code>, <code>predictions.csv</code> All 18 runs complete Auditor Author QA Report (pass/fail) No leakage, artifacts exist Author Reviewer Draft paper sections Metrics match artifacts Reviewer Lead Statistical validation report CIs computed, no overclaims"},{"location":"contracts/experiment_contract/#143-conflict-resolution","title":"14.3 Conflict Resolution","text":"Issue Resolution Path Discrepancy in metrics Auditor re-runs validation, traces to source Statistical claim disputed Reviewer provides CI/p-value evidence Scope creep Lead enforces original <code>experiment_contract.md</code>"},{"location":"contracts/experiment_contract/#document-history","title":"Document History","text":"Version Date Changes 1.0 2026-01-17 Initial contract 2.0 2026-01-17 Complete enhancement with hyperparameters, metrics, and protocols 3.0 2026-01-18 Added \u00a711-14: Literature benchmarks, ablation design, statistical validation, team coordination"},{"location":"contracts/methodology_contract/","title":"State of the Art Analysis: Intrusion Detection on UNSW-NB15","text":"<p>Document Version: 2.0 Last Updated: 2026-01-17 Status: APPROVED FOR IMPLEMENTATION</p>"},{"location":"contracts/methodology_contract/#1-introduction","title":"1. Introduction","text":"<p>This document provides a comprehensive analysis of the proposed methodology for developing an Intrusion Detection System (IDS) using the UNSW-NB15 dataset. The analysis compares the proposed approach against current State of the Art (SOTA) research to ensure scientific rigor, relevance, and contribution significance.</p>"},{"location":"contracts/methodology_contract/#11-research-objectives","title":"1.1 Research Objectives","text":"Objective Description Priority O1 Build binary IDS models (Normal vs Attack) HIGH O2 Build multiclass IDS models (Normal + 9 attacks) HIGH O3 Systematic comparison of 3 imbalance strategies HIGH O4 Per-class analysis focusing on rare attacks HIGH O5 Reproducible baseline pipeline MEDIUM"},{"location":"contracts/methodology_contract/#12-research-questions","title":"1.2 Research Questions","text":"<ul> <li>RQ1: How does class imbalance in UNSW-NB15 affect the performance of classical ML models on binary vs multiclass intrusion detection tasks?</li> <li>RQ2: To what extent do class weighting and oversampling improve detection of minority attack classes compared to raw imbalanced data?</li> <li>RQ3: Is there a consistent pattern in how different models (LR, RF, XGB) respond to imbalance-handling methods across binary and multiclass tasks?</li> <li>RQ4: For extremely rare classes (Worms, Shellcode), does oversampling significantly improve recall without degrading majority class performance?</li> </ul>"},{"location":"contracts/methodology_contract/#2-dataset-context-unsw-nb15","title":"2. Dataset Context: UNSW-NB15","text":""},{"location":"contracts/methodology_contract/#21-dataset-overview","title":"2.1 Dataset Overview","text":"<p>The UNSW-NB15 dataset is a modern benchmark for Network Intrusion Detection Systems (NIDS), developed by the Australian Centre for Cyber Security (ACCS) using the IXIA PerfectStorm toolset. It supersedes legacy datasets (KDD99, NSL-KDD) by reflecting contemporary threat landscapes with nine distinct attack families.</p> <p>Key Statistics:</p> Metric Training Set Testing Set Total Total Records 175,341 82,332 257,673 Normal 56,000 (31.9%) 37,000 (44.9%) 93,000 Attack 119,341 (68.1%) 45,332 (55.1%) 164,673 Features 42 (after ID removal) 42 42"},{"location":"contracts/methodology_contract/#22-class-distribution-analysis","title":"2.2 Class Distribution Analysis","text":""},{"location":"contracts/methodology_contract/#221-multiclass-distribution-attack-categories","title":"2.2.1 Multiclass Distribution (Attack Categories)","text":"Attack Category Training Count Testing Count Training % Imbalance Ratio Normal 56,000 37,000 31.94% 1.00 (baseline) Generic 40,000 18,871 22.82% 0.71 Exploits 33,393 11,132 19.04% 0.60 Fuzzers 18,184 6,062 10.37% 0.32 DoS 12,264 4,089 6.99% 0.22 Reconnaissance 10,491 3,496 5.98% 0.19 Analysis 2,000 677 1.14% 0.04 Backdoor 1,746 583 1.00% 0.03 Shellcode 1,133 378 0.65% 0.02 Worms 130 44 0.07% 0.002 <p>[!WARNING] Extreme Imbalance Alert: Worms class represents only 0.07% of training data (130 samples). Traditional ML models will likely achieve 0% recall on this class without intervention.</p>"},{"location":"contracts/methodology_contract/#222-rare-class-identification","title":"2.2.2 Rare Class Identification","text":"<p>Classes requiring special analysis (&lt; 3% of training data):</p> Class Training Samples Category Worms 130 Critically Rare Shellcode 1,133 Rare Backdoor 1,746 Rare Analysis 2,000 Moderately Rare"},{"location":"contracts/methodology_contract/#23-sota-alignment","title":"2.3 SOTA Alignment","text":"<ul> <li>Dataset Selection: UNSW-NB15 is standard practice in recent literature (2019-2025). Older datasets are correctly discarded.</li> <li>Challenge: The dataset is effectively \"solved\" for binary classification (High Accuracy &gt; 99%), but remains challenging for multiclass classification of rare events.</li> </ul>"},{"location":"contracts/methodology_contract/#3-methodology-assessment","title":"3. Methodology Assessment","text":""},{"location":"contracts/methodology_contract/#31-problem-formulation","title":"3.1 Problem Formulation","text":"<p>Proposed: Dual-track approach (Binary vs. Multiclass) with specific focus on Class Imbalance.</p> <p>Analysis:</p> Aspect Strength Gap Addressed Separation of tasks Allows identifying where \"Overall Accuracy\" masks failures Many papers only report high binary accuracy Focus on reliability More operationally relevant than marginal accuracy gains SOTA papers chase 99.5% \u2192 99.6% Per-class analysis Exposes detection failures on rare attacks Most papers use Weighted F1 dominated by majority"},{"location":"contracts/methodology_contract/#32-data-preprocessing-pipeline","title":"3.2 Data Preprocessing Pipeline","text":"<p>Proposed Pipeline:</p> <pre><code>Raw Data \u2192 Feature Cleaning \u2192 Missing Value Handling \u2192 Label Preparation\n         \u2192 Encoding \u2192 Scaling \u2192 Train/Val/Test Split \u2192 Ready for Modeling\n</code></pre>"},{"location":"contracts/methodology_contract/#321-feature-cleaning-and-selection","title":"3.2.1 Feature Cleaning and Selection","text":"Step Action Justification Drop <code>id</code> Row identifier, non-predictive Standard practice Drop <code>srcip</code>, <code>dstip</code> High cardinality, not generalizable Prevents overfitting to specific IPs Drop <code>sport</code>, <code>dsport</code> Contains ports, can be kept as numeric but dropped for simplicity Reduces noise Drop <code>stime</code>, <code>ltime</code> Timestamps, not generalizable Prevents temporal leakage Retain all 42 features All predictive features kept Maximizes information for rare class detection <p>[!NOTE] Decision Rationale: Aggressive feature selection often prioritizes features explaining majority variance, potentially discarding subtle signals for rare attacks. All 42 predictive features are retained.</p>"},{"location":"contracts/methodology_contract/#322-missing-value-handling","title":"3.2.2 Missing Value Handling","text":"Feature Type Strategy Implementation Numerical Median Imputation <code>SimpleImputer(strategy='median')</code> Categorical \"missing\" token <code>SimpleImputer(strategy='constant', fill_value='missing')</code>"},{"location":"contracts/methodology_contract/#323-label-preparation","title":"3.2.3 Label Preparation","text":"<p>Binary Classification:</p> <pre><code>y_binary = (attack_cat != 'Normal').astype(int)  # 0 = Normal, 1 = Attack\n</code></pre> <p>Multiclass Classification:</p> <pre><code>label_encoder = LabelEncoder()\ny_multi = label_encoder.fit_transform(attack_cat)  # 0-9 classes\n</code></pre>"},{"location":"contracts/methodology_contract/#324-encoding-and-scaling","title":"3.2.4 Encoding and Scaling","text":"Feature Type Encoding Implementation Categorical (<code>proto</code>, <code>state</code>, <code>service</code>) One-Hot Encoding <code>OneHotEncoder(handle_unknown='ignore', sparse=False)</code> Numerical (all others) StandardScaler <code>StandardScaler()</code> <p>Expected Dimensionality After Encoding:</p> Stage Feature Count Original (after drops) 42 After One-Hot ~196 (varies based on cardinality)"},{"location":"contracts/methodology_contract/#325-data-splitting-strategy","title":"3.2.5 Data Splitting Strategy","text":"<pre><code>Official Train Set (175,341) \u2500\u252c\u2500\u2192 Training (80%): 140,273\n                              \u2514\u2500\u2192 Validation (20%): 35,068 (Stratified)\n\nOfficial Test Set (82,332) \u2500\u2500\u2500\u2192 Test (100%): 82,332 (UNTOUCHED until final evaluation)\n</code></pre> <p>[!CAUTION] Data Leakage Prevention: 1. Preprocessing statistics (mean, std, categories) computed on TRAINING ONLY 2. Resampling (SMOTE/ROS) applied ONLY to training split 3. Test set NEVER used for any tuning or validation</p>"},{"location":"contracts/methodology_contract/#33-imbalance-handling-strategies","title":"3.3 Imbalance Handling Strategies","text":""},{"location":"contracts/methodology_contract/#strategy-matrix","title":"Strategy Matrix","text":"Strategy ID Name Description When Applied S0 No Balancing (Baseline) Raw imbalanced data Training S1 Class Weighting Inverse frequency weights During model training S2a Random Oversampling Duplicate minority samples Training data only S2b SMOTE Synthetic minority oversampling Training data only (optional)"},{"location":"contracts/methodology_contract/#331-s0-no-balancing-baseline","title":"3.3.1 S0: No Balancing (Baseline)","text":"<ul> <li>Purpose: Establish baseline performance showing how models fail on minority classes</li> <li>Expected Outcome: High accuracy, near-zero recall for Worms/Shellcode</li> <li>Implementation: No modification to training data</li> </ul>"},{"location":"contracts/methodology_contract/#332-s1-class-weighting","title":"3.3.2 S1: Class Weighting","text":"<p>Binary Classification:</p> <pre><code>class_weight = 'balanced'  # Automatically computes n_samples / (n_classes * np.bincount(y))\n</code></pre> <p>Multiclass Classification:</p> <pre><code>from sklearn.utils.class_weight import compute_class_weight\nweights = compute_class_weight('balanced', classes=np.unique(y), y=y)\nsample_weight = weights[y]  # For models without class_weight param\n</code></pre> <p>XGBoost Binary Specific:</p> <pre><code>scale_pos_weight = len(y_train[y_train==0]) / len(y_train[y_train==1])\n</code></pre>"},{"location":"contracts/methodology_contract/#333-s2-resampling-strategies","title":"3.3.3 S2: Resampling Strategies","text":"<p>S2a: RandomOverSampler</p> <pre><code>from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n</code></pre> <p>S2b: SMOTE (Synthetic Minority Over-sampling Technique)</p> <pre><code>from imblearn.over_sampling import SMOTE\nsmote = SMOTE(\n    k_neighbors=5,           # Number of nearest neighbors\n    random_state=42,\n    n_jobs=-1\n)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n</code></pre> <p>[!IMPORTANT] SMOTE Constraint: For classes with &lt; 6 samples, SMOTE will fail due to k_neighbors requirement. Use RandomOverSampler as fallback.</p> <p>Memory Considerations: | Strategy | Training Size (Binary) | Training Size (10-class) | |----------|------------------------|--------------------------| | S0 | 140,273 | 140,273 | | S1 | 140,273 | 140,273 | | S2a/S2b | ~224,000 (\u22481.6x) | ~560,000 (\u22484x) |</p>"},{"location":"contracts/methodology_contract/#34-model-selection","title":"3.4 Model Selection","text":""},{"location":"contracts/methodology_contract/#341-model-families","title":"3.4.1 Model Families","text":"Model Family Strengths Weaknesses Logistic Regression Linear Interpretable, fast, probabilistic Limited by linear separability Random Forest Bagging Robust, handles non-linearity Memory intensive, slower XGBoost Boosting SOTA for tabular data, efficient Requires careful tuning"},{"location":"contracts/methodology_contract/#342-hyperparameter-specifications","title":"3.4.2 Hyperparameter Specifications","text":"<p>Logistic Regression:</p> <pre><code>LogisticRegression(\n    C=1.0,                    # Regularization (tune: [0.01, 0.1, 1, 10])\n    solver='saga',            # Supports L1/L2/elastic-net\n    max_iter=1000,            # Sufficient for convergence\n    multi_class='multinomial',# For multiclass\n    class_weight='balanced',  # For S1 strategy\n    n_jobs=-1,\n    random_state=42\n)\n</code></pre> <p>Random Forest:</p> <pre><code>RandomForestClassifier(\n    n_estimators=200,         # Tune: [100, 200]\n    max_depth=20,             # Tune: [10, 20, None]\n    min_samples_split=5,\n    min_samples_leaf=2,\n    class_weight='balanced',  # For S1 strategy\n    n_jobs=-1,\n    random_state=42\n)\n</code></pre> <p>XGBoost:</p> <pre><code>XGBClassifier(\n    n_estimators=200,         # Tune: [100, 200]\n    learning_rate=0.1,        # Tune: [0.01, 0.1]\n    max_depth=10,             # Tune: [5, 10]\n    scale_pos_weight=None,    # Computed for S1 binary\n    use_label_encoder=False,\n    eval_metric='mlogloss',   # For multiclass\n    n_jobs=-1,\n    random_state=42\n)\n</code></pre>"},{"location":"contracts/methodology_contract/#343-hyperparameter-tuning-protocol","title":"3.4.3 Hyperparameter Tuning Protocol","text":"Aspect Specification Method Grid Search with Stratified K-Fold Folds 3 (for time efficiency) Scoring Macro F1 (to weight minority classes equally) Data Used Validation Set ONLY Best Model Selection Highest Macro F1 on validation"},{"location":"contracts/methodology_contract/#35-evaluation-metrics","title":"3.5 Evaluation Metrics","text":""},{"location":"contracts/methodology_contract/#351-overall-metrics","title":"3.5.1 Overall Metrics","text":"Metric Formula Purpose Accuracy (TP + TN) / Total Baseline; misleading for imbalanced data Macro F1 Mean of per-class F1 Equal weight to all classes Weighted F1 Weighted mean by support Reflects majority class performance G-Mean \u221a(Sensitivity \u00d7 Specificity) Primary metric for imbalanced data ROC-AUC Area under ROC curve Threshold-independent performance <p>G-Mean Calculation:</p> <p>For Binary:</p> <pre><code>from imblearn.metrics import geometric_mean_score\ng_mean = geometric_mean_score(y_true, y_pred)\n</code></pre> <p>For Multiclass:</p> <pre><code>g_mean = geometric_mean_score(y_true, y_pred, average='macro')\n</code></pre>"},{"location":"contracts/methodology_contract/#352-per-class-metrics","title":"3.5.2 Per-Class Metrics","text":"Metric Formula Purpose Precision TP / (TP + FP) False positive control Recall TP / (TP + FN) Critical for rare classes F1-Score 2 \u00d7 (P \u00d7 R) / (P + R) Harmonic mean Support Count per class Context for interpretation"},{"location":"contracts/methodology_contract/#353-roc-auc-computation","title":"3.5.3 ROC-AUC Computation","text":"<p>Binary Classification:</p> <pre><code>from sklearn.metrics import roc_auc_score\nroc_auc = roc_auc_score(y_true, y_pred_proba)\n</code></pre> <p>Multiclass Classification:</p> <pre><code>roc_auc = roc_auc_score(\n    y_true, \n    y_pred_proba, \n    multi_class='ovr',      # One-vs-Rest\n    average='macro'         # Equal weight per class\n)\n</code></pre>"},{"location":"contracts/methodology_contract/#354-confusion-matrix-specifications","title":"3.5.4 Confusion Matrix Specifications","text":"Task Size Key Focus Binary 2\u00d72 False Negatives (missed attacks) Multiclass 10\u00d710 Rare class rows (Worms, Shellcode, Backdoor, Analysis)"},{"location":"contracts/methodology_contract/#36-rare-class-analysis-protocol","title":"3.6 Rare-Class Analysis Protocol","text":"<p>[!IMPORTANT] This is the core differentiator of the study. Special attention must be given to: - Worms (130 training samples) - Shellcode (1,133 samples) - Backdoor (1,746 samples) - Analysis (2,000 samples)</p>"},{"location":"contracts/methodology_contract/#361-success-thresholds","title":"3.6.1 Success Thresholds","text":"Class Baseline (S0) Expected Recall Target Recall (S2) Improvement Threshold Worms 0% - 5% &gt; 20% +15pp Shellcode 5% - 20% &gt; 40% +20pp Backdoor 10% - 30% &gt; 50% +20pp Analysis 20% - 40% &gt; 60% +20pp"},{"location":"contracts/methodology_contract/#362-analysis-methodology","title":"3.6.2 Analysis Methodology","text":"<ol> <li>Baseline Documentation: Record exact recall for each rare class under S0</li> <li>Strategy Comparison: Create comparison tables showing:</li> <li>Recall improvement (S1 vs S0, S2 vs S0)</li> <li>Trade-off with majority class precision</li> <li>Statistical Significance: McNemar's test for paired comparisons</li> <li>Visualization: Per-class recall bar charts across strategies</li> </ol>"},{"location":"contracts/methodology_contract/#4-technology-stack","title":"4. Technology Stack","text":"Component Tool Version Programming Language Python 3.10+ Core ML Library scikit-learn 1.3+ Imbalanced Learning imbalanced-learn 0.11+ Gradient Boosting xgboost 2.0+ Data Manipulation pandas, numpy Latest Visualization matplotlib, seaborn Latest Configuration PyYAML Latest Experiment Tracking JSON/CSV logs N/A"},{"location":"contracts/methodology_contract/#5-strategic-recommendations","title":"5. Strategic Recommendations","text":""},{"location":"contracts/methodology_contract/#51-implementation-priorities","title":"5.1 Implementation Priorities","text":"Priority Recommendation Rationale 1 Strict data isolation Prevents leakage, ensures validity 2 Document S0 baseline thoroughly Shows severity of imbalance problem 3 Reproducibility via config Enables fair comparison 4 Focus on per-class metrics Core contribution of the study"},{"location":"contracts/methodology_contract/#52-experimental-best-practices","title":"5.2 Experimental Best Practices","text":"<ol> <li>Seed Control: All random operations use <code>random_state=42</code></li> <li>Checkpoint Saving: Save model after each experiment</li> <li>Metric Logging: JSON output per experiment</li> <li>Version Control: Git commit after each phase</li> </ol>"},{"location":"contracts/methodology_contract/#6-conclusion","title":"6. Conclusion","text":"<p>Status: The methodology meets all requirements for a high-quality scientific study and is ready for implementation.</p> <p>Key Differentiators: - Systematic comparison of 3 strategies \u00d7 3 models \u00d7 2 tasks = 18 experiments - Explicit focus on rare attack classes (Worms, Shellcode, Backdoor, Analysis) - G-Mean as primary metric (not accuracy) - Transparent, reproducible pipeline</p>"},{"location":"contracts/methodology_contract/#appendix-a-complete-feature-list","title":"Appendix A: Complete Feature List","text":""},{"location":"contracts/methodology_contract/#a1-features-to-drop-identifiers","title":"A.1 Features to Drop (Identifiers)","text":"Feature Type Reason for Dropping <code>id</code> Integer Row identifier <code>srcip</code> String Source IP - high cardinality <code>dstip</code> String Destination IP - high cardinality <code>sport</code> Integer Source Port <code>dsport</code> Integer Destination Port <code>stime</code> Timestamp Start time - temporal leakage <code>ltime</code> Timestamp Last time - temporal leakage"},{"location":"contracts/methodology_contract/#a2-categorical-features-one-hot-encoded","title":"A.2 Categorical Features (One-Hot Encoded)","text":"Feature Description Cardinality <code>proto</code> Protocol (tcp, udp, etc.) ~130 <code>state</code> Connection state ~15 <code>service</code> Service type ~13"},{"location":"contracts/methodology_contract/#a3-numerical-features-scaled","title":"A.3 Numerical Features (Scaled)","text":"<p>All remaining features are treated as numerical and scaled using StandardScaler. Total: 36 features.</p>"},{"location":"contracts/methodology_contract/#a4-target-labels","title":"A.4 Target Labels","text":"Label Type Values <code>label</code> Binary 0 (Normal), 1 (Attack) <code>attack_cat</code> Multiclass Normal, Fuzzers, Analysis, Backdoor, DoS, Exploits, Generic, Reconnaissance, Shellcode, Worms"},{"location":"contracts/methodology_contract/#appendix-b-experiment-naming-convention","title":"Appendix B: Experiment Naming Convention","text":"<pre><code>{task}_{model}_{strategy}\n</code></pre> <p>Examples: - <code>binary_lr_s0</code> - Binary Logistic Regression, No balancing - <code>multi_rf_s1</code> - Multiclass Random Forest, Class weighting - <code>binary_xgb_s2a</code> - Binary XGBoost, RandomOverSampler - <code>multi_xgb_s2b</code> - Multiclass XGBoost, SMOTE</p>"},{"location":"contracts/methodology_contract/#7-novelty-statement-contributions","title":"7. Novelty Statement &amp; Contributions","text":"<p>[!IMPORTANT] This section explicitly states what makes this research unique for publication purposes.</p>"},{"location":"contracts/methodology_contract/#71-research-contributions","title":"7.1 Research Contributions","text":"# Contribution Novelty Evidence C1 First systematic comparison of 3 imbalance strategies \u00d7 3 classical ML models \u00d7 2 tasks (18 experiments) on UNSW-NB15 Prior work focuses on single strategy or single task Full experiment grid in <code>experiment_log.csv</code> C2 First explicit rare-class analysis with quantified recall targets (Worms &gt;20%, Shellcode &gt;40%) Prior work uses aggregate metrics that mask rare class failures <code>rare_class_report.csv</code> C3 Reproducible baseline pipeline with complete artifact contracts Most prior work lacks reproducibility artifacts <code>data_contract.md</code>, <code>experiment_contract.md</code> C4 Statistical validation protocol with bootstrap CIs and McNemar's tests Prior work reports point estimates without uncertainty <code>metric_confidence_intervals.csv</code>"},{"location":"contracts/methodology_contract/#72-gap-addressed","title":"7.2 Gap Addressed","text":"<p>The Critical Gap: Existing literature on UNSW-NB15 reports high binary accuracy (95-99%) but fails to address: - 0% recall for Worms class (only 130 training samples) - &lt;20% recall for Shellcode, Backdoor, Analysis classes - Lack of per-class metrics in published results</p> <p>Our Solution: Systematic imbalance handling with transparent per-class reporting.</p>"},{"location":"contracts/methodology_contract/#8-limitations-threats-to-validity","title":"8. Limitations &amp; Threats to Validity","text":""},{"location":"contracts/methodology_contract/#81-threats-to-internal-validity","title":"8.1 Threats to Internal Validity","text":"Threat Impact Mitigation Hyperparameter sensitivity Results may change with tuning Fixed hyperparameters documented in <code>configs/main.yaml</code> Single random seed Non-reproducible variance Seed=42 fixed; multi-seed recommended as future work Preprocessing order Encoding before scaling may affect some models Consistent pipeline for all experiments"},{"location":"contracts/methodology_contract/#82-threats-to-external-validity","title":"8.2 Threats to External Validity","text":"Threat Impact Mitigation Single dataset Results may not generalize UNSW-NB15 is standard benchmark; replication on other datasets recommended Classical ML only DL may outperform Excluded by design; provides interpretable baseline Synthetic attacks May not reflect real-world distribution Acknowledged; dataset limitation"},{"location":"contracts/methodology_contract/#83-threats-to-conclusion-validity","title":"8.3 Threats to Conclusion Validity","text":"Threat Impact Mitigation Multiple comparisons Inflated Type I error Bonferroni correction (\u03b1 = 0.05/18) Small rare class support Wide confidence intervals Explicitly report CIs for rare classes G-Mean sensitivity May overweight minority classes Report multiple metrics (Accuracy, Macro-F1, G-Mean)"},{"location":"contracts/methodology_contract/#84-acknowledged-limitations","title":"8.4 Acknowledged Limitations","text":"<ol> <li>No deep learning comparison: Intentionally excluded to establish classical ML baseline.</li> <li>No time-series modeling: Treats each connection independently.</li> <li>No adversarial robustness testing: Assumes clean test data.</li> <li>No real-time deployment evaluation: Focus is on research metrics, not latency.</li> </ol>"},{"location":"contracts/methodology_contract/#document-history","title":"Document History","text":"Version Date Changes 1.0 2026-01-17 Initial analysis 2.0 2026-01-17 Complete enhancement with specifications 3.0 2026-01-18 Added \u00a77 Novelty Statement, \u00a78 Limitations &amp; Threats to Validity"},{"location":"experiments/colab/","title":"Running Experiments in Google Colab","text":"<p>This guide explains how to execute the full 90-experiment grid in Google Colab using the provided <code>colab_full_grid.py</code> launcher. This approach is recommended for users who want to leverage cloud GPUs and ensure data persistence via Google Drive.</p>"},{"location":"experiments/colab/#overview","title":"\ud83d\ude80 Overview","text":"<p>The <code>colab_full_grid.py</code> script automates the entire experiment pipeline: 1.  Mounts Google Drive: Ensures your results are saved persistently (no data loss if runtime disconnects). 2.  Sets up the Environment: Clones the repository and installs dependencies. 3.  Optimizes Configuration: Adjusts parallelism (<code>n_jobs=1</code>) to prevent Colab resource crashes. 4.  Runs the Grid: Executes all experiments (Binary/Multi \u00d7 LR/RF/XGB \u00d7 S0/S1/S2a). 5.  Periodic Sync: Backs up results to Drive every 60 seconds.</p>"},{"location":"experiments/colab/#step-by-step-guide","title":"\ud83d\udee0\ufe0f Step-by-Step Guide","text":""},{"location":"experiments/colab/#1-open-the-notebook","title":"1. Open the Notebook","text":"<p>Open a new Google Colab notebook or upload the <code>UNSW_NB15_Full_Grid.ipynb</code> if you have it locally.</p>"},{"location":"experiments/colab/#2-copy-the-launcher-script","title":"2. Copy the Launcher Script","text":"<p>If not using the notebook, paste the content of <code>colab_full_grid.py</code> into a code cell. </p>"},{"location":"experiments/colab/#3-key-configurations","title":"3. Key Configurations","text":"<p>Look for the <code>CONFIGURATION</code> section in the script and adjust if necessary:</p> <pre><code># ==============================================================================\n# CONFIGURATION - MODIFY THESE AS NEEDED\n# ==============================================================================\nREPO_URL = \"https://github.com/StartDust/ML_PAPER_REVIEW.git\" # Your Repo URL\nBRANCH = \"main\"\nPROJECT_DIR = \"/content/ml_project\"\nDRIVE_BASE_DIR = \"/content/drive/MyDrive/UNSW_Archive\" # Where results save\n</code></pre>"},{"location":"experiments/colab/#4-run-the-cell","title":"4. Run the Cell","text":"<p>Execute the cell. You will be prompted to authorize Google Drive access.</p>"},{"location":"experiments/colab/#5-monitor-progress","title":"5. Monitor Progress","text":"<p>The script will print progress logs directly in the output. -   Dependencies: Installing... \u2705 -   Config: Optimizing for Colab... \u2705 -   Execution: Running Main...</p>"},{"location":"experiments/colab/#6-access-results","title":"6. Access Results","text":"<p>Once completed (or even during execution), navigate to your Google Drive folder defined in <code>DRIVE_BASE_DIR</code> (e.g., <code>UNSW_Archive/run_20260123_120000</code>). You will find: -   <code>results/metrics/</code>: JSON files for every experiment. -   <code>results/figures/</code>: Generated plots (Radar charts, Heatmaps). -   <code>results/experiment_log.csv</code>: Master summary log.</p>"},{"location":"experiments/colab/#colab-tips","title":"\u26a1 Colab Tips","text":"<ul> <li>Runtime Type: A Standard CPU runtime is often sufficient, but a GPU runtime will speed up XGBoost training significantly.</li> <li>Timeout: Colab runtimes can disconnect. The script's periodic sync ensures you don't lose progress. If disconnected, simply restart the cell; you can modify <code>main.yaml</code> to resume from a specific point if needed, or use the <code>clean_results(force_clean=False)</code> option in the script to resume (requires minor code tweak).</li> </ul>"},{"location":"experiments/colab/#troubleshooting","title":"\u26a0\ufe0f Troubleshooting","text":"<ul> <li>Drive Mount Fails: Ensure you are signed in and accept the permission popup.</li> <li>Resource Exhaustion: The script sets <code>n_jobs=1</code> to avoid this. Do not increase <code>n_jobs</code> in <code>main.yaml</code> when running on Colab.</li> </ul>"},{"location":"experiments/reproducibility/","title":"Reproducibility","text":"<p>Reproducibility is a non-negotiable tenet of this research. This document outlines the mechanisms we use to ensure that any researcher can replicate our results exactly.</p>"},{"location":"experiments/reproducibility/#seeding-strategy","title":"\ud83c\udf31 Seeding Strategy","text":"<p>All stochastic processes in the pipeline are controlled by fixed random seeds.</p> Component Controlled By Default Data Splitting <code>sklearn.model_selection</code> <code>random_state=42</code> Resampling (S2a/S2b) <code>imblearn.RandomOverSampler</code> <code>random_state=42</code> Model Training LR, RF, XGB instances <code>random_state=42</code> <p>Configuration (<code>configs/main.yaml</code>):</p> <pre><code>experiments:\n  seeds: [42, 43, 44, 45, 46]  # For multi-seed runs\n</code></pre>"},{"location":"experiments/reproducibility/#data-contracts","title":"\ud83d\udcdc Data Contracts","text":"<p>We enforce strict contracts to prevent data leakage and ensure feature consistency. See the full contract: Data Contract.</p>"},{"location":"experiments/reproducibility/#key-rules","title":"Key Rules:","text":"<ol> <li>Split First: Use official UNSW-NB15 train/test splits. Never mix.</li> <li>Fit on Train Only: All preprocessing transformations (scaling, encoding, imputation) are fitted exclusively on the training set.</li> <li>Transform All: The fitted transformers are then applied to validation and test sets.</li> <li>No Touch Test: The official test set is untouched until final evaluation.</li> </ol> <pre><code>graph LR\n    A[Official Train Set] --&gt; B{Fit Preprocessor}\n    B --&gt; C[Transform Train]\n    B --&gt; D[Transform Test]\n    C --&gt; E[Train Model]\n    D --&gt; F[Evaluate on Test]\n</code></pre>"},{"location":"experiments/reproducibility/#environment-locking","title":"\ud83d\udce6 Environment Locking","text":"<p>Dependencies are frozen in <code>requirements.txt</code> to prevent version drift.</p> <p>Critical Dependencies:</p> Package Version Purpose <code>scikit-learn</code> 1.3.x Core ML framework <code>xgboost</code> 2.0.x Gradient boosting <code>imbalanced-learn</code> 0.12.x Resampling algorithms <code>pandas</code> 2.0.x Data manipulation <code>numpy</code> 1.24.x Numerical operations <p>To create an identical environment:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\npip install -r requirements.txt\n</code></pre>"},{"location":"experiments/reproducibility/#verification-artifacts","title":"\u2705 Verification Artifacts","text":"<p>After every experiment run, the following artifacts are generated to enable verification:</p> Artifact Path Purpose Preprocessing Metadata <code>results/processed/preprocessing_metadata.json</code> Feature names, class mappings Experiment Log <code>results/experiment_log.csv</code> Timestamped record of all runs Raw Metrics <code>results/metrics/*.json</code> Complete metrics per experiment Configuration Snapshot <code>configs/main.yaml</code> The exact config used"},{"location":"experiments/reproducibility/#reproducing-our-results","title":"\ud83d\udd2c Reproducing Our Results","text":"<ol> <li>Clone the repository at the specified commit.</li> <li>Install dependencies from <code>requirements.txt</code>.</li> <li>Ensure <code>dataset/UNSW_NB15_training-set.csv</code> and <code>dataset/UNSW_NB15_testing-set.csv</code> are present.</li> <li>Run:     <code>bash     python main.py --config configs/main.yaml</code></li> <li>Compare your <code>results/metrics/*.json</code> against the published artifacts.</li> </ol>"},{"location":"experiments/running/","title":"Running Experiments","text":"<p>This guide provides a comprehensive reference for executing experiments locally or in the cloud.</p>"},{"location":"experiments/running/#overview","title":"Overview","text":"<p>The project executes a systematic grid of experiments:</p> Dimension Values Count Tasks Binary, Multiclass 2 Models LR, RF, XGBoost 3 Strategies S0, S1, S2a 3 Total 18 <p>Each experiment produces a JSON file with performance metrics and a confusion matrix figure.</p>"},{"location":"experiments/running/#execution-methods","title":"Execution Methods","text":""},{"location":"experiments/running/#method-1-mainpy-recommended","title":"Method 1: <code>main.py</code> (Recommended)","text":"<p>The primary entry point for running the full grid.</p> <pre><code>python main.py --config configs/main.yaml\n</code></pre> <p>CLI Arguments:</p> Argument Default Description <code>--config</code> <code>configs/main.yaml</code> Path to the YAML configuration file. <p>Key Configuration Options (<code>configs/main.yaml</code>):</p> <pre><code>experiments:\n  n_seeds: 1          # Number of seeds (1-5 recommended)\n  n_jobs: -1          # Parallelism (-1 = all cores, 1 = sequential)\n  tasks:\n    - binary\n    - multi\n  models:\n    - lr\n    - rf\n    - xgb\n  strategies:\n    - s0              # Baseline (no balancing)\n    - s1              # Class Weighting\n    - s2a             # Random Oversampling\n</code></pre>"},{"location":"experiments/running/#method-2-colab_full_gridpy-cloud","title":"Method 2: <code>colab_full_grid.py</code> (Cloud)","text":"<p>Optimized for Google Colab execution with Google Drive integration.</p> <p>See the dedicated Colab Guide for detailed instructions.</p>"},{"location":"experiments/running/#method-3-runnerpy-advanced","title":"Method 3: <code>runner.py</code> (Advanced)","text":"<p>A lower-level script for executing individual configurations. Useful for debugging or resuming specific experiments.</p> <pre><code>python runner.py --task binary --model xgb --strategy s1 --seed 42\n</code></pre>"},{"location":"experiments/running/#output-structure","title":"Output Structure","text":"<p>Every run generates artifacts in the <code>results/</code> directory:</p> <pre><code>results/\n\u251c\u2500\u2500 metrics/\n\u2502   \u251c\u2500\u2500 binary_lr_s0_s42.json\n\u2502   \u251c\u2500\u2500 binary_lr_s1_s42.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 figures/\n\u2502   \u251c\u2500\u2500 cm_binary_lr_s0_s42.png      # Confusion Matrix\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 tables/\n\u2502   \u251c\u2500\u2500 experiment_log.csv           # Master summary\n\u2502   \u251c\u2500\u2500 per_class_metrics.csv        # Rare class detail\n\u2502   \u2514\u2500\u2500 final_summary_tables.csv\n\u251c\u2500\u2500 logs/\n\u2502   \u2514\u2500\u2500 run_YYYYMMDD_HHMMSS.log\n\u2514\u2500\u2500 processed/\n    \u2514\u2500\u2500 preprocessing_metadata.json  # Feature names, scalers\n</code></pre>"},{"location":"experiments/running/#understanding-the-output-files","title":"Understanding the Output Files","text":""},{"location":"experiments/running/#metricsjson","title":"<code>metrics/*.json</code>","text":"<p>Each JSON contains the complete result of one experiment:</p> Field Description <code>experiment_id</code> Unique identifier (e.g., <code>binary_xgb_s1_s42</code>). <code>task</code> <code>binary</code> or <code>multi</code>. <code>model</code> <code>lr</code>, <code>rf</code>, or <code>xgb</code>. <code>strategy</code> <code>s0</code>, <code>s1</code>, or <code>s2a</code>. <code>seed</code> Random seed used. <code>metrics</code> Dict of Accuracy, Macro-F1, G-Mean, ROC-AUC, etc. <code>confusion_matrix</code> 2D array of confusion matrix values. <code>per_class_report</code> Dict of Precision/Recall/F1 per class."},{"location":"experiments/running/#figurespng","title":"<code>figures/*.png</code>","text":"<p>Visual representations of model performance: -   Confusion Matrices: Heatmaps showing TP, FP, TN, FN. -   Radar Charts: Strategy comparisons across multiple metrics.</p>"},{"location":"experiments/running/#monitoring-long-runs","title":"Monitoring Long Runs","text":"<p>For long runs (especially Multiclass with S2a), you can monitor progress:</p> <ol> <li>Check <code>results/metrics/</code>: JSON files appear as experiments complete.</li> <li>Tail the log file:     <code>bash     tail -f results/logs/run_*.log</code></li> <li>View <code>experiment_log.csv</code>: Appended after each experiment.</li> </ol>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome! This section guides you through setting up your environment and running your first experiment.</p>"},{"location":"getting-started/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> Requirement Version Notes Python 3.9+ <code>python --version</code> to check. pip Latest <code>pip install --upgrade pip</code> Git Any For cloning the repository. RAM 8GB+ 16GB recommended for full grid runs. <p>[!TIP] No local setup? Run everything in the cloud using our Google Colab Guide.</p>"},{"location":"getting-started/#setup-roadmap","title":"\ud83d\uddfa\ufe0f Setup Roadmap","text":"<p>Follow these steps in order:</p> 1\ufe0f\u20e3 Installation <p>Clone the repository and install dependencies.</p> \u2192 Installation Guide 2\ufe0f\u20e3 Quickstart <p>Run a single experiment to verify your setup.</p> \u2192 Quickstart Guide 3\ufe0f\u20e3 Full Grid <p>Execute the complete 18-experiment grid.</p> \u2192 Running Experiments"},{"location":"getting-started/#need-help","title":"\ud83e\udd14 Need Help?","text":"<p>If you encounter issues: 1.  Check the Troubleshooting section in the Installation Guide. 2.  Review the FAQ below. 3.  Open an issue on GitHub.</p>"},{"location":"getting-started/#faq","title":"\u2753 FAQ","text":"<p>Q: Can I use my own dataset? A: The pipeline is designed for UNSW-NB15 with specific column names. Adapting to other datasets requires modifying <code>src/data/loader.py</code> and the configuration files.</p> <p>Q: How long does the full grid take? A: Approximately 3-6 hours depending on your hardware. Binary tasks complete in ~45 minutes; Multiclass with S2a (oversampling to ~560k samples) takes the longest.</p> <p>Q: Do I need a GPU? A: Not required. All models (LR, RF, XGB) are CPU-based. A GPU can speed up XGBoost slightly if configured with <code>tree_method='gpu_hist'</code>.</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>You can run this project in two ways: Google Colab (Recommended) or Locally.</p>"},{"location":"getting-started/installation/#option-a-google-colab-zero-setup","title":"Option A: Google Colab (Zero Setup)","text":"<p>We have provided a unified script to handle everything for you in the cloud.</p> <ol> <li>Open the UNSW_NB15_Full_Grid.ipynb guide.</li> <li>Follow the instructions to launch the notebook.</li> <li>No local installation is required.</li> </ol>"},{"location":"getting-started/installation/#option-b-local-installation","title":"Option B: Local Installation","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/ihmorol/unsw-nb15-handling-binary-multiclass-ids.git\ncd unsw-nb15-handling-binary-multiclass-ids\n</code></pre>"},{"location":"getting-started/installation/#2-set-up-virtual-environment","title":"2. Set up Virtual Environment","text":"<p>It is recommended to use a virtual environment to manage dependencies.</p> <pre><code># Windows\npython -m venv .venv\n.venv\\Scripts\\activate\n\n# Linux/macOS\npython3 -m venv .venv\nsource .venv/bin/activate\n</code></pre>"},{"location":"getting-started/installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<p>Install the required packages from <code>requirements.txt</code>.</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#documentation-dependencies-optional","title":"Documentation Dependencies (Optional)","text":"<p>If you plan to build this documentation locally:</p> <pre><code>pip install mkdocs mkdocs-material mkdocstrings[python]\n</code></pre>"},{"location":"getting-started/installation/#4-verify-installation","title":"4. Verify Installation","text":"<p>Run the smoke test to ensure everything is set up correctly.</p> <pre><code>pytest src/tests/test_smoke.py\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quickstart Guide","text":"<p>This guide will walk you through running a single experiment to verify your setup is correct.</p>"},{"location":"getting-started/quickstart/#estimated-time-5-minutes","title":"\u23f1\ufe0f Estimated Time: 5 minutes","text":""},{"location":"getting-started/quickstart/#step-1-activate-your-environment","title":"Step 1: Activate Your Environment","text":"<p>If you haven't already, activate your Python virtual environment:</p> <p>=== \"Windows\"     <code>bash     .venv\\Scripts\\activate</code></p> <p>=== \"Linux/macOS\"     <code>bash     source .venv/bin/activate</code></p>"},{"location":"getting-started/quickstart/#step-2-run-a-single-experiment","title":"Step 2: Run a Single Experiment","text":"<p>Execute the main script. By default, it runs a full grid, but you can limit to a quick test with a single configuration:</p> <pre><code>python main.py --config configs/main.yaml\n</code></pre> <p>[!NOTE] On first run, the script will load the dataset from <code>dataset/</code> and apply preprocessing. This may take 1-2 minutes.</p>"},{"location":"getting-started/quickstart/#step-3-check-the-output","title":"Step 3: Check the Output","text":"<p>After execution, you'll see output similar to this:</p> <pre><code>================================================================================\n\ud83d\ude80 EXPERIMENT GRID EXECUTION\n================================================================================\nRunning Experiment 1/18: binary_lr_s0_s42\n   \u2705 Training complete (32.4s)\n   \ud83d\udcca Metrics: Accuracy=0.809, Macro-F1=0.795, G-Mean=0.791\n   \ud83d\udcbe Saved: results/metrics/binary_lr_s0_s42.json\n...\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-explore-results","title":"Step 4: Explore Results","text":"<p>Results are saved to the <code>results/</code> directory:</p> Path Content <code>results/metrics/*.json</code> Raw performance metrics per experiment. <code>results/figures/*.png</code> Confusion matrices and radar charts. <code>results/tables/*.csv</code> Summary tables (per-class metrics, etc.). <code>results/logs/*.log</code> Detailed execution logs. <p>Example JSON (<code>results/metrics/binary_lr_s0_s42.json</code>):</p> <pre><code>{\n  \"experiment_id\": \"binary_lr_s0_s42\",\n  \"task\": \"binary\",\n  \"model\": \"lr\",\n  \"strategy\": \"s0\",\n  \"seed\": 42,\n  \"metrics\": {\n    \"accuracy\": 0.809,\n    \"macro_f1\": 0.795,\n    \"weighted_f1\": 0.791,\n    \"g_mean\": 0.791,\n    \"roc_auc\": 0.956\n  }\n}\n</code></pre>"},{"location":"getting-started/quickstart/#success","title":"\ud83c\udf89 Success!","text":"<p>If you see the output above, your environment is correctly configured.</p> <p>Next Steps: -   Run the Full Grid: Execute all 18 experiments. -   Run on Colab: Use cloud resources for faster execution. -   Understand the Methodology: Learn about S0, S1, S2 strategies.</p>"},{"location":"guides/codebase_tour/","title":"Codebase Tour &amp; Script Guide","text":"<p>This document provides a comprehensive explanation of the scripts and modules in this repository, designed to help you navigate, modify, and extend the codebase.</p>"},{"location":"guides/codebase_tour/#root-directory-scripts","title":"\ud83c\udfd7\ufe0f Root Directory Scripts","text":"<p>These are the entry points for running experiments and pipelines.</p>"},{"location":"guides/codebase_tour/#mainpy","title":"<code>main.py</code>","text":"<p>The Master Orchestrator. -   Role: Coordinates the entire experimental pipeline. -   Key Functions:     -   <code>run_single_experiment()</code>: The atomic unit of work. Executes one (Task, Model, Strategy) combination.     -   <code>main()</code>: Reads the config, creates the experiment queue (the \"Grid\"), and executes them (optionally in parallel).     -   Aggregates results into <code>tables/aggregated_summary.csv</code>. -   Usage: <code>python main.py</code> (uses default <code>configs/main.yaml</code>)</p>"},{"location":"guides/codebase_tour/#runnerpy","title":"<code>runner.py</code>","text":"<p>The Scalpel (Single Run). -   Role: Runs exactly one specific experiment configuration. -   Key Feature: Appends <code>_single</code> to the experiment ID (e.g., <code>binary_lr_s0_single</code>) to avoid overwriting your main grid results. -   Usage: <code>python runner.py --task binary --model lr --strategy s0</code> -   Why use it: Perfect for smoke testing, debugging a specific model crash, or testing a new strategy without waiting for the full grid.</p>"},{"location":"guides/codebase_tour/#run_full_gridpy","title":"<code>run_full_grid.py</code>","text":"<p>The Safe Runner (Colab/Low-Resource). -   Role: A wrapper that ensures stability on constrained hardware. -   Key Feature: Forces <code>n_jobs=1</code> (sequential execution) regardless of the config file. This prevents RAM explosion or \"fork bombs\" on Google Colab. -   Usage: <code>python run_full_grid.py</code></p>"},{"location":"guides/codebase_tour/#source-modules-src","title":"\ud83d\udce6 Source Modules (<code>src/</code>)","text":"<p>The core logic resides here.</p>"},{"location":"guides/codebase_tour/#srcdata","title":"<code>src/data/</code>","text":"<ul> <li><code>loader.py</code>:<ul> <li>Handles reading the CSV files (<code>UNSW_NB15_training-set.csv</code>, etc.).</li> <li>Renames columns to standard snake_case.</li> <li>Fixes basic data types.</li> </ul> </li> <li><code>preprocessing.py</code>:<ul> <li>Crucial Class: <code>UNSWPreprocessor</code>.</li> <li>Strict Logic: Implements the \"Fit on Train, Transform on Test\" rule to guarantee Zero Data Leakage.</li> <li>Drops ID columns, handles imputing, one-hot encoding, and standard scaling.</li> </ul> </li> </ul>"},{"location":"guides/codebase_tour/#srcmodels","title":"<code>src/models/</code>","text":"<ul> <li><code>trainer.py</code>:<ul> <li>Class: <code>ModelTrainer</code>.</li> <li>Abstraction: Provides a single <code>.train()</code> and <code>.predict()</code> method that works for Logistic Regression, Random Forest, and XGBoost.</li> <li>Smart Handling: Automatically detects if a model needs <code>class_weight='balanced'</code> (sklearn) or <code>scale_pos_weight</code> (XGBoost).</li> </ul> </li> <li><code>config.py</code>:<ul> <li>Stores the \"Best Parameters\" found during research.</li> <li>Ensures every model instance uses the exact same <code>random_state</code>.</li> </ul> </li> </ul>"},{"location":"guides/codebase_tour/#srcstrategies","title":"<code>src/strategies/</code>","text":"<ul> <li><code>imbalance.py</code>:<ul> <li>Heart of the Study: Defines the S0, S1, S2a, S2b strategies.</li> <li>Factory: <code>get_strategy('s2a')</code> returns the correct class instance.</li> <li>S2a_RandomOverSampler: Implements the logic to safety upsample only the training data.</li> </ul> </li> </ul>"},{"location":"guides/codebase_tour/#srcevaluation","title":"<code>src/evaluation/</code>","text":"<ul> <li><code>metrics.py</code>: Calculates G-Mean, Macro-F1, and per-class metrics.</li> <li><code>plots.py</code>: Generates the Confusion Matrices, ROC curves, and Learning Curves found in <code>results/figures/</code>.</li> </ul>"},{"location":"guides/codebase_tour/#analysis-scripts-scripts","title":"\ud83d\udee0\ufe0f Analysis Scripts (<code>scripts/</code>)","text":"<p>Post-processing and deeper analysis tools.</p>"},{"location":"guides/codebase_tour/#reporting-visualization","title":"Reporting &amp; Visualization","text":"<ul> <li><code>generate_report.py</code>:<ul> <li>Scans <code>results/metrics/</code> for all JSON files.</li> <li>Compiles them into the master <code>final_summary_tables.csv</code>.</li> </ul> </li> <li><code>generate_publication_figures.py</code>:<ul> <li>Creates the complex \"Radar Charts\" and \"Bar Comparisons\" used in the final paper.</li> </ul> </li> <li><code>generate_statistics.py</code>:<ul> <li>Performs the Friedman Test and Nemenyi post-hoc analysis.</li> <li>Generates the Critical Difference (CD) diagrams.</li> </ul> </li> </ul>"},{"location":"guides/codebase_tour/#maintenance-audit","title":"Maintenance &amp; Audit","text":"<ul> <li><code>deep_audit.py</code>:<ul> <li>Quality Assurance: Checks if all 18 experiments exist, if seeds are consistent, and if data contracts were respected.</li> </ul> </li> <li><code>cleanup_stale_xgb.py</code>:<ul> <li>Utility to kill stuck XGBoost processes (useful on Windows).</li> </ul> </li> <li><code>generate_dashboard.py</code>:<ul> <li>Creates a simple HTML dashboard to view results results.</li> </ul> </li> </ul>"},{"location":"guides/codebase_tour/#documentation-docs","title":"\ud83d\udcc4 Documentation (<code>docs/</code>)","text":"<ul> <li><code>research/methodology.md</code>: The scientific theory behind the code.</li> <li><code>contracts/*.md</code>: The \"Laws\" that the code must follow (e.g., \"Never split validation data dynamically\").</li> </ul>"},{"location":"guides/colab_run_guide/","title":"\ud83d\ude80 Comprehensive Guide: Running Experiments on Google Colab","text":"<p>This guide explains how to execute the full 90-experiment grid on Google Colab, ensuring all results are safely synced to your Google Drive.</p>"},{"location":"guides/colab_run_guide/#time-estimation","title":"\u23f1\ufe0f Time Estimation","text":"<p>Estimated Total Runtime: ~4 to 6 Hours</p> Phase Description Est. Time Setup Cloning repo, installing deps 2-3 mins Binary Task 45 runs (LR, RF, XGB) ~45 mins Multiclass Task 45 runs (LR, RF, XGB) ~3 - 4 hours Total ~4 - 5 Hours <p>Why so long? The Multiclass S2a (RandomOverSampler) experiments explode the training set size from 175k to ~560k samples to balance all 10 classes. Training XGBoost on this expanded dataset 5 times takes the majority of the time.</p>"},{"location":"guides/colab_run_guide/#step-by-step-execution-guide","title":"\ud83d\udee0\ufe0f Step-by-Step Execution Guide","text":""},{"location":"guides/colab_run_guide/#1-preparation-local","title":"1. Preparation (Local)","text":"<p>Before going to Colab, ensure your latest code is on GitHub.</p> <ol> <li>Commit and Push your changes:     <code>bash     git add .     git commit -m \"feat: Ready for full Colab run\"     git push origin main</code></li> </ol>"},{"location":"guides/colab_run_guide/#2-open-google-colab","title":"2. Open Google Colab","text":"<ol> <li>Navigate to colab.research.google.com.</li> <li>Click File &gt; Open Notebook.</li> <li>Select the GitHub tab.</li> <li>Enter your repository URL: <code>https://github.com/StartDust/ML_PAPER_REVIEW</code>.</li> <li>Select the <code>UNSW_NB15_Full_Grid.ipynb</code> file from the list.</li> </ol>"},{"location":"guides/colab_run_guide/#3-configure-runtime-critical","title":"3. Configure Runtime (Critical)","text":"<p>To speed up XGBoost and ensure stability: 1.  Click Runtime in the top menu. 2.  Select Change runtime type. 3.  Hardware accelerator: Select T4 GPU. 4.  Runtime shape: Standard is fine, \"High-RAM\" is better if available (Colab Pro). 5.  Click Save.</p>"},{"location":"guides/colab_run_guide/#4-run-the-experiments","title":"4. Run the Experiments","text":"<ol> <li>Locate the cell titled \ud83d\ude80 Run Full Experiment Grid (0-89).</li> <li>Verify the configuration form fields:<ul> <li><code>REPO_URL</code>: Your GitHub URL.</li> <li><code>BRANCH</code>: <code>main</code> (usually).</li> <li><code>FORCE_FRESH_RUN</code>: <code>True</code> (checks <code>check</code> to wipe old partial runs).</li> <li><code>SYNC_INTERVAL_SECONDS</code>: <code>60</code>.</li> </ul> </li> <li>Start execution:<ul> <li>Click Runtime &gt; Run all (or press <code>Ctrl + F9</code>).</li> <li>Or click the \"Play\" button on the cell.</li> </ul> </li> </ol>"},{"location":"guides/colab_run_guide/#5-authentication","title":"5. Authentication","text":"<ol> <li>The script will prompt: <code>Mounting Google Drive...</code>.</li> <li>A pop-up window will ask for permission to access your Google Drive.</li> <li>Click Connect to Google Drive and authorize it.<ul> <li>Note: This is required to save your results to <code>UNSW_Archive</code> so they aren't lost if Colab disconnects.</li> </ul> </li> </ol>"},{"location":"guides/colab_run_guide/#6-monitor-progress","title":"6. Monitor Progress","text":"<ul> <li>The cell output will stream logs in real-time.</li> <li>Do not close the tab: You must keep the browser tab open (active or background) for the run to continue.</li> <li>Prevent Disconnects:<ul> <li>Colab might timeout if idle for 90 mins.</li> <li>Since the cell is printing output (streaming), it usually stays active.</li> <li>Check on it every hour if possible.</li> </ul> </li> </ul>"},{"location":"guides/colab_run_guide/#7-view-results-real-time","title":"7. View Results (Real-Time)","text":"<p>You don't need to wait for it to finish! 1.  Open a new tab to Google Drive. 2.  Navigate to <code>My Drive &gt; UNSW_Archive</code>. 3.  Find the folder <code>run_YYYYMMDD_HHMMSS</code> (matching the timestamp in the Colab output). 4.  You can watch JSONs and PNGs appear in <code>results/metrics</code> and <code>results/figures</code> as they complete.</p>"},{"location":"guides/colab_run_guide/#troubleshooting","title":"\ud83c\udd98 Troubleshooting","text":""},{"location":"guides/colab_run_guide/#runtime-disconnected","title":"\"Runtime Disconnected\"","text":"<p>If Colab disconnects halfway: 1.  Don't panic. Your results up to the last 60 seconds are safe in Drive. 2.  Reconnect the runtime. 3.  Uncheck <code>FORCE_FRESH_RUN</code> in the notebook form. 4.  Re-run the cell.     - The script identifies existing JSON files in your Drive folder (if you mounted it to the same place... actually, the script creates a new timestamp folder by default).     - To Resume specific folder: You would need to manually set <code>RESULTS_DIR</code> in the script to the old timestamp folder.     - Easier Path: Just accept the split results (partial run in Folder A, rest in Folder B) and merge them later using <code>scripts/consolidate_all.py</code>.</p>"},{"location":"guides/colab_run_guide/#out-of-memory-oom","title":"\"Out of Memory\" (OOM)","text":"<p>If the session crashes during Multiclass S2a: - This is rare on T4 GPU but possible with standard RAM. - Fix: The script already uses <code>n_jobs=1</code> to minimize memory spikes. If it crashes, unfortunately, you might skip S2a or try Colab Pro.</p>"},{"location":"guides/colab_run_guide/#drive-not-syncing","title":"\"Drive not syncing\"","text":"<ul> <li>Check the output logs for <code>rsync</code> errors.</li> <li>Ensure you have free space in Google Drive (Project is ~500MB - 1GB total).</li> </ul>"},{"location":"plans/strategic_engineering_plan/","title":"Strategic Engineering Plan: The \"Clean Slate\" Doctrine (v2.0)","text":"<p>Date: 2026-01-23 Version: 2.0.0 (Multi-Persona Reviewed) Status: PROPOSED \u2192 REVIEWED</p>"},{"location":"plans/strategic_engineering_plan/#executive-summary-orchestrator-assessment","title":"Executive Summary (Orchestrator Assessment)","text":"<p>Overall Rating: \u2b50\u2b50\u2b50\u2b50 (4/5 - Solid, needs minor polish)</p> Persona Verdict Key Point Orchestrator \u2705 Approve Strategy is sound, addresses user's \"juggling\" pain point Auditor \u26a0\ufe0f Conditional Missing formal acceptance tests Architect \u2705 Pass Existing <code>main.py</code> is robust enough; new script optional Reviewer \u2705 Approve N=5 seeds is statistically valid for CIs and t-tests Author \u26a0\ufe0f Advice Add \"Rare Class Narrative\" as explicit goal"},{"location":"plans/strategic_engineering_plan/#1-the-strategy-clean-slate-full-grid","title":"1. The Strategy: \"Clean Slate &amp; Full Grid\"","text":"<p>Objective: Eliminate confusion, data skew, and environmental variance by executing a single, unified experiment grid from start to finish.</p> <p>Recommendation on Environment: - Local (Preferred): Run overnight. Simplest, avoids Colab artifacts. - Colab (Fallback): Only if RAM &lt; 16GB. Use strict \"Zip &amp; Download\" workflow.</p>"},{"location":"plans/strategic_engineering_plan/#2-the-golden-grid-specification","title":"2. The \"Golden Grid\" Specification","text":"Dimension Values Count Tasks Binary, Multiclass 2 Models LR, RF, XGB 3 Strategies S0, S1, S2a 3 Seeds 42, 43, 44, 45, 46 5 Total 90"},{"location":"plans/strategic_engineering_plan/#configuration-lock-configsmainyaml","title":"Configuration Lock (<code>configs/main.yaml</code>)","text":"<pre><code>experiments:\n  n_seeds: 5\n  n_jobs: -1\n  tasks: [binary, multi]\n  models: [lr, rf, xgb]\n  strategies: [s0, s1, s2a]\n</code></pre>"},{"location":"plans/strategic_engineering_plan/#3-execution-protocol","title":"3. Execution Protocol","text":""},{"location":"plans/strategic_engineering_plan/#step-1-the-purge-archive","title":"Step 1: The Purge (Archive)","text":"<pre><code># Safety: Never delete, only archive\nmv results results_archive_$(date +%Y%m%d_%H%M%S)\nmkdir -p results/{metrics,figures,tables,logs,processed}\n</code></pre>"},{"location":"plans/strategic_engineering_plan/#step-2-the-run","title":"Step 2: The Run","text":"<p>Option A (Simple): Use existing <code>main.py</code> with config (Recommended).</p> <pre><code>python main.py --config configs/main.yaml\n</code></pre> <p>Option B (Advanced): Create <code>scripts/run_golden_grid.py</code> with resume logic.</p>"},{"location":"plans/strategic_engineering_plan/#step-3-verification-auditor-requirement","title":"Step 3: Verification (Auditor Requirement \u26a0\ufe0f)","text":"<p>NEW: Formal acceptance test script.</p> <pre><code>python scripts/verify_golden_grid.py\n</code></pre> <p>Tests: - <code>T001</code>: 90 JSON files exist in <code>results/metrics/</code>. - <code>T002</code>: Each JSON has keys: <code>experiment_id</code>, <code>task</code>, <code>model</code>, <code>strategy</code>, <code>seed</code>, <code>metrics</code>. - <code>T003</code>: Preprocessing metadata checksum matches for all runs (same split).</p>"},{"location":"plans/strategic_engineering_plan/#4-the-sota-analysis-layer","title":"4. The SOTA Analysis Layer","text":""},{"location":"plans/strategic_engineering_plan/#41-statistical-validity-critical","title":"4.1 Statistical Validity (Critical)","text":"<ul> <li>Script: <code>scripts/generate_statistics.py</code></li> <li>Outputs: <code>metric_confidence_intervals.csv</code>, <code>paired_significance_tests.csv</code></li> <li>Success Criteria: Multiclass S2a &gt; S0 with $p &lt; 0.05$.</li> </ul>"},{"location":"plans/strategic_engineering_plan/#42-global-ranking-wow-factor","title":"4.2 Global Ranking (Wow Factor)","text":"<ul> <li>Script: <code>scripts/generate_cd_diagram.py</code></li> <li>Outputs: <code>results/figures/cd_diagram_macro_f1.png</code></li> <li>Method: Friedman test + Nemenyi post-hoc.</li> </ul>"},{"location":"plans/strategic_engineering_plan/#43-explainability-differentiator","title":"4.3 Explainability (Differentiator)","text":"<ul> <li>Script: <code>scripts/generate_shap.py</code></li> <li>Output: <code>results/figures/shap_worms_summary.png</code></li> <li>Why: Explains why S2a improves Worms detection.</li> </ul>"},{"location":"plans/strategic_engineering_plan/#44-rare-class-narrative-author-requirement","title":"4.4 Rare Class Narrative (Author Requirement \u26a0\ufe0f)","text":"<ul> <li>NEW: Explicitly frame the paper around \"Rare Class Detection\".</li> <li>Hook: \"Worms recall improves from 0% to &gt;80% with simple resampling.\"</li> </ul>"},{"location":"plans/strategic_engineering_plan/#5-acceptance-tests-checklist-auditor-addition","title":"5. Acceptance Tests Checklist (Auditor Addition)","text":"Test ID Assertion Script T001 90 metric JSONs exist <code>verify_golden_grid.py</code> T002 All JSONs structurally valid <code>verify_golden_grid.py</code> T003 Preprocessing consistent Checksum on <code>preprocessing_metadata.json</code> T004 CIs computed for all 18 configs <code>generate_statistics.py</code> T005 CD diagram generated File exists check T006 SHAP plot for Worms exists File exists check"},{"location":"plans/strategic_engineering_plan/#6-roadmap-timeline","title":"6. Roadmap &amp; Timeline","text":"Phase Task Time Owner 1. Prep Archive, verify config 15 min User 2. Run Execute 90 experiments 3-6 hrs main.py 3. Verify Acceptance tests 5 min verify_golden_grid.py 4. Analyze Stats, CD, SHAP 30 min Scripts 5. Write Update Results section 1 hr Author"},{"location":"plans/strategic_engineering_plan/#7-risk-mitigation-new-section","title":"7. Risk Mitigation (New Section)","text":"Risk Mitigation Run fails mid-way <code>main.py</code> has resume logic (skips existing JSONs) OOM on SMOTE S2a (ROS) is used, not SMOTE; lower memory Colab timeout Run locally; or batch by task in Colab Statistical insignificance N=5 is sufficient for t-tests; if fails, result is still valid (honest reporting)"},{"location":"plans/strategic_engineering_plan/#recommendation","title":"Recommendation","text":"<p>Proceed with v2.0. The plan is now auditor-approved with acceptance tests to catch errors early.</p> <p>Action Required: Confirm \"Purge &amp; Run\" to begin execution.</p>"},{"location":"research/findings/","title":"Research Findings","text":""},{"location":"research/findings/#1-executive-summary-core-insights","title":"1. Executive Summary &amp; Core Insights","text":"<p>The rigorous evaluation of the UNSW-NB15 dataset reveals a fundamental dichotomy in strategy performance. We evaluated every model on the Standard 5-Metric Suite: Accuracy (Acc), Macro F1 (Mac-F1), Weighted F1 (W-F1), Geometric Mean (G-Mean), and ROC AUC.</p> <ol> <li>Binary Classification: Effectively solved. XGBoost (S1) dominates across all 5 metrics (Mac-F1 0.90, AUC 0.986).</li> <li>Multiclass Classification:<ul> <li>Accuracy Paradox: High Accuracy (e.g., XGB-S0: 0.768) often masks poor minority class performance (Mac-F1: 0.507).</li> <li>G-Mean Sensitivity: S1/S2a strategies significantly boost G-Mean (XGB-S1: 0.795 vs XGB-S0: 0.725), proving they successfully balance performance across classes despite lower raw Accuracy.</li> <li>The \"S2a Effect\": Random Over Sampling (S2a) transforms XGBoost from a \"safe\" classifier into a \"high-recall\" hunter, maximizing G-Mean and Macro F1 at the expense of Accuracy and Weighted F1.</li> </ul> </li> </ol>"},{"location":"research/findings/#visual-summary-strategy-trade-offs","title":"Visual Summary (Strategy Trade-offs)","text":"<p>The radar charts illustrate the trade-offs between strategies (S0 vs S1 vs S2a) for XGBoost:</p> <p> </p>"},{"location":"research/findings/#2-binary-classification-analysis-normal-vs-attack","title":"2. Binary Classification Analysis (Normal vs Attack)","text":""},{"location":"research/findings/#21-logistic-regression-lr","title":"2.1 Logistic Regression (LR)","text":"Strategy Accuracy Macro F1 Weighted F1 G-Mean ROC AUC S0 (Base) 0.809 0.795 0.791 0.791 0.956 S1 (Weight) 0.837 0.831 0.834 0.827 0.956 S2a (ROS) 0.838 0.832 0.835 0.827 0.956 <ul> <li>Analysis: S1 and S2a provide a clear, unified improvement across all 5 metrics. Regularization via S1 is preferred as it achieves the same gains as S2a without the computational cost of training on 1.36x more data. ROC AUC hits a hard ceiling at 0.956, limited by the model's linearity.</li> </ul>"},{"location":"research/findings/#22-random-forest-rf","title":"2.2 Random Forest (RF)","text":"Strategy Accuracy Macro F1 Weighted F1 G-Mean ROC AUC S0 (Base) 0.870 0.864 0.867 0.857 0.981 S1 (Weight) 0.870 0.864 0.867 0.857 0.980 S2a (ROS) 0.885 0.881 0.883 0.874 0.981 <ul> <li>Analysis: Unlike LR, RF benefits meaningfully from S2a across all metrics (+1.5% Accuracy, +1.7% Mac-F1). S1 (Class Weighting) is ineffective here, yielding results identical to S0. This suggests RF needs the explicit oversampling of minority samples to refine its splits in the tree leaves.</li> </ul>"},{"location":"research/findings/#23-xgboost-xgb-the-state-of-the-art","title":"2.3 XGBoost (XGB) - The State-of-the-Art","text":"Strategy Accuracy Macro F1 Weighted F1 G-Mean ROC AUC S0 (Base) 0.874 0.868 0.871 0.861 0.985 S1 (Weight) 0.905 0.902 0.904 0.897 0.986 S2a (ROS) 0.901 0.897 0.899 0.892 0.986 <ul> <li>Verdict: XGBoost with S1 (Class Weighting) is the global winner, achieving the highest scores in every single metric. It beats S2a slightly, proving that for Gradient Boosting, adjusting the loss function weight is cleaner than oversampling for this binary task.</li> </ul>"},{"location":"research/findings/#3-multiclass-classification-deep-dive","title":"3. Multiclass Classification Deep Dive","text":""},{"location":"research/findings/#31-logistic-regression-lr","title":"3.1 Logistic Regression (LR)","text":"Strategy Accuracy Macro F1 Weighted F1 G-Mean ROC AUC S0 (Base) 0.698 0.338 0.707 0.609 0.949 S1 (Weight) 0.621 0.354 0.685 0.728 0.942 S2a (ROS) 0.623 0.358 0.687 0.732 0.942 <ul> <li>Trade-off Analysis: LR presents a classic \"Accuracy vs Fairness\" trade-off. S0 maximizes Accuracy (0.698) by effectively ignoring rare classes, leading to a dismal G-Mean (0.609) and Macro F1 (0.338).</li> <li>Correction: S1 and S2a sacrifice ~7% Accuracy to boost G-Mean by ~12%, indicating much better detection of minority classes, even though the overall Macro F1 remains poor (&lt; 0.36).</li> </ul>"},{"location":"research/findings/#32-random-forest-rf","title":"3.2 Random Forest (RF)","text":"Strategy Accuracy Macro F1 Weighted F1 G-Mean ROC AUC S0 (Base) 0.751 0.482 0.784 0.702 0.944 S1 (Weight) 0.749 0.473 0.783 0.697 0.943 S2a (ROS) 0.728 0.493 0.768 0.729 0.911 <ul> <li>Metric Conflict: S2a provides the best Macro F1 (0.493) and G-Mean (0.729), indicating it is the most \"balanced\" classifier. However, it suffers a significant drop in ROC AUC (0.911 vs 0.944) and Accuracy.</li> <li>Interpretation: S2a forces RF to make \"noisier\" predictions to catch rare classes, increasing False Positives which hurts AUC/Accuracy but helps Recall (G-Mean).</li> </ul>"},{"location":"research/findings/#33-xgboost-xgb-the-shellcode-solver","title":"3.3 XGBoost (XGB) - The \"Shellcode\" Solver","text":"Strategy Accuracy Macro F1 Weighted F1 G-Mean ROC AUC S0 (Base) 0.768 0.507 0.781 0.725 0.963 S1 (Weight) 0.686 0.513 0.740 0.795 0.959 S2a (ROS) 0.699 0.516 0.750 0.787 0.958 <ul> <li>Critical Finding:<ul> <li>Accuracy: S0 wins (0.768) because it biases towards the majority \"Normal\" class.</li> <li>G-Mean: S1 exploits the Gradient Boosting mechanics to achieve a massive G-Mean (0.795), far superior to S0 (0.725). This metric proves S1 is properly identifying the geometric center of all classes, not just the majority.</li> <li>Macro F1: S2a edges out S1 slightly (0.516 vs 0.513).</li> </ul> </li> <li>Recommendation: If the goal is Detection Capability (catching all attack types), XGB-S1 or XGB-S2a are the only valid choices despite the lower Accuracy. If the goal is minimizing False Alarms (Precision), S0 is safer.</li> </ul>"},{"location":"research/findings/#rare-class-analysis-worms-shellcode-backdoor","title":"Rare Class Analysis (Worms, Shellcode, Backdoor)","text":"<p>The impact of strategies on the detection of rare classes is substantial. As shown below, S2a and S1 dramatically improve Recall for the most difficult classes compared to the baseline.</p> <p></p>"},{"location":"research/findings/#4-discussion-strategic-recommendations","title":"4. Discussion &amp; Strategic Recommendations","text":""},{"location":"research/findings/#41-the-cost-of-recall","title":"4.1 The Cost of Recall","text":"<p>Our experiments verify the \"no free lunch\" theorem in imbalanced learning. Strategies that improve rare class detection (S1, S2a) invariably degrade Overall Accuracy and Precision. *   Trade-off: To increase \"Worms\" recall from 2% to 75%, we accept a drop in overall Accuracy from 76.8% to 69.9%. *   Operational Impact: In a security operations center (SOC), this means S2a will catch the critical worm but generate significantly more false alarms (lower Precision).</p>"},{"location":"research/findings/#42-model-selection-advice","title":"4.2 Model Selection Advice","text":"<ol> <li>Metric Hierarchy: Stop reporting independent Accuracy. Always couple Accuracy with G-Mean to reveal the \"Majority vs Minority\" trade-off. S1/S2a consistently trade ~5-8% Accuracy for ~10-15% G-Mean improvements.</li> <li>Top Model: XGBoost is superior.<ul> <li>Binary: XGB-S1 (Best across all 5 metrics).</li> <li>Multiclass: XGB-S2a (Best Macro F1/G-Mean balance).</li> </ul> </li> <li>Feature Blindness: The persistent failure of all models (G-Mean/F1 caps) on \"Analysis\" and \"Backdoor\" classes is a data limitation, not a model limitation.</li> </ol>"},{"location":"research/methodology/","title":"Research Methodology","text":"<p>This document details the experimental methodology used in this study, including mathematical definitions for metrics and strategies.</p>"},{"location":"research/methodology/#1-problem-statement","title":"1. Problem Statement","text":"<p>Intrusion Detection Systems (IDS) face severe class imbalance: normal traffic vastly outweighs attack traffic in real-world network datasets. The UNSW-NB15 dataset exemplifies this:</p> Class Label Count (Train) Percentage Normal 0 56,000 31.9% Generic 1 40,000 22.8% Exploits 2 33,000 18.8% ... ... ... ... Worms 9 130 0.07% Shellcode 8 1,133 0.65% <p>Consequence: Standard classifiers (optimizing Accuracy) learn to ignore rare classes, achieving high Accuracy but near-zero Recall on critical attacks.</p>"},{"location":"research/methodology/#2-strategies-evaluated","title":"2. Strategies Evaluated","text":"<p>We evaluate three distinct strategies for handling class imbalance.</p>"},{"location":"research/methodology/#21-s0-baseline-no-handling","title":"2.1 S0: Baseline (No Handling)","text":"<ul> <li>Description: Train on the original, imbalanced class distribution.</li> <li>Goal: Establish a baseline for comparison.</li> <li>Expected Behavior: High Accuracy, but poor G-Mean and Macro-F1 due to majority class bias.</li> </ul>"},{"location":"research/methodology/#22-s1-cost-sensitive-learning-class-weighting","title":"2.2 S1: Cost-Sensitive Learning (Class Weighting)","text":"<ul> <li>Description: Assign higher misclassification costs to minority classes during training.</li> <li>Mechanism: The learning algorithm's loss function is modified to penalize errors on minority classes more heavily.</li> </ul> <p>Mathematical Definition (Sklearn <code>balanced</code>):</p> <p>For each class $i$, the weight $w_i$ is calculated as:</p> <p>$$w_i = \\frac{N}{k \\cdot n_i}$$</p> <p>Where: -   $N$ = Total number of samples in the training set. -   $k$ = Number of unique classes. -   $n_i$ = Number of samples in class $i$.</p> Implementation Parameter Logistic Regression <code>class_weight='balanced'</code> Random Forest <code>class_weight='balanced'</code> XGBoost <code>scale_pos_weight</code> (for Binary) / <code>sample_weight</code> vector (for Multi)"},{"location":"research/methodology/#23-s2-resampling","title":"2.3 S2: Resampling","text":"<p>Resampling modifies the training data distribution before training.</p> <p>[!CAUTION] Resampling is applied only to the training set. Applying it to validation or test sets constitutes data leakage and invalidates results.</p>"},{"location":"research/methodology/#s2a-random-oversampling-ros","title":"S2a: Random Oversampling (ROS)","text":"<ul> <li>Description: Duplicates minority class samples uniformly at random until all classes have the same count as the majority class.</li> <li>Pros: Simple, fast, preserves original feature space.</li> <li>Cons: Can lead to overfitting on duplicated samples.</li> </ul>"},{"location":"research/methodology/#s2b-smote-synthetic-minority-over-sampling-technique","title":"S2b: SMOTE (Synthetic Minority Over-sampling Technique)","text":"<ul> <li>Description: Generates synthetic samples by interpolating between existing minority class samples and their k-nearest neighbors.</li> <li>Pros: Creates new, diverse samples.</li> <li>Cons: Can create noisy samples if classes overlap in feature space.</li> </ul> <p>Note: S2b (SMOTE) is considered optional in this study; S2a is the primary resampling strategy.</p>"},{"location":"research/methodology/#3-models","title":"3. Models","text":"<p>We evaluate three classical machine learning models with optimized hyperparameters (see Experiment Contract for full details).</p> Model Description Key Hyperparameters Logistic Regression (LR) Linear model; serves as a baseline. <code>solver='lbfgs'</code>, <code>max_iter=1000</code>, <code>C=1.0</code> Random Forest (RF) Ensemble of decision trees (bagging). <code>n_estimators=300</code>, <code>max_depth=None</code>, <code>class_weight='balanced_subsample'</code> XGBoost (XGB) Gradient boosting method. <code>n_estimators=150</code>, <code>max_depth=15</code>, <code>learning_rate=0.05</code>, <code>gamma=1.0</code>"},{"location":"research/methodology/#4-metrics","title":"4. Metrics","text":"<p>We prioritize macro-averaged and class-balanced metrics to ensure fair evaluation across all classes, regardless of size.</p>"},{"location":"research/methodology/#41-macro-f1-score","title":"4.1 Macro-F1 Score","text":"<p>The harmonic mean of Precision and Recall, averaged equally across all $k$ classes.</p> <p>$$\\text{Macro-F1} = \\frac{1}{k} \\sum_{i=1}^{k} F1_i$$</p> <p>Where $F1_i = \\frac{2 \\cdot P_i \\cdot R_i}{P_i + R_i}$ is the F1-Score for class $i$.</p>"},{"location":"research/methodology/#42-g-mean-geometric-mean","title":"4.2 G-Mean (Geometric Mean)","text":"<p>Measures the balance between sensitivity (Recall) across all classes. It is the $k$-th root of the product of per-class Recalls.</p> <p>$$G\\text{-}Mean = \\left( \\prod_{i=1}^{k} R_i \\right)^{1/k}$$</p> <p>Interpretation: A high G-Mean indicates that the model performs well across all classes. If any class has zero Recall (e.g., Worms), G-Mean collapses to zero.</p>"},{"location":"research/methodology/#43-per-class-recall","title":"4.3 Per-Class Recall","text":"<p>We explicitly report Recall (Sensitivity/True Positive Rate) for critical rare categories: -   Worms (0.07%) -   Shellcode (0.65%) -   Backdoor (1.2%) -   Analysis (1.5%)</p>"},{"location":"research/methodology/#5-experimental-protocol","title":"5. Experimental Protocol","text":"<ol> <li>Data Split: Use official UNSW-NB15 training and testing sets. No re-splitting.</li> <li>Preprocessing:<ul> <li>Drop identifiers (<code>id</code>, <code>srcip</code>, <code>dstip</code>, <code>sport</code>, <code>dsport</code>).</li> <li>Impute missing numerics with median (fit on train).</li> <li>One-Hot Encode categoricals (fit on train, <code>handle_unknown='ignore'</code>).</li> <li>Standard scale numerics (fit on train).</li> </ul> </li> <li>Strategy Application: Apply S1 or S2 only to the training set.</li> <li>Training: Train model with fixed hyperparameters.</li> <li>Evaluation: Compute metrics on the untouched official test set.</li> </ol>"},{"location":"research/methodology/#6-codebase-structure-scripts","title":"6. Codebase Structure &amp; Scripts","text":"<p>The implementation is modularized to ensure reproducibility and extensibility. Below is a guide to the key scripts and their roles in the methodology.</p>"},{"location":"research/methodology/#61-core-execution-scripts-root","title":"6.1 Core Execution Scripts (Root)","text":"Script Purpose When to Use <code>main.py</code> Orchestrator: Runs the full 18-experiment grid (or subsets if configured). Manages parallel execution, logging, and result saving. To run the full study reproduction. <code>runner.py</code> Single Experiment Runner: Runs an isolated experiment (e.g., <code>binary lr s0</code>) without affecting the main grid results. Uses <code>_single</code> suffix. For debugging, smoke testing, or quick validation. <code>run_full_grid.py</code> Optimized Grid Runner: A wrapper around <code>main.py</code> that forces sequential execution (n_jobs=1) to prevent resource exhaustion on constrained environments (e.g., Colab). When running on Colab or limited hardware."},{"location":"research/methodology/#62-source-modules-src","title":"6.2 Source Modules (<code>src/</code>)","text":""},{"location":"research/methodology/#data-handling-srcdata","title":"Data Handling (<code>src/data/</code>)","text":"<ul> <li><code>loader.py</code>: Manages loading of CSVs, column naming, and basic cleaning.</li> <li><code>preprocessing.py</code>: Implements the strict <code>UNSWPreprocessor</code>. Handles splitting, imputation, one-hot encoding, and scaling. Enforces the \"fit on train only\" rule.</li> </ul>"},{"location":"research/methodology/#modeling-srcmodels","title":"Modeling (<code>src/models/</code>)","text":"<ul> <li><code>trainer.py</code>: Contains the <code>ModelTrainer</code> class. Unified interface for LR, RF, and XGB. Handles class weighting application and training loops.</li> <li><code>config.py</code>: Central repository for model hyperparameters (fixed random states, solver parameters, tree depths).</li> </ul>"},{"location":"research/methodology/#strategies-srcstrategies","title":"Strategies (<code>src/strategies/</code>)","text":"<ul> <li><code>imbalance.py</code>: Implements the <code>ImbalanceStrategy</code> abstract base class and concrete strategies (<code>S0</code>, <code>S1</code>, <code>S2a</code>, <code>S2b</code>). Contains the logic for resampling and weight calculation.</li> </ul>"},{"location":"research/methodology/#evaluation-srcevaluation","title":"Evaluation (<code>src/evaluation/</code>)","text":"<ul> <li><code>metrics.py</code>: Calculates all performance metrics (Macro-F1, G-Mean, Recalls).</li> <li><code>plots.py</code>: Generates visual artifacts (Confusion Matrices, ROC/PR Curves, Learning Curves).</li> </ul>"},{"location":"research/methodology/#63-analysis-scripts-scripts","title":"6.3 Analysis Scripts (<code>scripts/</code>)","text":"Script Purpose <code>generate_statistics.py</code> Conducts statistical significance tests (Friedman, Nemenyi) and generates Critical Difference (CD) diagrams. <code>generate_publication_figures.py</code> Produces high-resolution, publication-ready composite figures (e.g., Radar Charts, Bar Plots). <code>generate_report.py</code> Compiles JSON metrics into readable CSV summary tables (<code>aggregated_summary.csv</code>, <code>rare_class_report.csv</code>). <code>deep_audit.py</code> Scans the codebase and results to verify compliance with contracts (Leakage checks, Seed verification)."}]}