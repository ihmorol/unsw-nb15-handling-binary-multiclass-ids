\chapter{Methodology}

This chapter outlines the steps taken to build and evaluate the intrusion detection system. The process begins with selecting the dataset, followed by cleaning and preparing the data, training the models using different strategies to handle class imbalance, and finally evaluating the results.

\section{Dataset Description}
We selected the \textbf{UNSW-NB15} dataset for this research \cite{moustafa2015unsw}. This dataset was created by Moustafa and Slay to address the limitations of older datasets like KDD99. KDD99 is often considered outdated because it contains duplicate records and does not include modern attack patterns \cite{moustafa2016evaluation}. In contrast, UNSW-NB15 includes nine types of contemporary attacks, making it a more realistic benchmark for current network security. Surveys of intrusion detection datasets confirm that UNSW-NB15 is a reliable choice for research, though it is noted to be highly imbalanced, which makes classification difficult \cite{ring2019survey}.

\section{Data Preprocessing}
Raw network data cannot be used directly by machine learning algorithms. We applied a consistent preprocessing pipeline to clean and format the data.

\subsection{Feature Cleaning and Selection}
The version of the UNSW-NB15 dataset used in this study (the train/test partition) already excludes high-cardinality identifiers such as \texttt{srcip}, \texttt{dstip}, \texttt{sport}, \texttt{dsport}, \texttt{stime}, and \texttt{ltime}. These features are specific to the network setup and do not help the model learn general attack patterns. We confirmed their absence in the input files to ensure no leakage of specific IP addresses or timestamps occurs. We utilized feature selection concepts discussed by Janarthanan and Zargari to ensure only relevant information remained \cite{8001537}.

\subsection{Missing Values}
Real-world data often has gaps. To handle this, we checked for missing values in the dataset. For numerical features, we filled missing entries with the median value of that column. For categorical features, we replaced missing entries with a placeholder category labeled ``missing''.

\subsection{Labeling (Binary and Multiclass)}
We organized the experiment into two distinct tracking tasks:
\begin{itemize}
    \item \textbf{Task A (Binary Classification):} Two labels were used — ‘0’ for Normal traffic and ‘1’ for Attack traffic.
    \item \textbf{Task B (Multiclass Classification):} We retained the detailed labels for all nine attack categories along with Normal traffic.
\end{itemize}

\subsection{Data Scaling}
Since different features have different ranges, we scaled the numeric features. We used standard scaling to ensure the values fall within a similar range. This step is critical for models like Logistic Regression but less critical for tree-based models.

\subsection{Splitting Into Train, Validation, and Test}
We adhered to the standard training and testing splits provided by the UNSW-NB15 authors. Within the training set, we created a validation split (80\% training, 20\% validation) to tune model settings before final testing.

\section{System Flow}
The system follows a structured pipeline from raw data to final evaluation. The experimental setup compares different strategies to handle the dataset imbalance.

\textbf{Experimental Strategies:}
\begin{enumerate}
    \item \textbf{S0 (No Balancing):} Baseline model training without modifying class distribution.
    \item \textbf{S1 (Class Weighting):} Applying higher misclassification penalties for minority classes (cost-sensitive learning) \cite{5596486}.
    \item \textbf{S2 (Oversampling):} Increasing the number of minority attack samples through resampling techniques \cite{bagui2021resampling}.
\end{enumerate}

\begin{figure}[h]
\centering
\begin{verbatim}
+---------------------+
|  Raw UNSW-NB15 Data |
+----------+----------+
           |
           v
+---------------------+      +------------------------+
|  Data Preprocessing | ---> | Feature Cleaning & IDs |
|      (Pipeline)     |      | Impute Missing Values  |
|                     |      | Encode & Scale Data    |
+----------+----------+      +------------------------+
           |
           v
+---------------------+
|   Data Splitting    |
| (Train / Val / Test)|
+----------+----------+
           |
           v
+-------------------------------------------------------+
|            Imbalance Handling Strategies              |
|                                                       |
|  [S0: None]    [S1: Class Weights]   [S2: Oversample] |
+----------+----------------+-----------------+---------+
           |                |                 |
           v                v                 v
+-------------------------------------------------------+
|                 Model Training                        |
|   1. Logistic Regression                              |
|   2. Random Forest                                    |
|   3. XGBoost                                          |
+--------------------------+----------------------------+
                           |
                           v
+-------------------------------------------------------+
|                 Performance Evaluation                |
|  (Accuracy, F1-Score, ROC-AUC, G-Mean, Confusion Mat) |
+-------------------------------------------------------+
\end{verbatim}
\caption{System Flow Diagram}
\label{fig:systemflow}
\end{figure}

\section{Technology Used}
The implementation was carried out using \textbf{Python}. We utilized the \textbf{scikit-learn} library for preprocessing, Logistic Regression, and Random Forest models. For gradient boosting, we used \textbf{XGBoost}. Oversampling was implemented using the \textbf{imbalanced-learn} library.

\subsection{Model Hyperparameters}
To establish a reproducible baseline, we used fixed hyperparameters across all experiments rather than performing hyperparameter tuning. This choice ensures fair comparison across strategies and simplifies replication.

\begin{itemize}
    \item \textbf{Logistic Regression:} $C=1.0$, solver=\texttt{saga}, penalty=\texttt{l2}, max\_iter=1000.
    \item \textbf{Random Forest:} n\_estimators=100, max\_depth=25, min\_samples\_split=5, min\_samples\_leaf=2.
    \item \textbf{XGBoost:} n\_estimators=100, learning\_rate=0.1, max\_depth=10, subsample=0.8, colsample\_bytree=0.8.
\end{itemize}

For XGBoost, we employed \textbf{early stopping} (patience=10 iterations) using the validation set to prevent overfitting. Training terminates if the evaluation metric does not improve for 10 consecutive boosting rounds, selecting the best iteration. This approach reduces overfitting risk without constituting hyperparameter tuning, as the patience value was fixed a priori.

All models were initialized with \texttt{random\_state=42} for reproducibility.

\section{Evaluation Metrics}
To evaluate the performance of our models, especially for minority attack classes, we used the following metrics:
\begin{itemize}
    \item \textbf{Accuracy:} Overall correctness of predictions.
    \item \textbf{Macro F1-Score:} Average F1 score across all classes, treating each class equally.
    \item \textbf{ROC-AUC:} Ability of the model to distinguish between classes across thresholds.
    \item \textbf{G-Mean:} Balances recall of both majority and minority classes, useful for imbalanced datasets \cite{choudhary2025review}.
    \item \textbf{Confusion Matrix:} Summarizes correct and incorrect predictions for each class.
\end{itemize}


