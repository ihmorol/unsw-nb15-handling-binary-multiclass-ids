\chapter{Conclusion}

This chapter summarizes our contributions, acknowledges limitations, and outlines directions for future research.

\section{Summary}

This study investigated the impact of class imbalance on machine learning-based intrusion detection using the UNSW-NB15 dataset. We systematically compared three classical models---Logistic Regression, Random Forest, and XGBoost---across three imbalance-handling strategies for both binary and multiclass classification tasks. Our 18-experiment evaluation yielded several significant findings:

\textbf{Addressing Research Questions:}

\begin{itemize}
    \item \textbf{RQ1 (Imbalance Impact)}: Class imbalance severely degrades multiclass detection, with baseline models achieving 0\% recall on all four rare attack categories despite high overall accuracy.
    
    \item \textbf{RQ2 (Strategy Effectiveness)}: Both class weighting and oversampling dramatically improve minority class detection. Class weighting increased Worms recall from 0\% to 82\% and Shellcode recall from 0\% to 96\%.
    
    \item \textbf{RQ3 (Model Consistency)}: All three models benefited from imbalance handling, though XGBoost consistently outperformed alternatives across both tasks and all strategies.
    
    \item \textbf{RQ4 (Rare Class Trade-offs)}: Improved recall for rare classes comes at the cost of reduced precision, representing an inherent precision-recall trade-off in imbalanced classification.
\end{itemize}

\textbf{Key Contributions:}

\begin{enumerate}
    \item We provided the first systematic comparison of three imbalance strategies across three classical ML models on both binary and multiclass UNSW-NB15 tasks (18 experiments).
    
    \item We demonstrated that aggregate accuracy metrics can be misleading, advocating for G-Mean and per-class metrics in IDS evaluation.
    
    \item We established a reproducible baseline pipeline with fixed hyperparameters and documented methodology for future research comparison.
    
    \item We quantified the critical failure of baseline models on rare attacks, providing empirical evidence for the necessity of imbalance handling in security-critical applications.
\end{enumerate}

\section{Limitations}

We acknowledge several limitations of this study:

\begin{itemize}
    \item \textbf{Single Dataset}: All experiments were conducted on UNSW-NB15. While this is a standard benchmark, results may not generalize to other network environments or datasets (e.g., CIC-IDS2017, BoT-IoT).
    
    \item \textbf{Fixed Hyperparameters}: We did not perform hyperparameter optimization, instead using fixed configurations to ensure fair comparison. Tuned models may achieve higher performance.
    
    \item \textbf{Single Random Seed}: All experiments used \texttt{random\_state=42}. Results may exhibit variance across different seeds, though we expect patterns to remain consistent.
    
    \item \textbf{Classical ML Only}: We did not compare against deep learning approaches (CNN, LSTM, Transformers), which may offer superior performance on complex network traffic patterns \cite{Vinayakumar2019}.
    
    \item \textbf{Low Rare Class Precision}: Despite improved recall, precision for rare classes remains low (3--22\%), resulting in elevated false positive rates that may limit practical deployment.
    
    \item \textbf{No Temporal Analysis}: Network traffic is inherently time-series data, but our models treat each connection independently without considering temporal patterns.
\end{itemize}

\section{Future Work}

Several promising directions emerge from this research:

\begin{itemize}
    \item \textbf{Hyperparameter Optimization}: Applying Bayesian optimization or automated machine learning (AutoML) to identify optimal model configurations.
    
    \item \textbf{Advanced Sampling Strategies}: Evaluating hybrid techniques such as SMOTE-ENN or SMOTE-Tomek that combine oversampling with noise reduction \cite{chawla2002smote}.
    
    \item \textbf{Deep Learning Comparison}: Benchmarking classical models against modern architectures including Convolutional Neural Networks, Recurrent Neural Networks, and Transformer-based models \cite{Vibhute2024}.
    
    \item \textbf{Multi-Dataset Validation}: Replicating experiments on CIC-IDS2017, CSE-CIC-IDS2018, and BoT-IoT datasets to assess generalizability \cite{8993711}.
    
    \item \textbf{Cost-Sensitive Deep Learning}: Exploring focal loss and other cost-sensitive objectives for neural network-based IDS.
    
    \item \textbf{Real-Time Deployment}: Evaluating inference latency and developing optimized models suitable for production network monitoring.
    
    \item \textbf{Adversarial Robustness}: Testing model resilience against adversarial examples designed to evade detection.
\end{itemize}

In conclusion, this study demonstrates that addressing class imbalance is not optional but essential for reliable intrusion detection. Our reproducible baseline and comprehensive evaluation provide a foundation for future research advancing the state of the art in machine learning-based network security.
